# -*- coding: utf-8 -*-
"""tradeBotFinal

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbWv4C8odhLKwuhZLTPBrGcHXQsf3C40

# Project Overview

The objective of this research project is to explore and compare the efficacy of various NN architectures in the prediction of future stock price movements.

The models will be trained on historical OHLC stock prices of $SPY from 2016 to 2023 inclusive, from the 5min interval chart. (roughly 158k samples)

The default parameters that will remain constant are; *timesteps*, *look_ahead_period*.

The model architectures to be tested are:
- LSTM; CNN; CNN+LSTM; Transformer

The top 3 performing models will then undergo additional testing by stacking another simple linear NN on top of the output, with the intention of categorizing any given moment as a [strong sell, sell, hold, buy, or strong buy].

# ---Environment Prep---
"""

# !pip install -r /content/drive/MyDrive/NeuralNetworkTradeBot/requirements.txt

!pip install tensorflow

!pip install mplfinance

!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
!tar -xzf ta-lib-0.4.0-src.tar.gz
!cd ta-lib && ./configure --prefix=/usr && make && make install
!pip install TA-Lib

"""# Imports"""

import pandas as pd
import numpy as np
import talib as ta
import tensorflow as tf
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, mean_absolute_error, r2_score
import seaborn as sns
import mplfinance as mpf
import matplotlib.dates as mdates
import time
import os
import gc
from scipy.signal import argrelextrema
from sklearn.metrics import mean_squared_error
import plotly.graph_objects as go
from scipy.stats import norm
import shap
import random
import torch

# import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""# Helper Functions - adding features"""

def addFVGandATRFeatures(dataset, atr_file=f'/content/drive/MyDrive/NeuralNetworkTradeBot/live/SPYdailyATR.csv'):
    #'/content/drive/MyDrive/NeuralNetworkTradeBot/SPYdaily_ATRv2.csv'
    """
    Adds FVG signals and distances from calculated ATR lines to the dataset.

    Parameters:
    - dataset: The input data containing columns such as datetime, close, open, high, low, etc.
    - atr_file: The path to the CSV file containing daily ATR values (default: 'historical_data/SPYdaily_ATR.csv').

    Returns:
    - A DataFrame with added FVG signals and distances from ATR lines.
    """
    data = pd.DataFrame(dataset)

    # Convert the 'datetime' column to datetime format
    data['datetime'] = pd.to_datetime(data['datetime'])

    # Initialize the new column for FVG signals
    data['fvg_signal'] = 0

    # Iterate through the DataFrame starting from the third row
    for i in range(2, len(data)):
        current_row = data.iloc[i]
        two_steps_back_row = data.iloc[i - 2]

        # Check for bullish FVG
        if current_row['low'] > two_steps_back_row['high']:
            data.at[i, 'fvg_signal'] = 1
        # Check for bearish FVG
        elif current_row['high'] < two_steps_back_row['low']:
            data.at[i, 'fvg_signal'] = -1

    # Add ATR Lines and Distances
    atr_vals = pd.read_csv(atr_file)

    # Convert the 'datetime' columns to datetime objects
    atr_vals['datetime'] = pd.to_datetime(atr_vals['datetime'])

    # Extract the date part from the 'datetime' column in data and ATR values
    data['date'] = data['datetime'].dt.date
    atr_vals['date'] = atr_vals['datetime'].dt.date

    # Set 'date' as the index for atr_vals
    atr_vals.set_index('date', inplace=True)

    data[f'daily_atr'] = None

    # Initialize new columns for distances from upper and lower lines
    for i in range(1, 7):
        data[f'distance_from_upperLine{i}'] = None
        data[f'distance_from_lowerLine{i}'] = None

    prev_day_close = None
    upper_lines = None
    lower_lines = None

    for i in range(len(data)):
        current_row = data.iloc[i]

        # Check if a new day starts
        if i == 0 or current_row['date'] != data.iloc[i - 1]['date']:
            if i == 0:
                prev_day_close = current_row['close']
            else:
                # depends on prev value being 4pm
                prev_day_close = data.iloc[i - 1]['close']

            date = current_row['date']

            # Retrieve the ATR value for the specific date, handling missing dates
            atr = atr_vals.at[date, 'atr'] if date in atr_vals.index else None

            if atr is not None:
                # Calculate upper and lower lines based on ATR multiples
                upper_lines = [prev_day_close + (multiplier * atr) for multiplier in
                               [0.236, 0.382, 0.5, 0.618, 0.786, 1]]
                lower_lines = [prev_day_close - (multiplier * atr) for multiplier in
                               [0.236, 0.382, 0.5, 0.618, 0.786, 1]]

        if upper_lines and lower_lines:
            data.at[i, 'daily_atr'] = atr
            # Calculate distances from upper and lower lines for each row
            for j in range(6):
                data.at[i, f'distance_from_upperLine{j + 1}'] = round(current_row['close'] - upper_lines[j], 2)
                data.at[i, f'upperLine{j + 1}'] = round(upper_lines[j], 2)
                data.at[i, f'distance_from_lowerLine{j + 1}'] = round(current_row['close'] - lower_lines[j], 2)
                data.at[i, f'lowerLine{j + 1}'] = round(lower_lines[j], 2)

    # Drop the extra 'date' column
    # data = data.drop(columns=['date'])

    return data

def add_relative_time_of_day(data, trading_start_time="09:30", trading_end_time="16:00"):
    """
    Adds a feature indicating the relative time of day to the dataset.

    Parameters:
    - data: DataFrame containing the dataset, including a 'datetime' column.
    - trading_start_time: The start time of the trading day (default is "09:30").
    - trading_end_time: The end time of the trading day (default is "16:00").

    Returns:
    - DataFrame with the 'relative_time_of_day' feature added.
    """
    data = pd.DataFrame(data)
    # Convert start and end times to datetime.time objects
    trading_start = pd.to_datetime(trading_start_time).time()
    trading_end = pd.to_datetime(trading_end_time).time()

    # Calculate total trading minutes in a day
    total_trading_minutes = (pd.to_datetime(trading_end_time) - pd.to_datetime(trading_start_time)).total_seconds() / 60.0

    # Function to calculate relative time of day
    def calculate_relative_time_of_day(row):
        # Convert 'datetime' to ensure it's in datetime format if not already
        row['datetime'] = pd.to_datetime(row['datetime'])

        current_time = row['datetime'].time()
        minutes_since_open = (pd.to_datetime(current_time.strftime("%H:%M")) - pd.to_datetime(trading_start_time)).total_seconds() / 60.0
        return round(minutes_since_open / total_trading_minutes, 4)  # Normalized to [0, 1]

    # Apply the function to the dataset
    data['relative_time_of_day'] = data.apply(calculate_relative_time_of_day, axis=1)

    return data

"""# Helper Functions - Pattern Detection"""

def detect_head_shoulder(df, window=3):
# Define the rolling window
    df = pd.DataFrame(df)
    roll_window = window
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['high'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['low'].rolling(window=roll_window).min()
    # Create a boolean mask for Head and Shoulder pattern
    mask_head_shoulder = ((df['high_roll_max'] > df['high'].shift(1)) & (df['high'] < df['high'].shift(1)))
    # Create a boolean mask for Inverse Head and Shoulder pattern
    mask_inv_head_shoulder = ((df['low_roll_min'] < df['low'].shift(1)) & (df['low'] > df['low'].shift(1)))
    # Create a new column for Head and Shoulder and its inverse pattern and populate it using the boolean masks
    df[f'head_shoulder_pattern_{window}'] = 0
    df.loc[mask_head_shoulder, f'head_shoulder_pattern_{window}'] = 1
    df.loc[mask_inv_head_shoulder, f'head_shoulder_pattern_{window}'] = -1
    return df
    # return not df['head_shoulder_pattern'].isna().any().item()


def calculate_support_resistance(df, window=3):
# Define the rolling window
    df = pd.DataFrame(df)
    roll_window = window
    # Set the number of standard deviation
    std_dev = 2
    # Calculate the mean and standard deviation for High and Low
    mean_high = df['high'].rolling(window=roll_window).mean()
    std_high = df['high'].rolling(window=roll_window).std()
    mean_low = df['low'].rolling(window=roll_window).mean()
    std_low = df['low'].rolling(window=roll_window).std()
    # Create a new column for support and resistance
    df[f'flat_support_{window}'] = mean_low - std_dev * std_low
    df[f'flat_resistance_{window}'] = mean_high + std_dev * std_high
    return df


def detect_triangle_pattern(df, window=3):
    # Define the rolling window
    df = pd.DataFrame(df)
    roll_window = window
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['high'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['low'].rolling(window=roll_window).min()
    # Create a boolean mask for ascending triangle pattern
    mask_asc = (df['high_roll_max'] >= df['high'].shift(1)) & (df['low_roll_min'] <= df['low'].shift(1)) & (df['close'] > df['close'].shift(1))
    # Create a boolean mask for descending triangle pattern
    mask_desc = (df['high_roll_max'] <= df['high'].shift(1)) & (df['low_roll_min'] >= df['low'].shift(1)) & (df['close'] < df['close'].shift(1))
    # Create a new column for triangle pattern and populate it using the boolean masks
    df[f'triangle_pattern_{window}'] = 0
    df.loc[mask_asc, f'triangle_pattern_{window}'] = 1
    df.loc[mask_desc, f'triangle_pattern_{window}'] = -1
    return df


def detect_wedge(df, window=3):
    df = pd.DataFrame(df)
    # Define the rolling window
    roll_window = window

    # Create a rolling window for High and Low
    df['high_roll_max'] = df['high'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['low'].rolling(window=roll_window).min()

    # Create trend indicators for High and Low using .iloc
    df['trend_high'] = df['high'].rolling(window=roll_window).apply(lambda x: 1 if (x.iloc[-1] - x.iloc[0]) > 0 else -1 if (x.iloc[-1] - x.iloc[0]) < 0 else 0)
    df['trend_low'] = df['low'].rolling(window=roll_window).apply(lambda x: 1 if (x.iloc[-1] - x.iloc[0]) > 0 else -1 if (x.iloc[-1] - x.iloc[0]) < 0 else 0)

    # Drop rows with NaN values created by rolling
    df = df.dropna(subset=['high_roll_max', 'low_roll_min', 'trend_high', 'trend_low'])

    # Create a boolean mask for Wedge Up pattern
    mask_wedge_up = (df['high_roll_max'] >= df['high'].shift(1)) & \
                    (df['low_roll_min'] <= df['low'].shift(1)) & \
                    (df['trend_high'] == 1) & \
                    (df['trend_low'] == 1)

    # Create a boolean mask for Wedge Down pattern
    mask_wedge_down = (df['high_roll_max'] <= df['high'].shift(1)) & \
                      (df['low_roll_min'] >= df['low'].shift(1)) & \
                      (df['trend_high'] == -1) & \
                      (df['trend_low'] == -1)

    # Create a new column for Wedge Up and Wedge Down pattern
    df[f'wedge_pattern_{window}'] = 0
    df.loc[mask_wedge_up, f'wedge_pattern_{window}'] = 1
    df.loc[mask_wedge_down, f'wedge_pattern_{window}'] = -1

    return df


def detect_channel(df, window=3):
    df = pd.DataFrame(df)
    # Define the rolling window
    roll_window = window
    # Define a factor to check for the range of channel
    channel_range = 0.1
    # Create a rolling window for High and Low
    df['high_roll_max'] = df['high'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['low'].rolling(window=roll_window).min()
    # Create trend indicators for High and Low using .iloc
    df['trend_high'] = df['high'].rolling(window=roll_window).apply(lambda x: 1 if (x.iloc[-1] - x.iloc[0]) > 0 else -1 if (x.iloc[-1] - x.iloc[0]) < 0 else 0)
    df['trend_low'] = df['low'].rolling(window=roll_window).apply(lambda x: 1 if (x.iloc[-1] - x.iloc[0]) > 0 else -1 if (x.iloc[-1] - x.iloc[0]) < 0 else 0)
    # Create a boolean mask for Channel Up pattern
    mask_channel_up = (df['high_roll_max'] >= df['high'].shift(1)) & (df['low_roll_min'] <= df['low'].shift(1)) & (df['high_roll_max'] - df['low_roll_min'] <= channel_range * (df['high_roll_max'] + df['low_roll_min'])/2) & (df['trend_high'] == 1) & (df['trend_low'] == 1)
    # Create a boolean mask for Channel Down pattern
    mask_channel_down = (df['high_roll_max'] <= df['high'].shift(1)) & (df['low_roll_min'] >= df['low'].shift(1)) & (df['high_roll_max'] - df['low_roll_min'] <= channel_range * (df['high_roll_max'] + df['low_roll_min'])/2) & (df['trend_high'] == -1) & (df['trend_low'] == -1)
    # Create a new column for Channel Up and Channel Down pattern and populate it using the boolean masks
    df[f'channel_pattern_{window}'] = 0
    df.loc[mask_channel_up, f'channel_pattern_{window}'] = 1
    df.loc[mask_channel_down, f'channel_pattern_{window}'] = -1
    return df


def detect_double_top_bottom(df, window=3, threshold=0.05):
    df = pd.DataFrame(df)
    # Define the rolling window
    roll_window = window
    # Define a threshold to check for the range of pattern
    range_threshold = threshold

    # Create a rolling window for High and Low
    df['high_roll_max'] = df['high'].rolling(window=roll_window).max()
    df['low_roll_min'] = df['low'].rolling(window=roll_window).min()

    # Create a boolean mask for Double Top pattern
    mask_double_top = (df['high_roll_max'] >= df['high'].shift(1)) & (df['high'] < df['high'].shift(1)) & ((df['high'].shift(1) - df['low'].shift(1)) <= range_threshold * (df['high'].shift(1) + df['low'].shift(1))/2)
    # Create a boolean mask for Double Bottom pattern
    mask_double_bottom = (df['low_roll_min'] <= df['low'].shift(1)) & (df['low'] > df['low'].shift(1)) & ((df['high'].shift(1) - df['low'].shift(1)) <= range_threshold * (df['high'].shift(1) + df['low'].shift(1))/2)

    # Create a new column for Double Top and Double Bottom pattern and populate it using the boolean masks
    df[f'double_pattern_{window}'] = 0
    df.loc[mask_double_top, f'double_pattern_{window}'] = -1
    df.loc[mask_double_bottom, f'double_pattern_{window}'] = 1

    return df


def detect_trendline(df, window=12):
    df = pd.DataFrame(df)
    # Define the rolling window
    roll_window = window
    # Create new columns for the linear regression slope and y-intercept
    df['slope'] = np.nan
    df['intercept'] = np.nan

    for i in range(window, len(df)):
        x = np.array(range(i-window, i))
        y = df['close'][i-window:i]
        A = np.vstack([x, np.ones(len(x))]).T
        m, c = np.linalg.lstsq(A, y, rcond=None)[0]
        df.at[df.index[i], 'slope'] = m
        df.at[df.index[i], 'intercept'] = c

    # Create a boolean mask for trendline support
    mask_support = df['slope'] > 0

    # Create a boolean mask for trendline resistance
    mask_resistance = df['slope'] < 0

    # Create new columns for trendline support and resistance
    df[f'support_{window}'] = df['close']
    df[f'resistance_{window}'] = df['close']

    # Populate the new columns using the boolean masks
    df.loc[mask_support, f'support_{window}'] = df['close'] * df['slope'] + df['intercept']
    df.loc[mask_resistance, f'resistance_{window}'] = df['close'] * df['slope'] + df['intercept']

    df.drop(columns=['slope', 'intercept'], inplace=True)

    return df


def detect_sr_trendlines(df, window=12, num_extrema=3, error_threshold=100):
    df = pd.DataFrame(df)
    # Calculate local minima and maxima indices once for the entire DataFrame
    minima_indices = argrelextrema(df['close'].values, np.less_equal, order=window)[0]
    maxima_indices = argrelextrema(df['close'].values, np.greater_equal, order=window)[0]

    # Initialize columns for support and resistance trendlines
    df[f'support_trendline_{window}'] = np.nan
    df[f'resistance_trendline_{window}'] = np.nan

    # Iterate through the DataFrame and calculate trendlines for each timestep
    for i in range(len(df)):
        # Select the relevant extrema indices for support and resistance
        selected_minima = minima_indices[minima_indices < (i - window / 2)][-num_extrema:]
        selected_maxima = maxima_indices[maxima_indices < (i - window / 2)][-num_extrema:]

        if len(selected_minima) == num_extrema:
            # Fit trendline for support (minima)
            x_vals = selected_minima
            y_vals = df['close'].iloc[x_vals].values
            slope, intercept = np.polyfit(x_vals, y_vals, 1)
            trendline_fit = slope * x_vals + intercept
            error = mean_squared_error(y_vals, trendline_fit)
            if error <= error_threshold:
                df.loc[i, f'support_trendline_{window}'] = slope * i + intercept

        if len(selected_maxima) == num_extrema:
            # Fit trendline for resistance (maxima)
            x_vals = selected_maxima
            y_vals = df['close'].iloc[x_vals].values
            slope, intercept = np.polyfit(x_vals, y_vals, 1)
            trendline_fit = slope * x_vals + intercept
            error = mean_squared_error(y_vals, trendline_fit)
            if error <= error_threshold:
                df.loc[i, f'resistance_trendline_{window}'] = slope * i + intercept

    # Fill NaN values for missing trendlines with the closing price
    df[f'support_trendline_{window}'].fillna(df['close'], inplace=True)
    df[f'resistance_trendline_{window}'].fillna(df['close'], inplace=True)

    return df




# def find_pivots(df):
#     # Calculate differences between consecutive highs and lows
#     high_diffs = df['high'].diff()
#     low_diffs = df['low'].diff()

#     # Find higher high
#     higher_high_mask = (high_diffs > 0) & (high_diffs.shift(-1) < 0)

#     # Find lower low
#     lower_low_mask = (low_diffs < 0) & (low_diffs.shift(-1) > 0)

#     # Find lower high
#     lower_high_mask = (high_diffs < 0) & (high_diffs.shift(-1) > 0)

#     # Find higher low
#     higher_low_mask = (low_diffs > 0) & (low_diffs.shift(-1) < 0)

#     # Create signals column
#     df['signal'] = ''
#     df.loc[higher_high_mask, 'signal'] = 'HH'
#     df.loc[lower_low_mask, 'signal'] = 'LL'
#     df.loc[lower_high_mask, 'signal'] = 'LH'
#     df.loc[higher_low_mask, 'signal'] = 'HL'

#     return df

"""# Helper Functions - for NNs"""

def create_mask(input_data, mask_percentage=0.2):
    """
    Create a mask where 0 indicates masked values, and 1 indicates visible values.
    :param input_data: The input data to mask.
    :param mask_percentage: The percentage of data to mask.
    :return: The input data with a mask applied.
    """
    # Create a random mask
    mask = np.random.rand(*input_data.shape) < mask_percentage
    masked_data = np.copy(input_data)
    masked_data[mask] = 0  # Masking with zeros (you could also use NaN or another value)
    return masked_data, mask

# Custom quantile loss function
def quantile_loss(q, y_true, y_pred):
    err = y_true - y_pred
    return tf.reduce_mean(tf.maximum(q * err, (q - 1) * err))

# Wrapper function to assign `q` correctly in loss_dict
def quantile_loss_wrapper(q):
    def loss(y_true, y_pred):
        return quantile_loss(q, y_true, y_pred)
    return loss

# Custom callback to save model every 5th epoch
class SaveEvery5thEpoch(tf.keras.callbacks.Callback):
    def __init__(self, save_path, model_name):
        super(SaveEvery5thEpoch, self).__init__()
        self.save_path = save_path
        self.model_name = model_name

    def on_epoch_end(self, epoch, logs=None):
        if (epoch + 1) % 5 == 0 or epoch == 0:
            save_filename = f"{self.save_path}/{self.model_name}_epoch_{epoch + 1}.keras"
            self.model.save(save_filename)
            print(f"Model saved at {save_filename}")

tf.keras.utils.register_keras_serializable()
def f1_score(y_true, y_pred):
    # Ensure both tensors are the same type (float32)
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    # Convert predictions to binary (1 or 0)
    y_pred = tf.round(y_pred)  # assuming y_pred are probabilities from a softmax

    # Calculate true positives, false positives, false negatives
    true_positives = tf.reduce_sum(y_true * y_pred)
    false_positives = tf.reduce_sum((1 - y_true) * y_pred)
    false_negatives = tf.reduce_sum(y_true * (1 - y_pred))

    # Calculate precision and recall
    precision = true_positives / (true_positives + false_positives + tf.keras.backend.epsilon())
    recall = true_positives / (true_positives + false_negatives + tf.keras.backend.epsilon())

    # Calculate F1 score
    f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())

    return f1

# F1 score function for multi-class classification
tf.keras.utils.register_keras_serializable()
def f1_score_multiclass(y_true, y_pred):
    # Ensure both tensors are of type float32
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)

    # Get the predicted class for each instance (with highest probability)
    y_pred_classes = tf.argmax(y_pred, axis=-1)  # Get the class with the max probability for each sample
    y_true_classes = tf.argmax(y_true, axis=-1)  # Get the true class label for each sample

    # Calculate the number of true positives, false positives, and false negatives
    true_positives = tf.reduce_sum(tf.cast(tf.equal(y_true_classes, y_pred_classes), tf.float32))
    false_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(y_true_classes, y_pred_classes),
                                                           tf.equal(y_pred_classes, 1)), tf.float32))  # You can generalize this by looping through classes if necessary
    false_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(y_true_classes, y_pred_classes),
                                                            tf.equal(y_true_classes, 1)), tf.float32))  # Same for false_negatives

    # Calculate precision, recall, and F1 score
    precision = true_positives / (true_positives + false_positives + tf.keras.backend.epsilon())
    recall = true_positives / (true_positives + false_negatives + tf.keras.backend.epsilon())

    # Calculate F1 score
    f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())

    return f1

"""# Helper Functions - Visualizations"""

from plotly.subplots import make_subplots
import plotly.graph_objects as go

def plot_training_history(history1, history2=None, history3=None):
    # Create subplots for loss
    fig = make_subplots(rows=1, cols=1, subplot_titles=("Training & Validation Loss"))

    # Plot loss for history1
    fig.add_trace(go.Scatter(
        x=list(range(1, len(history1['loss']) + 1)),
        y=history1['loss'],
        mode='lines',
        name='Training Loss (History 1)',
        line=dict(color='red')
    ), row=1, col=1)

    fig.add_trace(go.Scatter(
        x=list(range(1, len(history1['val_loss']) + 1)),
        y=history1['val_loss'],
        mode='lines',
        name='Validation Loss (History 1)',
        line=dict(color='orange')
    ), row=1, col=1)

    if history2 is not None:
        # Plot loss for history2
        fig.add_trace(go.Scatter(
            x=list(range(1, len(history2['loss']) + 1)),
            y=history2['loss'],
            mode='lines',
            name='Training Loss (History 2)',
            line=dict(color='blue')
        ), row=1, col=1)

        fig.add_trace(go.Scatter(
            x=list(range(1, len(history2['val_loss']) + 1)),
            y=history2['val_loss'],
            mode='lines',
            name='Validation Loss (History 2)',
            line=dict(color='green')
        ), row=1, col=1)

    if history3 is not None:
        # Plot loss for history3
        fig.add_trace(go.Scatter(
            x=list(range(1, len(history3['loss']) + 1)),
            y=history3['loss'],
            mode='lines',
            name='Training Loss (History 3)',
            line=dict(color='blue')
        ), row=1, col=1)

        fig.add_trace(go.Scatter(
            x=list(range(1, len(history3['val_loss']) + 1)),
            y=history3['val_loss'],
            mode='lines',
            name='Validation Loss (History 3)',
            line=dict(color='green')
        ), row=1, col=1)

    # Update layout
    fig.update_layout(
        title="Model Training Performance",
        xaxis_title="Epochs",
        yaxis_title="Loss",
        showlegend=True,
        template="plotly_dark"
    )

    fig.show()

def plot_prediction_scatter(y_true, y_pred, title="Predicted vs Actual Values"):
    """
    Create an interactive scatter plot comparing predicted vs actual values,
    including quadrant analysis.

    Parameters:
    -----------
    y_true : array-like
        The true/actual values
    y_pred : array-like
        The predicted values
    title : str, optional
        The title for the plot

    Returns:
    --------
    None
        Displays the interactive plot
    """

    # Calculate error metrics
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)

    # Calculate quadrant percentages
    y_true_mean = np.mean(y_true)
    y_pred_mean = np.mean(y_pred)

    q1 = np.sum((y_true >= y_true_mean) & (y_pred >= y_pred_mean)) / len(y_true) * 100
    q2 = np.sum((y_true < y_true_mean) & (y_pred >= y_pred_mean)) / len(y_true) * 100
    q3 = np.sum((y_true < y_true_mean) & (y_pred < y_pred_mean)) / len(y_true) * 100
    q4 = np.sum((y_true >= y_true_mean) & (y_pred < y_pred_mean)) / len(y_true) * 100

    # Create the scatter plot
    fig = go.Figure()

    # Add scatter plot
    fig.add_trace(go.Scatter(
        x=y_true,
        y=y_pred,
        mode='markers',
        name='Predictions',
        marker=dict(
            size=8,
            color='blue',
            opacity=0.6
        ),
        hovertemplate="Actual: %{x:.3f}<br>Predicted: %{y:.3f}<extra></extra>"
    ))

    # Add perfect prediction line
    min_val = min(min(y_true), min(y_pred))
    max_val = max(max(y_true), max(y_pred))
    fig.add_trace(go.Scatter(
        x=[min_val, max_val],
        y=[min_val, max_val],
        mode='lines',
        name='Perfect Prediction',
        line=dict(color='red', dash='dash')
    ))

    # Add quadrant lines
    fig.add_hline(y=y_pred_mean, line_dash="dot", line_color="gray", opacity=0.5)
    fig.add_vline(x=y_true_mean, line_dash="dot", line_color="gray", opacity=0.5)

    # Add quadrant annotations
    annotations = [
        dict(x=max_val, y=max_val, xref="x", yref="y",
             text=f"Q1: {q1:.1f}%", showarrow=False,
             font=dict(size=10), xanchor="left", yanchor="bottom"),
        dict(x=min_val, y=max_val, xref="x", yref="y",
             text=f"Q2: {q2:.1f}%", showarrow=False,
             font=dict(size=10), xanchor="right", yanchor="bottom"),
        dict(x=min_val, y=min_val, xref="x", yref="y",
             text=f"Q3: {q3:.1f}%", showarrow=False,
             font=dict(size=10), xanchor="right", yanchor="top"),
        dict(x=max_val, y=min_val, xref="x", yref="y",
             text=f"Q4: {q4:.1f}%", showarrow=False,
             font=dict(size=10), xanchor="left", yanchor="top")
    ]

    # Update layout with metrics
    fig.update_layout(
        title=dict(
            text=f"{title}<br><sub>RMSE: {rmse:.3f} | MAE: {mae:.3f} | R²: {r2:.3f}</sub>",
            x=0.5,
            xanchor='center'
        ),
        xaxis_title="Actual Values",
        yaxis_title="Predicted Values",
        showlegend=True,
        width=800,
        height=600,
        template='plotly_white',
        hovermode='closest',
        annotations=annotations
    )

    # Make the plot square and equal axes
    fig.update_layout(
        yaxis=dict(
            scaleanchor="x",
            scaleratio=1,
        )
    )

    # Add grid
    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')
    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')

    # Show the plot
    fig.show()

import plotly.graph_objects as go
import plotly.io as pio
from plotly.subplots import make_subplots

def plot_ohlc_w_simple_close_pred(X, y_pred, df, atr_column='daily_atr',
                                  threshold=0.1, y_pred_classes=None):
    """
    Plot the OHLC candles from df along with the model's predictions (y_pred) using Plotly.

    Args:
    - X: Features (input data).
    - y_pred: Model predictions (output of the LSTM model). These are the predicted deltas normalized by ATR.
    - df: DataFrame containing price and ATR information.
    - atr_column: Name of the column containing ATR values.
    - threshold: Optional, prediction threshold for visualizing significant movements.
    """

    # Ensure 'datetime' is a datetime object
    df['datetime'] = pd.to_datetime(df['datetime'])

    # Calculate predicted close price (add the predicted delta to the current close price)
    predicted_close = df['close'].values + y_pred.flatten() * df[atr_column].values
    print(np.array(predicted_close.shape))
    predicted_close_shifted = pd.Series(predicted_close).shift(12)
    predicted_close_shifted = predicted_close_shifted.fillna(method='bfill')

    # Check for NaNs or extreme values in predicted_close
    if pd.isnull(predicted_close).any():
        print("Warning: NaN values found in predicted_close. Consider handling them.")

    # Ensure lengths are consistent
    if len(df['datetime']) != len(predicted_close):
        print("Error: Length mismatch between datetime and predicted_close arrays.")
        return

    # Check data ranges to ensure no issues with extreme values
    print("Low prices min:", df['low'].min())
    print("High prices max:", df['high'].max())
    print("Predicted close min:", predicted_close.min())
    print("Predicted close max:", predicted_close.max())

    fig = make_subplots(specs=[[{"secondary_y": True}]])

    # Candlestick on primary y-axis
    fig.add_trace(go.Candlestick(
        x=df['datetime'],
        open=df['open'],
        high=df['high'],
        low=df['low'],
        close=df['close'],
        name='Candlesticks',
        increasing_line_color='green',  # Color for up candles
        decreasing_line_color='red',
    ), secondary_y=False)

    # Add predicted close prices as a line plot (normalized by ATR)
    fig.add_trace(go.Scatter(
        x=df['datetime'],
        y=predicted_close,
        mode='markers',
        name='Predicted Points',
        marker=dict(color='purple', size=4, symbol='circle'),
        opacity=0.6,
    ), secondary_y=True)

    # Add predicted close prices as a line plot (normalized by ATR)
    fig.add_trace(go.Scatter(
        x=df['datetime'],
        y=predicted_close_shifted,
        mode='lines',
        name='Predicted Points (fwd-shifted)',
        line=dict(color='purple'),
        opacity=0.6,
    ), secondary_y=True)

    # Add markers for predicted classes (arrows)
    # if y_pred_classes is not None:
    #     for i in range(len(y_pred_classes)):
    #         if y_pred_classes[i] == 0 or y_pred_classes[i] == 1:
    #             # Plot red down arrow for class 0 or 1
    #             fig.add_trace(go.Scatter(
    #                 x=[df['datetime'].iloc[i]],
    #                 y=[predicted_close[i]],
    #                 mode='markers+text',
    #                 name='Down Arrow',
    #                 marker=dict(color='red', size=10, symbol='triangle-down'),
    #                 text=['↓'],
    #                 textposition='bottom center',
    #                 opacity=0.8
    #             ), secondary_y=True)
    #         elif y_pred_classes[i] == 3 or y_pred_classes[i] == 4:
    #             # Plot green up arrow for class 3 or 4
    #             fig.add_trace(go.Scatter(
    #                 x=[df['datetime'].iloc[i]],
    #                 y=[predicted_close[i]],
    #                 mode='markers+text',
    #                 name='Up Arrow',
    #                 marker=dict(color='green', size=10, symbol='triangle-up'),
    #                 text=['↑'],
    #                 textposition='top center',
    #                 opacity=0.8
    #             ), secondary_y=True)


    # Update layout
    fig.update_layout(
        title='OHLC Candlestick Chart with LSTM Predictions',
        xaxis_title='Date',
        yaxis_title='Price',
        yaxis2_title='Predicted Price',  # Label for secondary y-axis
        yaxis2=dict(
            overlaying='y',  # Ensure it shares the same x-axis scale
            side='right',    # Secondary axis will be on the right
            range=[df['low'].min(), df['high'].max()]  # Set range to match primary axis
        ),
        yaxis=dict(
            range=[df['low'].min(), df['high'].max()]  # Set range to match primary axis
        )
    )

    # Show the plot
    pio.show(fig)

def plot_ohlc_distext_pred(X, y_pred_min, y_pred_max, df, atr_column='daily_atr',
                                  threshold=0.1, y_pred_classes=None):
    """
    Plot the OHLC candles from df along with the model's predictions (y_pred) using Plotly.

    Args:
    - X: Features (input data).
    - y_pred: Model predictions (output of the LSTM model). These are the predicted deltas normalized by ATR.
    - df: DataFrame containing price and ATR information.
    - atr_column: Name of the column containing ATR values.
    - threshold: Optional, prediction threshold for visualizing significant movements.
    """

    # Ensure 'datetime' is a datetime object
    df['datetime'] = pd.to_datetime(df['datetime'])

    # Calculate predicted close price (add the predicted delta to the current close price)
    predicted_min = df['close'].values + y_pred_min.flatten() * df[atr_column].values
    predicted_max = df['close'].values + y_pred_max.flatten() * df[atr_column].values
    print(np.array(predicted_min.shape))

    # Check for NaNs or extreme values in predicted_close
    if pd.isnull(predicted_min).any():
        print("Warning: NaN values found in predicted_close. Consider handling them.")

    # Ensure lengths are consistent
    if len(df['datetime']) != len(predicted_min):
        print("Error: Length mismatch between datetime and predicted_close arrays.")
        return

    # Check data ranges to ensure no issues with extreme values
    print("Low prices min:", df['low'].min())
    print("High prices max:", df['high'].max())
    print("Predicted close min:", predicted_min.min())
    print("Predicted close max:", predicted_max.max())

    fig = make_subplots(specs=[[{"secondary_y": True}]])

    # Candlestick on primary y-axis
    fig.add_trace(go.Candlestick(
        x=df['datetime'],
        open=df['open'],
        high=df['high'],
        low=df['low'],
        close=df['close'],
        name='Candlesticks',
        increasing_line_color='green',  # Color for up candles
        decreasing_line_color='red',
    ), secondary_y=False)

    # Add predicted close prices as a line plot (normalized by ATR)
    # fig.add_trace(go.Scatter(
    #     x=df['datetime'],
    #     y=predicted_close,
    #     mode='markers',
    #     name='Predicted Points',
    #     marker=dict(color='purple', size=4, symbol='circle'),
    #     opacity=0.6,
    # ), secondary_y=True)

    # Add predicted close prices as a line plot (normalized by ATR)
    fig.add_trace(go.Scatter(
        x=df['datetime'],
        y=predicted_min,
        mode='lines',
        name='Predicted Points (fwd-shifted)',
        line=dict(color='green'),
        opacity=0.6,
    ), secondary_y=True)

    fig.add_trace(go.Scatter(
        x=df['datetime'],
        y=predicted_max,
        mode='lines',
        name='Predicted Points (fwd-shifted)',
        line=dict(color='red'),
        opacity=0.6,
    ), secondary_y=True)

    # Update layout
    fig.update_layout(
        title='OHLC Candlestick Chart with LSTM Predictions',
        xaxis_title='Date',
        yaxis_title='Price',
        yaxis2_title='Predicted Price',  # Label for secondary y-axis
        yaxis2=dict(
            overlaying='y',  # Ensure it shares the same x-axis scale
            side='right',    # Secondary axis will be on the right
            range=[df['low'].min(), df['high'].max()]  # Set range to match primary axis
        ),
        yaxis=dict(
            range=[df['low'].min(), df['high'].max()]  # Set range to match primary axis
        )
    )

    # Show the plot
    pio.show(fig)

# make scikit? confusion matrix
# evaluate_classification_trading_strategy()  --- need to create this fn first
def visualize_model_performance(model, X_test, y_test, df=None):
    """
    Visualizes the performance of a trained model on the test data.

    Parameters:
    - model: Trained LSTM model.
    - X_test: Test data (features).
    - y_test: True labels for the test data.
    - class_labels: List of class labels (default is [-2, -1, 0, 1, 2]).

    Outputs:
    - Bar chart showing precision for each class.
    - Confusion matrix displaying model performance.
    """

    # Predict the labels for the test data
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)  # Adjust to match class labels
    # y_test = np.argmax(y_test, axis=1)
    print(np.unique(y_pred_classes))

    class_labels = [0, 1, 2, 3, 4]
    # class_labels = [-2, -1, 0, 1, 2]

    # Generate a classification report
    report = classification_report(y_test, y_pred_classes, target_names=[str(label) for label in class_labels], output_dict=True)

    # Extract precision for each class
    precision_values = [report[str(label)]['precision'] for label in class_labels]

    # Plot precision for each class
    plt.figure(figsize=(10, 6))
    sns.barplot(x=class_labels, y=precision_values, palette="viridis")
    plt.title('Precision for Each Class')
    plt.xlabel('Class Labels')
    plt.ylabel('Precision')
    plt.show()

    # Generate and display the confusion matrix
    cm = confusion_matrix(y_test, y_pred_classes, labels=class_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()

def split_into_chunks(X, y_pred, df, y_pred2=None, chunk_size=158):
    """
    Split X, y_pred, and df into smaller chunks of size `chunk_size`.

    Args:
    - X: Features (input data).
    - y_pred: Model predictions (output of the LSTM model).
    - df: DataFrame containing price and ATR information.
    - chunk_size: Size of each chunk to split the data into.

    Returns:
    - A list of tuples, each containing a chunk of X, y_pred, and df.
    """
    num_chunks = len(X) // chunk_size
    chunks = []

    for i in range(num_chunks):
        start_idx = i * chunk_size
        end_idx = start_idx + chunk_size

        if y_pred2 is None:
            # Slice each of X, y_pred, and df into chunks
            X_chunk = X[start_idx:end_idx]
            y_pred_chunk = y_pred[start_idx:end_idx]
            df_chunk = df.iloc[start_idx:end_idx]

            chunks.append((X_chunk, y_pred_chunk, df_chunk))

        else:
            X_chunk = X[start_idx:end_idx]
            y_pred_min_chunk = y_pred[start_idx:end_idx]
            y_pred_max_chunk = y_pred2[start_idx:end_idx]
            df_chunk = df.iloc[start_idx:end_idx]

            chunks.append((X_chunk, y_pred_min_chunk, y_pred_max_chunk, df_chunk))

    return chunks

"""# Helper Functions - Evaluations"""

def evaluate_simple_close_trading_strategy(X, y_pred, df, atr_column='daily_atr',
                                           stop_loss_timesteps=18, take_profit_factor=1/10,
                                           threshold=0.25):
    """
    Evaluate the trading strategy based on the LSTM model's predictions (including long and short trades).

    Args:
    - X: Features (input data).
    - y_pred: Model predictions (output of the LSTM model).
    - df: DataFrame containing price and ATR information.
    - atr_column: Name of the column containing ATR values.
    - stop_loss_timesteps: Number of timesteps to wait before closing a trade (12 timesteps = 1 hour).
    - take_profit_factor: Fraction of the ATR value for take profit condition.
    - threshold: Threshold for deciding when to initiate a long or short trade.

    Returns:
    - total_profit: Total profit or loss from all trades.
    - successful_trades: Number of successful trades.
    - detailed_stats: Dictionary with detailed stats for long and short trades.
    """

    total_profit = 0
    successful_trades = 0
    long_trades = 0
    short_trades = 0
    long_wins = 0
    short_wins = 0
    total_long_profit = 0
    total_short_profit = 0
    total_long_loss = 0
    total_short_loss = 0

    current_trade = None  # No active trade initially

    for i in range(1, len(y_pred)):
        # Check if a new trade should be initiated (when y_pred > threshold or y_pred < -threshold)
        if current_trade is None:
            # Long trade entry condition (y_pred > threshold)
            if y_pred[i] > threshold:
                entry_price = df.iloc[i]['close']
                atr = df.iloc[i][atr_column]
                take_profit_factor = y_pred[i] * 0.75
                take_profit_price = entry_price + take_profit_factor * atr  # Price for take profit
                # stop_loss_price = entry_price - threshold * 2 * atr  # Price for stop loss
                stop_loss_price = entry_price - take_profit_factor * atr

                # Set the current trade details for long trade
                current_trade = {
                    'type': 'long',
                    'entry_price': entry_price,
                    'take_profit_price': take_profit_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                long_trades += 1  # Increment long trade count

            # Short trade entry condition (y_pred < -threshold)
            elif y_pred[i] < -threshold:
                entry_price = df.iloc[i]['close']
                atr = df.iloc[i][atr_column]
                take_profit_factor = y_pred[i] * 0.75
                take_profit_price = entry_price + take_profit_factor * atr  # Price for take profit (for short, it's lower)
                # stop_loss_price = entry_price + threshold * 2 * atr  # Price for stop loss (for short, it's higher)
                stop_loss_price = entry_price - take_profit_factor * atr

                # Set the current trade details for short trade
                current_trade = {
                    'type': 'short',
                    'entry_price': entry_price,
                    'take_profit_price': take_profit_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                short_trades += 1  # Increment short trade count

        # If we have an active trade, check the stop loss / take profit condition
        elif current_trade is not None:
            current_idx = i
            current_price = df.iloc[current_idx]['close']
            current_high = df.iloc[current_idx]['high']
            current_low = df.iloc[current_idx]['low']

            # Check if the take profit or stop loss condition is met
            if current_trade['type'] == 'long':
                # Long trade: Take profit when price goes up, stop loss when price goes down
                if current_price >= current_trade['take_profit_price'] or current_high >= current_trade['take_profit_price']:
                    # Take profit for long
                    # profit = (current_price - current_trade['entry_price'])
                    profit = (current_trade['take_profit_price'] - current_trade['entry_price'])
                    total_profit += profit
                    total_long_profit += profit
                    successful_trades += 1
                    long_wins += 1  # Increment long wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price <= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                # elif (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for long
                    loss = (current_price - current_trade['entry_price'])
                    total_profit += loss
                    total_long_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

            elif current_trade['type'] == 'short':
                # Short trade: Take profit when price goes down, stop loss when price goes up
                if current_price <= current_trade['take_profit_price'] or current_low <= current_trade['take_profit_price']:
                    # Take profit for short
                    # profit = (current_trade['entry_price'] - current_price)
                    profit = (current_trade['entry_price'] - current_trade['take_profit_price'])
                    total_profit += profit
                    total_short_profit += profit
                    successful_trades += 1
                    short_wins += 1  # Increment short wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price >= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                # elif (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for short
                    loss = (current_trade['entry_price'] - current_price)
                    total_profit += loss
                    total_short_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

    # Calculate win rates
    long_winrate = (long_wins / long_trades) * 100 if long_trades > 0 else 0
    short_winrate = (short_wins / short_trades) * 100 if short_trades > 0 else 0

    # Convert NumPy arrays to single numeric values
    total_long_profit = total_long_profit.item() if isinstance(total_long_profit, np.ndarray) else total_long_profit
    total_short_profit = total_short_profit.item() if isinstance(total_short_profit, np.ndarray) else total_short_profit
    total_long_loss = total_long_loss.item() if isinstance(total_long_loss, np.ndarray) else total_long_loss
    total_short_loss = total_short_loss.item() if isinstance(total_short_loss, np.ndarray) else total_short_loss
    total_profit = total_profit.item() if isinstance(total_profit, np.ndarray) else total_profit

    # Prepare detailed stats dictionary
    detailed_stats = {
        'long_trades': long_trades,
        'short_trades': short_trades,
        'long_winrate': long_winrate,
        'short_winrate': short_winrate,
        'total_long_profit': total_long_profit,
        'total_short_profit': total_short_profit,
        'total_long_loss': total_long_loss,
        'total_short_loss': total_short_loss,
    }

    # Print detailed stats
    print(f"Total Long Trades: {long_trades}")
    print(f"Total Short Trades: {short_trades}")
    print(f"Long Trade Winrate: {long_winrate:.2f}%")
    print(f"Short Trade Winrate: {short_winrate:.2f}%")
    print(f"Total Long Profit: {total_long_profit:.4f}")
    print(f"Total Short Profit: {total_short_profit:.4f}")
    print(f"Total Long Loss: {total_long_loss:.4f}")
    print(f"Total Short Loss: {total_short_loss:.4f}")
    print(f"Total Profit from All Trades: {total_profit:.4f}")

    return total_profit, successful_trades, detailed_stats

# evaluate EMA-cross, and EMA-cross + close predictions
def evaluate_ema_trading_strategy(X, y_pred, df, atr_column='daily_atr',
                                           stop_loss_timesteps=18, take_profit_factor=1/10,
                                           threshold=0.25, use_pred=False):

    total_profit = 0
    successful_trades = 0
    long_trades = 0
    short_trades = 0
    long_wins = 0
    short_wins = 0
    total_long_profit = 0
    total_short_profit = 0
    total_long_loss = 0
    total_short_loss = 0

    current_trade = None  # No active trade initially

    for i in range(1, len(y_pred)):
        # Check if a new trade should be initiated (when y_pred > threshold or y_pred < -threshold)
        if current_trade is None:
            # Long trade entry condition (y_pred > threshold)
            if y_pred[i] > threshold:
                entry_price = df.iloc[i]['close']
                atr = df.iloc[i][atr_column]
                take_profit_factor = y_pred[i] * 0.75
                take_profit_price = entry_price + take_profit_factor * atr  # Price for take profit
                # stop_loss_price = entry_price - threshold * 2 * atr  # Price for stop loss
                stop_loss_price = entry_price - take_profit_factor * atr

                # Set the current trade details for long trade
                current_trade = {
                    'type': 'long',
                    'entry_price': entry_price,
                    'take_profit_price': take_profit_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                long_trades += 1  # Increment long trade count

            # Short trade entry condition (y_pred < -threshold)
            elif y_pred[i] < -threshold:
                entry_price = df.iloc[i]['close']
                atr = df.iloc[i][atr_column]
                take_profit_factor = y_pred[i] * 0.75
                take_profit_price = entry_price + take_profit_factor * atr  # Price for take profit (for short, it's lower)
                # stop_loss_price = entry_price + threshold * 2 * atr  # Price for stop loss (for short, it's higher)
                stop_loss_price = entry_price - take_profit_factor * atr

                # Set the current trade details for short trade
                current_trade = {
                    'type': 'short',
                    'entry_price': entry_price,
                    'take_profit_price': take_profit_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                short_trades += 1  # Increment short trade count

        # If we have an active trade, check the stop loss / take profit condition
        elif current_trade is not None:
            current_idx = i
            current_price = df.iloc[current_idx]['close']
            current_high = df.iloc[current_idx]['high']
            current_low = df.iloc[current_idx]['low']

            # Check if the take profit or stop loss condition is met
            if current_trade['type'] == 'long':
                # Long trade: Take profit when price goes up, stop loss when price goes down
                if current_price >= current_trade['take_profit_price'] or current_high >= current_trade['take_profit_price']:
                    # Take profit for long
                    # profit = (current_price - current_trade['entry_price'])
                    profit = (current_trade['take_profit_price'] - current_trade['entry_price'])
                    total_profit += profit
                    total_long_profit += profit
                    successful_trades += 1
                    long_wins += 1  # Increment long wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price <= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                # elif (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for long
                    loss = (current_price - current_trade['entry_price'])
                    total_profit += loss
                    total_long_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

            elif current_trade['type'] == 'short':
                # Short trade: Take profit when price goes down, stop loss when price goes up
                if current_price <= current_trade['take_profit_price'] or current_low <= current_trade['take_profit_price']:
                    # Take profit for short
                    # profit = (current_trade['entry_price'] - current_price)
                    profit = (current_trade['entry_price'] - current_trade['take_profit_price'])
                    total_profit += profit
                    total_short_profit += profit
                    successful_trades += 1
                    short_wins += 1  # Increment short wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price >= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                # elif (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for short
                    loss = (current_trade['entry_price'] - current_price)
                    total_profit += loss
                    total_short_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

    # Calculate win rates
    long_winrate = (long_wins / long_trades) * 100 if long_trades > 0 else 0
    short_winrate = (short_wins / short_trades) * 100 if short_trades > 0 else 0

    # Convert NumPy arrays to single numeric values
    total_long_profit = total_long_profit.item() if isinstance(total_long_profit, np.ndarray) else total_long_profit
    total_short_profit = total_short_profit.item() if isinstance(total_short_profit, np.ndarray) else total_short_profit
    total_long_loss = total_long_loss.item() if isinstance(total_long_loss, np.ndarray) else total_long_loss
    total_short_loss = total_short_loss.item() if isinstance(total_short_loss, np.ndarray) else total_short_loss
    total_profit = total_profit.item() if isinstance(total_profit, np.ndarray) else total_profit

    # Prepare detailed stats dictionary
    detailed_stats = {
        'long_trades': long_trades,
        'short_trades': short_trades,
        'long_winrate': long_winrate,
        'short_winrate': short_winrate,
        'total_long_profit': total_long_profit,
        'total_short_profit': total_short_profit,
        'total_long_loss': total_long_loss,
        'total_short_loss': total_short_loss,
    }

    # Print detailed stats
    print(f"Total Long Trades: {long_trades}")
    print(f"Total Short Trades: {short_trades}")
    print(f"Long Trade Winrate: {long_winrate:.2f}%")
    print(f"Short Trade Winrate: {short_winrate:.2f}%")
    print(f"Total Long Profit: {total_long_profit:.4f}")
    print(f"Total Short Profit: {total_short_profit:.4f}")
    print(f"Total Long Loss: {total_long_loss:.4f}")
    print(f"Total Short Loss: {total_short_loss:.4f}")
    print(f"Total Profit from All Trades: {total_profit:.4f}")

    return total_profit, successful_trades, detailed_stats

def evaluate_class_trading_strategy(X, y_pred, df, atr_column='daily_atr',
                                  stop_loss_timesteps=18, threshold=0.25):
    """
    Evaluate the trading strategy based on the LSTM model's predictions (5 categories for buy/sell/hold).

    Args:
    - X: Features (input data).
    - y_pred: Model predictions (output of the LSTM model).
    - df: DataFrame containing price and ATR information.
    - atr_column: Name of the column containing ATR values.
    - stop_loss_timesteps: Number of timesteps to wait before closing a trade.
    - threshold: Threshold for deciding when to initiate a long or short trade.

    Returns:
    - total_profit: Total profit or loss from all trades.
    - successful_trades: Number of successful trades.
    - detailed_stats: Dictionary with detailed stats for long and short trades.
    """

    total_profit = 0
    successful_trades = 0
    long_trades = 0
    short_trades = 0
    long_wins = 0
    short_wins = 0
    total_long_profit = 0
    total_short_profit = 0
    total_long_loss = 0
    total_short_loss = 0

    current_trade = None  # No active trade initially

    for i in range(1, len(y_pred)):
        current_price = df.iloc[i]['close']
        atr = df.iloc[i][atr_column]

        # Determine the target price based on y_pred
        if y_pred[i] == 0:  # Strong Sell
            target_price = current_price - (atr * 0.24)
            stop_loss_price = current_price + (atr * 0.12)
        elif y_pred[i] == 1:  # Sell
            target_price = current_price - (atr * 0.12)
            stop_loss_price = current_price + (atr * 0.06)
        elif y_pred[i] == 3:  # Buy
            target_price = current_price + (atr * 0.10)
            stop_loss_price = current_price - (atr * 0.05)
        elif y_pred[i] == 4:  # Strong Buy
            target_price = current_price + (atr * 0.20)
            stop_loss_price = current_price - (atr * 0.10)
        else:  # Hold
            continue  # No trade for hold

        # Check if a new trade should be initiated
        if current_trade is None:
            if y_pred[i] == 0 or y_pred[i] == 1:  # Short trade (strong sell, sell)
                current_trade = {
                    'type': 'short',
                    'entry_price': current_price,
                    'target_price': target_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                short_trades += 1  # Increment short trade count
            elif y_pred[i] == 3 or y_pred[i] == 4:  # Long trade (buy, strong buy)
                current_trade = {
                    'type': 'long',
                    'entry_price': current_price,
                    'target_price': target_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                long_trades += 1  # Increment long trade count

        # If we have an active trade, check the stop loss / take profit condition
        elif current_trade is not None:
            current_idx = i
            current_price = df.iloc[current_idx]['close']
            current_high = df.iloc[current_idx]['high']
            current_low = df.iloc[current_idx]['low']

            # Check if the take profit or stop loss condition is met
            if current_trade['type'] == 'long':
                # Long trade: Take profit when price goes up, stop loss when price goes down
                if current_price >= current_trade['target_price'] or current_high >= current_trade['target_price']:
                    # Take profit for long
                    profit = (current_trade['target_price'] - current_trade['entry_price'])
                    total_profit += profit
                    total_long_profit += profit
                    successful_trades += 1
                    long_wins += 1  # Increment long wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price <= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for long
                    loss = (current_trade['entry_price'] - current_price)
                    total_profit += loss
                    total_long_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

            elif current_trade['type'] == 'short':
                # Short trade: Take profit when price goes down, stop loss when price goes up
                if current_price <= current_trade['target_price'] or current_low <= current_trade['target_price']:
                    # Take profit for short
                    profit = (current_trade['entry_price'] - current_trade['target_price'])
                    total_profit += profit
                    total_short_profit += profit
                    successful_trades += 1
                    short_wins += 1  # Increment short wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price >= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for short
                    loss = (current_price - current_trade['entry_price'])
                    total_profit += loss
                    total_short_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

    # Calculate win rates
    long_winrate = (long_wins / long_trades) * 100 if long_trades > 0 else 0
    short_winrate = (short_wins / short_trades) * 100 if short_trades > 0 else 0

    # Prepare detailed stats dictionary
    detailed_stats = {
        'long_trades': long_trades,
        'short_trades': short_trades,
        'long_winrate': long_winrate,
        'short_winrate': short_winrate,
        'total_long_profit': total_long_profit,
        'total_short_profit': total_short_profit,
        'total_long_loss': total_long_loss,
        'total_short_loss': total_short_loss,
    }

    # Print detailed stats
    print(f"Total Long Trades: {long_trades}")
    print(f"Total Short Trades: {short_trades}")
    print(f"Long Trade Winrate: {long_winrate:.2f}%")
    print(f"Short Trade Winrate: {short_winrate:.2f}%")
    print(f"Total Long Profit: {total_long_profit:.4f}")
    print(f"Total Short Profit: {total_short_profit:.4f}")
    print(f"Total Long Loss: {total_long_loss:.4f}")
    print(f"Total Short Loss: {total_short_loss:.4f}")
    print(f"Total Profit from All Trades: {total_profit:.4f}")

    return total_profit, successful_trades, detailed_stats

# long condition: y_pred_max > 3*abs(y_pred_min) && ema10diff > 0
# long exit: 0.5 * y_pred_max
# short condition: abs(y_pred_min) > 3*y_pred_max && ema10diff < 0
# short exit: 0.5 * y_pred_min
def evaluate_distext_strategy1(X, y_pred_min, y_pred_max, df, atr_column='daily_atr',
                               ema_column='ema_50_diff', stop_loss_timesteps=18, take_profit_factor=1/10):
    """
    Evaluate a trading strategy based on predicted minima and maxima, and the EMA10 difference for long and short trades.

    Args:
    - X: Features (input data).
    - y_pred_min: Predicted minimum values (output of the LSTM model).
    - y_pred_max: Predicted maximum values (output of the LSTM model).
    - df: DataFrame containing price and ATR information.
    - atr_column: Name of the column containing ATR values.
    - ema_column: Name of the column containing the EMA10 difference (ema10diff).
    - stop_loss_timesteps: Number of timesteps to wait before closing a trade (12 timesteps = 1 hour).
    - take_profit_factor: Fraction of the ATR value for take profit condition.

    Returns:
    - total_profit: Total profit or loss from all trades.
    - successful_trades: Number of successful trades.
    - detailed_stats: Dictionary with detailed stats for long and short trades.
    """

    total_profit = 0
    successful_trades = 0
    long_trades = 0
    short_trades = 0
    long_wins = 0
    short_wins = 0
    total_long_profit = 0
    total_short_profit = 0
    total_long_loss = 0
    total_short_loss = 0

    current_trade = None  # No active trade initially

    for i in range(1, len(y_pred_min)):
        # Get current price, ATR, and EMA difference values
        current_price = df.iloc[i]['close']
        atr = df.iloc[i][atr_column]
        ema_diff = df.iloc[i][ema_column]

        # Check if a new trade should be initiated
        if current_trade is None:
            # Long trade entry condition: y_pred_max > 3 * abs(y_pred_min) and ema10diff > 0
            if y_pred_max[i] > 2 * abs(y_pred_min[i]) and ema_diff > 0 and y_pred_min[i] > 0:
                take_profit_factor = abs(y_pred_max[i] * 0.8)
                entry_price = current_price
                take_profit_price = entry_price + take_profit_factor * atr  # For take profit
                stop_loss_price = entry_price - take_profit_factor * atr *2  # For stop loss

                # Set the current trade details for long trade
                current_trade = {
                    'type': 'long',
                    'entry_price': entry_price,
                    'take_profit_price': take_profit_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                long_trades += 1  # Increment long trade count

            # Short trade entry condition: abs(y_pred_min) > 3 * y_pred_max and ema10diff < 0
            elif abs(y_pred_min[i]) > 2 * y_pred_max[i] and ema_diff < 0 and y_pred_max[i] < 0:
                take_profit_factor = abs(y_pred_min[i] * 0.8)
                entry_price = current_price
                take_profit_price = entry_price - take_profit_factor * atr  # For take profit (for short, it's lower)
                stop_loss_price = entry_price + take_profit_factor * atr *2  # For stop loss (for short, it's higher)

                # Set the current trade details for short trade
                current_trade = {
                    'type': 'short',
                    'entry_price': entry_price,
                    'take_profit_price': take_profit_price,
                    'stop_loss_price': stop_loss_price,
                    'entry_idx': i,
                }
                short_trades += 1  # Increment short trade count

        # If we have an active trade, check the stop loss / take profit condition
        elif current_trade is not None:
            current_idx = i
            current_price = df.iloc[current_idx]['close']
            current_high = df.iloc[current_idx]['high']
            current_low = df.iloc[current_idx]['low']

            # Check if the take profit or stop loss condition is met
            if current_trade['type'] == 'long':
                # Long trade: Take profit when price goes up, stop loss when price goes down
                if current_price >= current_trade['take_profit_price'] or current_high >= current_trade['take_profit_price']:
                    # Take profit for long
                    profit = (current_trade['take_profit_price'] - current_trade['entry_price'])
                    total_profit += profit
                    total_long_profit += profit
                    successful_trades += 1
                    long_wins += 1  # Increment long wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price <= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for long
                    loss = (current_price - current_trade['entry_price'])
                    total_profit += loss
                    total_long_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

            elif current_trade['type'] == 'short':
                # Short trade: Take profit when price goes down, stop loss when price goes up
                if current_price <= current_trade['take_profit_price'] or current_low <= current_trade['take_profit_price']:
                    # Take profit for short
                    profit = (current_trade['entry_price'] - current_trade['take_profit_price'])
                    total_profit += profit
                    total_short_profit += profit
                    successful_trades += 1
                    short_wins += 1  # Increment short wins
                    current_trade = None  # Reset trade after closing
                    continue

                elif current_price >= current_trade['stop_loss_price'] or (current_idx - current_trade['entry_idx'] >= stop_loss_timesteps):
                    # Stop loss for short
                    loss = (current_trade['entry_price'] - current_price)
                    total_profit += loss
                    total_short_loss += loss
                    current_trade = None  # Reset trade after closing
                    continue

    # Calculate win rates
    long_winrate = (long_wins / long_trades) * 100 if long_trades > 0 else 0
    short_winrate = (short_wins / short_trades) * 100 if short_trades > 0 else 0

    # Convert NumPy arrays to single numeric values
    total_long_profit = total_long_profit.item() if isinstance(total_long_profit, np.ndarray) else total_long_profit
    total_short_profit = total_short_profit.item() if isinstance(total_short_profit, np.ndarray) else total_short_profit
    total_long_loss = total_long_loss.item() if isinstance(total_long_loss, np.ndarray) else total_long_loss
    total_short_loss = total_short_loss.item() if isinstance(total_short_loss, np.ndarray) else total_short_loss
    total_profit = total_profit.item() if isinstance(total_profit, np.ndarray) else total_profit

    # Prepare detailed stats dictionary
    detailed_stats = {
        'long_trades': long_trades,
        'short_trades': short_trades,
        'long_winrate': long_winrate,
        'short_winrate': short_winrate,
        'total_long_profit': total_long_profit,
        'total_short_profit': total_short_profit,
        'total_long_loss': total_long_loss,
        'total_short_loss': total_short_loss,
    }

    # Print detailed stats
    print(f"Total Long Trades: {long_trades}")
    print(f"Total Short Trades: {short_trades}")
    print(f"Long Trade Winrate: {long_winrate:.2f}%")
    print(f"Short Trade Winrate: {short_winrate:.2f}%")
    print(f"Total Long Profit: {total_long_profit:.4f}")
    print(f"Total Short Profit: {total_short_profit:.4f}")
    print(f"Total Long Loss: {total_long_loss:.4f}")
    print(f"Total Short Loss: {total_short_loss:.4f}")
    print(f"Total Profit from All Trades: {total_profit:.4f}")

    return total_profit, successful_trades, detailed_stats

"""# Retrieving Historical Data"""

import requests

apikey 'INSERT_API_KEY_HERE'

def getTimeSeries_AV(symbol, interval, month, adjusted=False, extended=False, output_size='full'):
    # month == YYYY-MM
    # interval - the following values are supported: 1min, 5min, 15min, 30min, 60min, daily, weekly, monthly

    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY' \
          f'&symbol={symbol}&interval={interval}&apikey={apikey}' \
          f'&adjusted={adjusted}&extended_hours={extended}&outputsize={output_size}'
    if month is not None:
        url += f'&month={month}'
    else:
        url += f'&entitlement=realtime'

    response = requests.get(url)
    print(response.status_code)

    print(url)
    # print('------------')
    # print(response.json())

    return response.json()


def getATR_AV(symbol, interval, time_period, month):

    url = f'https://www.alphavantage.co/query?function=ATR' \
          f'&symbol={symbol}&interval={interval}&time_period={time_period}' \
          f'&apikey={apikey}&outputsize=full'
    if month is not None:
        url += f'&month={month}'
    else:
        url += '&entitlement=realtime'

    response = requests.get(url)

    print(response.status_code)
    return response.json()

def combineDataToCSV_AV(symbol, interval, month, time_period, realtime=True, opt_tag='', output_size='full', filtered=True):
    """Performs API call to retrieve price and indicator values, combines
    all the data into a list and writes it out to a new CSV"""
    data_price = getTimeSeries_AV(symbol=symbol, interval=interval, month=month, output_size=output_size)

    # Initialize the combined list
    combined_list = []

    for sample_price in data_price[f'Time Series ({interval})']:
        combined_sample = {
            'datetime': sample_price,  # other fn's depend on datetime at loc[0], and close at loc[1]
            'close': data_price[f'Time Series ({interval})'][sample_price]['4. close'],
            'open': data_price[f'Time Series ({interval})'][sample_price]['1. open'],
            'high': data_price[f'Time Series ({interval})'][sample_price]['2. high'],
            'low': data_price[f'Time Series ({interval})'][sample_price]['3. low'],
            'vol': data_price[f'Time Series ({interval})'][sample_price]['5. volume'],
        }
        combined_list.append(combined_sample)
        # print(f'{sample_price}, {sample_obv}, {sample_rsi}, {sample_atr}, {sample_macd}')

    # puts list in chronological order (loc[0] is oldest, loc[i] is most recent
    combined_list = combined_list[::-1]

    # Create a DataFrame from the combined list
    df = pd.DataFrame(combined_list)

    # Specify the CSV file path (adjust as needed) # removed {optional} from end of filepath
    if realtime == True:
        filepath = f'live/current'
    else:
        filepath = f'data/{symbol}{interval}_basic{opt_tag}/{symbol}{interval}{time_period}_{month}'

    csv_file_path = f'/content/drive/MyDrive/NeuralNetworkTradeBot/{filepath}.csv'

    if filtered:
        # Convert the timestamp column to datetime
        df['datetime'] = pd.to_datetime(df['datetime'])
        # Filter rows between 09:30:00 and 16:00:00
        filtered_df = df[(df['datetime'].dt.time >= pd.to_datetime('09:30:00').time()) &
                        (df['datetime'].dt.time <= pd.to_datetime('16:00:00').time())]
        filtered_df.to_csv(csv_file_path, index=False)

    else:
        df.to_csv(csv_file_path, index=False)

    print(f"Data saved to {csv_file_path}")

def combineDailyATRToCSV_AV(symbol, interval, time_period, optional=None):

    data_atr = getATR_AV(symbol=symbol, interval=interval, time_period=time_period, month=None)

    # Initialize the combined list
    combined_list = []

    # Merge indicator values
    for sample_atr in data_atr['Technical Analysis: ATR']:
        combined_sample = {
            'datetime': sample_atr,
            'atr': data_atr['Technical Analysis: ATR'][sample_atr]['ATR']
        }
        combined_list.append(combined_sample)

    # puts list in chronological order (loc[0] is oldest, loc[i] is most recent
    combined_list = combined_list[::-1]

    # Create a DataFrame from the combined list
    df = pd.DataFrame(combined_list)

    filtered_df = df[df['datetime'] >= '2000-01-01']

    csv_file_path = f'/content/drive/MyDrive/NeuralNetworkTradeBot/live/SPYdailyATR.csv'

    # Write the DataFrame to the CSV file
    filtered_df.to_csv(csv_file_path, index=False)

    print(f"Data saved to {csv_file_path}")

rebuild_data = False
interval = '5min'
symbol = 'SPY'
opt_tag = ''  # '_2024'

if rebuild_data:
    combineDailyATRToCSV_AV(symbol='SPY', interval='daily', time_period=14)
    years = [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]
    months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']
    # years = [2024]
    # months = ['01', '02', '03', '04', '05', '06', '07', '08', '09']
    for yr in years:
        for m in months:
            combineDataToCSV_AV(symbol=symbol, interval=interval, month=f'{yr}-{m}', time_period=14, realtime=False, opt_tag=opt_tag)

    # combine csv files for dif months into single csv
    df_csv_append = pd.DataFrame()
    path = f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/{symbol}{interval}_basic{opt_tag}'
    dirs = os.listdir(path)
    for file in dirs:
        if file.endswith(".csv"):
            df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/{symbol}{interval}_basic{opt_tag}/{file}')
            df_csv_append = pd.concat([df_csv_append, df], ignore_index=True)
    df_csv_append.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/{symbol}{interval}_unadjusted_basic{opt_tag}.csv', index=False)

    # filter out extended hours data
    df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/{symbol}{interval}_unadjusted_basic{opt_tag}.csv')  # basicCombined.csv')
    # Convert the timestamp column to datetime
    df['datetime'] = pd.to_datetime(df['datetime'])
    # Filter rows between 09:30:00 and 16:00:00
    filtered_df = df[(df['datetime'].dt.time >= pd.to_datetime('09:30:00').time()) &
                    (df['datetime'].dt.time <= pd.to_datetime('16:00:00').time())]
    # Print the filtered DataFrame
    filtered_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/{symbol}{interval}_unadjusted_basic_filtered{opt_tag}.csv', index=False)

"""# Build Features for df w/ Patterns from raw OHLCV data"""

def build_features_for_df_w_patterns(df, save_filepath=None):
    """
    Takes in df, creates all features for it, then drops the na rows (~2 days)
    """
    # Calculate Technical Indicators
    df['obv'] = ta.OBV(df['close'], df['vol'])
    df['rsi'] = ta.RSI(df['close'], timeperiod=14)
    df['atr'] = ta.ATR(df['high'], df['low'], df['close'], timeperiod=14)
    df['macd'], df['macd_signal'], df['macd_hist'] = ta.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)
    df['ema_10'] = ta.EMA(df['close'], timeperiod=10)
    df['ema_200'] = ta.EMA(df['close'], timeperiod=200)
    df['ema_50'] = ta.EMA(df['close'], timeperiod=50)
    df['distance_to_ema_10'] = df['close'] - df['ema_10']
    df['distance_to_ema_200'] = df['close'] - df['ema_200']
    df['distance_to_ema_50'] = df['close'] - df['ema_50']
    df['parabolic_sar'] = ta.SAR(df['high'], df['low'], acceleration=0.02, maximum=0.2)
    df['cmf'] = ta.ADOSC(df['high'], df['low'], df['close'], df['vol'], fastperiod=3, slowperiod=10)

    # Candlestick Patterns
    df['doji'] = ta.CDLDOJI(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['hammer'] = ta.CDLHAMMER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['inverted_hammer'] = ta.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['shooting_star'] = ta.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['bullish_engulfing'] = ta.CDLENGULFING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x == 100 else 0)
    df['bearish_engulfing'] = ta.CDLENGULFING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x == -100 else 0)
    df['morning_star'] = ta.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['evening_star'] = ta.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['harami'] = ta.CDLHARAMI(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['dark_cloud_cover'] = ta.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['piercing_pattern'] = ta.CDLPIERCING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['three_white_soldiers'] = ta.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['three_black_crows'] = ta.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['marubozu'] = ta.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['spinning_top'] = ta.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)

    # Saty Phase Oscillator Indicators
    df['pivot'] = ta.EMA(df['close'], timeperiod=21)
    df['above_pivot'] = (df['close'] >= df['pivot']).astype(int)
    df['bband_up'] = df['pivot'] + 2.0 * ta.STDDEV(df['close'], timeperiod=21)
    df['bband_down'] = df['pivot'] - 2.0 * ta.STDDEV(df['close'], timeperiod=21)
    df['compression_threshold_up'] = df['pivot'] + (2.0 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['compression_threshold_down'] = df['pivot'] - (2.0 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['expansion_threshold_up'] = df['pivot'] + (1.854 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['expansion_threshold_down'] = df['pivot'] - (1.854 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['compression'] = np.where(df['above_pivot'], df['bband_up'] - df['compression_threshold_up'], df['compression_threshold_down'] - df['bband_down'])
    df['in_expansion_zone'] = np.where(df['above_pivot'], df['bband_up'] - df['expansion_threshold_up'], df['expansion_threshold_down'] - df['bband_down'])
    df['compression_tracker'] = np.where((df['compression'].shift(1) <= df['compression']) & (df['in_expansion_zone'] > 0), 0, np.where(df['compression'] <= 0, 1, 0))
    df['raw_signal'] = ((df['close'] - df['pivot']) / (3.0 * df['atr'])) * 100
    df['oscillator'] = ta.EMA(df['raw_signal'], timeperiod=3)

    # Differentials and Additional Features
    df['open_diff'] = df['open'].diff()
    df['high_diff'] = df['high'].diff()
    df['low_diff'] = df['low'].diff()
    df['close_diff'] = df['close'].diff()
    df['vol_diff'] = df['vol'].diff()
    df['obv_diff'] = df['obv'].diff()
    df['rsi_diff'] = df['rsi'].diff()
    df['macd_diff'] = df['macd'].diff()
    df['atr_diff'] = df['atr'].diff()
    df['parabolic_sar_diff'] = df['parabolic_sar'].diff()
    df['cmf_diff'] = df['cmf'].diff()
    df['ema_10_diff'] = df['ema_10'].diff()
    df['ema_200_diff'] = df['ema_200'].diff()
    df['ema_50_diff'] = df['ema_50'].diff()
    df['bband_up_diff'] = df['bband_up'].diff()
    df['bband_down_diff'] = df['bband_down'].diff()
    df['pivot_diff'] = df['pivot'].diff()
    df['compression_threshold_up_diff'] = df['compression_threshold_up'].diff()
    df['compression_threshold_down_diff'] = df['compression_threshold_down'].diff()
    df['expansion_threshold_up_diff'] = df['expansion_threshold_up'].diff()
    df['expansion_threshold_down_diff'] = df['expansion_threshold_down'].diff()
    df['distance_to_pivot'] = (df['close'] - df['pivot']) / df['atr']
    df['distance_to_bband_up'] = df['close'] - df['bband_up']
    df['distance_to_bband_down'] = df['close'] - df['bband_down']
    df['price_return'] = df['close'].pct_change(periods=12)
    df['vroc'] = df['vol'].pct_change(periods=12)
    df['distance_open_to_close'] = df['open'] - df['close']  # Calculate the distance of open from close
    df['distance_high_to_close'] = df['high'] - df['close']  # Calculate the distance of high from close
    df['distance_low_to_close'] = df['low'] - df['close']  # Calculate the distance of low from close

    # Extract the time and date from 'datetime'
    df['datetime'] = pd.to_datetime(df['datetime'])
    df['date'] = df['datetime'].dt.date
    df['time'] = df['datetime'].dt.time

    # Daily values for the previous day's high, low, close
    daily_values = df.groupby('date').agg(
        prev_day_high=('high', 'max'),
        prev_day_low=('low', 'min'),
        prev_day_close=('close', 'last')
    ).shift(1)
    df = df.merge(daily_values, how='left', on='date')

    # Calculate the week number and year for grouping
    df['year_week'] = df['datetime'].dt.strftime('%Y-%U')

    # Weekly values for last week's high, low, and close based on available data
    weekly_values = df.groupby('year_week').agg(
        last_week_high=('high', 'max'),
        last_week_low=('low', 'min'),
        last_week_close=('close', 'last')
    ).shift(1)  # Shift by 1 to reference the previous week
    df = df.merge(weekly_values, how='left', left_on='year_week', right_index=True)

    # Calculate Yesterday's Value Area High, Low, and VPOC
    daily_value_area = df.groupby('date').apply(lambda x: pd.Series({
        'yesterday_value_area_high': x['high'].quantile(0.7),
        'yesterday_value_area_low': x['low'].quantile(0.3),
        'yesterday_vpoc': x['close'].mode()[0]  # Most common close as proxy for VPOC
    })).shift(1)
    df = df.merge(daily_value_area, how='left', on='date')

    # Distance to previous day's high, low, weekly values, and value area
    df['distance_from_prev_day_high'] = df['close'] - df['prev_day_high']
    df['distance_from_prev_day_low'] = df['close'] - df['prev_day_low']
    df['distance_from_last_week_high'] = df['close'] - df['last_week_high']
    df['distance_from_last_week_low'] = df['close'] - df['last_week_low']
    df['distance_from_value_area_high'] = df['close'] - df['yesterday_value_area_high']
    df['distance_from_value_area_low'] = df['close'] - df['yesterday_value_area_low']
    df['distance_from_yesterday_vpoc'] = df['close'] - df['yesterday_vpoc']
    df['distance_from_yesterday_close'] = df['close'] - df['prev_day_close']
    df['distance_from_last_week_close'] = df['close'] - df['last_week_close']

    df = addFVGandATRFeatures(df) # add fvg signals and distances to atr lines and daily atr val
    df = add_relative_time_of_day(df) # add relative time of day

    windows = [3, 6, 12, 24]
    for window in windows:
        print(f'Finding patterns with window: {window}')
        df = detect_head_shoulder(df, window)
        df = calculate_support_resistance(df, window)
        df = detect_triangle_pattern(df, window)
        df = detect_wedge(df, window)
        df = detect_channel(df, window)
        df = detect_double_top_bottom(df, window)
        df = detect_trendline(df, window)  # before 12/5/24 this was overwriting sup/res created from 'calculate_support_resistance()'
        df = detect_sr_trendlines(df, window)

    # Drop date and time if no longer needed
    df.drop(columns=['date', 'year_week'], inplace=True)
    df.dropna(inplace=True)

    if save_filepath is not None:
        df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/{save_filepath}.csv', index=False)
        print(f'df saved to /content/drive/MyDrive/NeuralNetworkTradeBot/{save_filepath}.csv')

    return df

def build_features_for_df(df, save_filepath=None):
    """
    Takes in df, creates all features for it, then drops the na rows (~2 days)
    """
    # Calculate Technical Indicators
    df['obv'] = ta.OBV(df['close'], df['vol'])
    df['rsi'] = ta.RSI(df['close'], timeperiod=14)
    df['atr'] = ta.ATR(df['high'], df['low'], df['close'], timeperiod=14)
    df['macd'], df['macd_signal'], df['macd_hist'] = ta.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)
    df['ema_10'] = ta.EMA(df['close'], timeperiod=10)
    df['ema_200'] = ta.EMA(df['close'], timeperiod=200)
    df['ema_50'] = ta.EMA(df['close'], timeperiod=50)
    df['distance_to_ema_10'] = df['close'] - df['ema_10']
    df['distance_to_ema_200'] = df['close'] - df['ema_200']
    df['distance_to_ema_50'] = df['close'] - df['ema_50']
    df['parabolic_sar'] = ta.SAR(df['high'], df['low'], acceleration=0.02, maximum=0.2)
    df['cmf'] = ta.ADOSC(df['high'], df['low'], df['close'], df['vol'], fastperiod=3, slowperiod=10)

    # Candlestick Patterns
    df['doji'] = ta.CDLDOJI(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['hammer'] = ta.CDLHAMMER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['inverted_hammer'] = ta.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['shooting_star'] = ta.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['bullish_engulfing'] = ta.CDLENGULFING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x == 100 else 0)
    df['bearish_engulfing'] = ta.CDLENGULFING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x == -100 else 0)
    df['morning_star'] = ta.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['evening_star'] = ta.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['harami'] = ta.CDLHARAMI(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['dark_cloud_cover'] = ta.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['piercing_pattern'] = ta.CDLPIERCING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['three_white_soldiers'] = ta.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['three_black_crows'] = ta.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['marubozu'] = ta.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['spinning_top'] = ta.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)

    # Saty Phase Oscillator Indicators
    df['pivot'] = ta.EMA(df['close'], timeperiod=21)
    df['above_pivot'] = (df['close'] >= df['pivot']).astype(int)
    df['bband_up'] = df['pivot'] + 2.0 * ta.STDDEV(df['close'], timeperiod=21)
    df['bband_down'] = df['pivot'] - 2.0 * ta.STDDEV(df['close'], timeperiod=21)
    df['compression_threshold_up'] = df['pivot'] + (2.0 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['compression_threshold_down'] = df['pivot'] - (2.0 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['expansion_threshold_up'] = df['pivot'] + (1.854 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['expansion_threshold_down'] = df['pivot'] - (1.854 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['compression'] = np.where(df['above_pivot'], df['bband_up'] - df['compression_threshold_up'], df['compression_threshold_down'] - df['bband_down'])
    df['in_expansion_zone'] = np.where(df['above_pivot'], df['bband_up'] - df['expansion_threshold_up'], df['expansion_threshold_down'] - df['bband_down'])
    df['compression_tracker'] = np.where((df['compression'].shift(1) <= df['compression']) & (df['in_expansion_zone'] > 0), 0, np.where(df['compression'] <= 0, 1, 0))
    df['raw_signal'] = ((df['close'] - df['pivot']) / (3.0 * df['atr'])) * 100
    df['oscillator'] = ta.EMA(df['raw_signal'], timeperiod=3)

    # Differentials and Additional Features
    df['vol_diff'] = df['vol'].diff()
    df['obv_diff'] = df['obv'].diff()
    df['rsi_diff'] = df['rsi'].diff()
    df['macd_diff'] = df['macd'].diff()
    df['close_diff'] = df['close'].diff()
    df['atr_diff'] = df['atr'].diff()
    df['parabolic_sar_diff'] = df['parabolic_sar'].diff()
    df['cmf_diff'] = df['cmf'].diff()
    df['ema_10_diff'] = df['ema_10'].diff()
    df['ema_200_diff'] = df['ema_200'].diff()
    df['ema_50_diff'] = df['ema_50'].diff()
    df['distance_to_pivot'] = (df['close'] - df['pivot']) / df['atr']
    df['distance_to_bband_up'] = df['close'] - df['bband_up']
    df['distance_to_bband_down'] = df['close'] - df['bband_down']
    df['price_return'] = df['close'].pct_change(periods=12)
    df['vroc'] = df['vol'].pct_change(periods=12)
    df['distance_open_to_close'] = df['open'] - df['close']  # Calculate the distance of open from close
    df['distance_high_to_close'] = df['high'] - df['close']  # Calculate the distance of high from close
    df['distance_low_to_close'] = df['low'] - df['close']  # Calculate the distance of low from close

    # Extract the time and date from 'datetime'
    df['datetime'] = pd.to_datetime(df['datetime'])
    df['date'] = df['datetime'].dt.date
    df['time'] = df['datetime'].dt.time

    # Daily values for the previous day's high, low, close
    daily_values = df.groupby('date').agg(
        prev_day_high=('high', 'max'),
        prev_day_low=('low', 'min'),
        prev_day_close=('close', 'last')
    ).shift(1)
    df = df.merge(daily_values, how='left', on='date')

    # Calculate the week number and year for grouping
    df['year_week'] = df['datetime'].dt.strftime('%Y-%U')

    # Weekly values for last week's high, low, and close based on available data
    weekly_values = df.groupby('year_week').agg(
        last_week_high=('high', 'max'),
        last_week_low=('low', 'min'),
        last_week_close=('close', 'last')
    ).shift(1)  # Shift by 1 to reference the previous week
    df = df.merge(weekly_values, how='left', left_on='year_week', right_index=True)

    # Calculate Yesterday's Value Area High, Low, and VPOC
    daily_value_area = df.groupby('date').apply(lambda x: pd.Series({
        'yesterday_value_area_high': x['high'].quantile(0.7),
        'yesterday_value_area_low': x['low'].quantile(0.3),
        'yesterday_vpoc': x['close'].mode()[0]  # Most common close as proxy for VPOC
    })).shift(1)
    df = df.merge(daily_value_area, how='left', on='date')

    # Distance to previous day's high, low, weekly values, and value area
    df['distance_from_prev_day_high'] = df['close'] - df['prev_day_high']
    df['distance_from_prev_day_low'] = df['close'] - df['prev_day_low']
    df['distance_from_last_week_high'] = df['close'] - df['last_week_high']
    df['distance_from_last_week_low'] = df['close'] - df['last_week_low']
    df['distance_from_value_area_high'] = df['close'] - df['yesterday_value_area_high']
    df['distance_from_value_area_low'] = df['close'] - df['yesterday_value_area_low']
    df['distance_from_yesterday_vpoc'] = df['close'] - df['yesterday_vpoc']
    df['distance_from_yesterday_close'] = df['close'] - df['prev_day_close']
    df['distance_from_last_week_close'] = df['close'] - df['last_week_close']

    df = addFVGandATRFeatures(df) # add fvg signals and distances to atr lines and daily atr val
    df = add_relative_time_of_day(df) # add relative time of day

    # Drop date and time if no longer needed
    df.drop(columns=['date', 'year_week'], inplace=True)
    df.dropna(inplace=True)

    if save_filepath is not None:
        df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/{save_filepath}.csv', index=False)
        print(f'df saved to /content/drive/MyDrive/NeuralNetworkTradeBot/{save_filepath}.csv')

    return df

"""# Testing Chart Pattern Detection Algos"""

import pandas as pd
import plotly.graph_objects as go
import random
from scipy.signal import argrelextrema
import numpy as np

# Load and sort the dataframe
df = pd.read_csv('/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# Select a random slice of the dataframe containing 500 samples
start_idx = random.randint(0, len(df) - 1200)
df_slice = df.iloc[start_idx:start_idx + 1200].reset_index(drop=True)

df_slice = detect_sr_trendlines(df_slice, 12, num_extrema=3)

# Identify local minima and maxima with a window of 12
window = 12
close_values = df_slice['close'].values

# Find indices of local minima and maxima
minima_indices = argrelextrema(close_values, np.less_equal, order=window)[0]
maxima_indices = argrelextrema(close_values, np.greater_equal, order=window)[0]

# Add minima and maxima to the dataframe slice for plotting
df_slice['minima'] = np.nan
df_slice['maxima'] = np.nan
df_slice.loc[minima_indices, 'minima'] = df_slice['close'].iloc[minima_indices]
df_slice.loc[maxima_indices, 'maxima'] = df_slice['close'].iloc[maxima_indices]

# Create the candlestick chart
fig = go.Figure(data=[go.Candlestick(
    x=df_slice['datetime'],
    open=df_slice['open'],
    high=df_slice['high'],
    low=df_slice['low'],
    close=df_slice['close'],
    name='Candlesticks'
)])

# Add markers for head and shoulder patterns
head_shoulder_points = df_slice[df_slice['head_shoulder_pattern_12'] != 0]
fig.add_trace(go.Scatter(
    x=head_shoulder_points['datetime'],
    y=head_shoulder_points['close'],
    mode='markers',
    marker=dict(color='blue', size=8, opacity=0.5),
    name='Head & Shoulder Pattern'
))

# Add markers for wedge patterns
wedge_points = df_slice[df_slice['wedge_pattern_12'] != 0]
fig.add_trace(go.Scatter(
    x=wedge_points['datetime'],
    y=wedge_points['close'],
    mode='markers',
    marker=dict(color='red', size=8, opacity=0.5),
    name='Wedge Pattern'
))

# Plot support trendlines
fig.add_trace(go.Scatter(
    x=df_slice['datetime'],
    y=df_slice['support_trendline_12'],
    mode='lines',
    line=dict(color='green', dash='dot'),
    name='Support Trendline'
))

# Plot resistance trendlines
fig.add_trace(go.Scatter(
    x=df_slice['datetime'],
    y=df_slice['resistance_trendline_12'],
    mode='lines',
    line=dict(color='orange', dash='dot'),
    name='Resistance Trendline'
))

# Add markers for local minima
fig.add_trace(go.Scatter(
    x=df_slice['datetime'].iloc[minima_indices],
    y=df_slice['minima'].iloc[minima_indices],
    mode='markers',
    marker=dict(color='purple', size=10, symbol='triangle-down'),
    name='Local Minima'
))

# Add markers for local maxima
fig.add_trace(go.Scatter(
    x=df_slice['datetime'].iloc[maxima_indices],
    y=df_slice['maxima'].iloc[maxima_indices],
    mode='markers',
    marker=dict(color='darkorange', size=10, symbol='triangle-up'),
    name='Local Maxima'
))

# Update layout for better readability
fig.update_layout(
    title='Candlestick Chart with Patterns, Trendlines, and Local Extrema',
    xaxis_title='Date',
    yaxis_title='Price',
    xaxis_rangeslider_visible=False,
    template='plotly_white',
    legend=dict(
        x=0.01,
        y=0.99,
        bgcolor="rgba(0,0,0,0.3)",
        bordercolor="white",
        borderwidth=1
    ),
    width=1000,
    height=600
)

# Show the plot
fig.show()

"""# ---Building and Training Models---

# Creating & Augmenting Training Data (Deprecate)
"""

interval = '5min'
data_build_version = 2  # 1=norm transformer, 2=quantile transformer

# data_build_version 1 variables
num_classes = 3
class_labels=[0, 1, 2]
sell_target = 0
buy_target = 2
hold_target = 1
target_datr_percentage = (1 / 10)
cutoff_time = "15:15"
target_creation_method = 3
# Define your target time window (e.g., 09:30 to 10:00)
start_time = pd.to_datetime("10:00").time()
end_time = pd.to_datetime("12:00").time()

# transformer variables
# ff_dim = 1704 # test 1200
# head_size = 854  # test 400
# num_heads = 4
# num_blocks = 4
# mlp_units = 256

ff_dim = 1536
head_size = 768  # test 768
num_heads = 6
num_blocks = 2  # test 2
mlp_units = 128  # test 256

batch_size = 48
use_daily_atr = True
add_close_pred = True

# trading variables
look_ahead_period =  48
timesteps = 96  # this is the lookback period

retrain_encoder = False
encoder_name = f'{interval}encoder_10t12'
retrain_tm = True
trade_model_name = f'{interval}tradeModel_quantile_p3t6'

rebuild_df_with_features = False
load_df_with_features = False
shuffle_train_data = True

if rebuild_df_with_features:
    # Load the data
    # df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_basicCombinedFiltered.csv', parse_dates=['datetime'])
    df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered.csv', parse_dates=['datetime'])
    df.sort_values('datetime', inplace=True)

    # TODO: features to add to df...
    # atr lines (raw), prev day high (plus the same candle's close), prev day low (plus same candle's close)
    # distance to prev day high range, distance to prev day low range, is_in_prev_day_high_range, is_in_prev_day_low_range,
    # change 10ema to 13ema, change 50ema to 48ema,

    df = build_features_for_df(df, 'dff_Bv2')

elif load_df_with_features:
    df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_Bv2.csv', parse_dates=['datetime'])
    df.sort_values('datetime', inplace=True)

"""# *Train/Test Data Preparation* (constant, for model comparison)

IDEA!: since removing ohlc from scaled features resulted in poor learninng, identify a way to normalize values across each sampled slice of data, rather than normalize across the board
"""

# def create_data_and_labels_lstm(df, timesteps, look_ahead_period):
#     # turn df into 3d tensor and make training labels
#     # Features list (assuming the DataFrame contains columns like 'close', 'high', 'low', 'atr', etc.)
#     exclusion_list = ['datetime', 'time', 'target', 'year_week'] ###############
#                       # 'open', 'close', 'high', 'low', 'vol', 'ema_10', 'ema_50', 'ema_200',
#                       # 'bband_up', 'bband_down', 'pivot', 'compression_threshold_up',
#                       # 'compression_threshold_down', 'expansion_threshold_up', 'expansion_threshold_down',
#                       # 'parabolic_sar']
#     features = [col for col in df.columns if col not in exclusion_list]

#     # Feature Scaling
#     # scaler = StandardScaler()
#     # scaled_features = scaler.fit_transform(df[features])

#    # Prepare 3D tensor for LSTM input (X) and labels (y)
#     num_samples = len(df) - timesteps - look_ahead_period
#     X = np.empty((num_samples, timesteps, len(features)))  # Preallocate space for X
#     y = np.empty(num_samples)  # Preallocate space for y

#     # Access the values in a NumPy array for faster slicing
#     data_values = df[features].values  # Shape: [num_samples, num_features]

#     unscaled_flat_X = df.iloc[timesteps : len(df) - look_ahead_period]

#     # Loop over the dataset and create sequences
#     for i in range(timesteps, len(df) - look_ahead_period):
#         current_close = df.iloc[i]['close']
#         future_close = df.iloc[i + look_ahead_period]['close']
#         current_daily_atr = df.iloc[i]['daily_atr']

#         # Extract the features for the previous `timesteps` timesteps
#         current_window = data_values[i - timesteps:i]  # Use the pre-accessed NumPy array

#         # Normalize the features in the current window (relative to the prior timesteps)
#         scaler = StandardScaler()
#         scaled_window = scaler.fit_transform(current_window)

#         # Append the scaled window to X and the corresponding label to y
#         X[i - timesteps] = scaled_window  # Preallocated X array
#         y[i - timesteps] = (future_close - current_close) / current_daily_atr  # Normalized label

#     # Convert to NumPy arrays
#     # X = np.array(X) # resulting shape is [num samples, timesteps, num features]
#     # y = np.array(y)

#     return X, y, unscaled_flat_X

def create_data_and_labels_lstm(df, timesteps, look_ahead_period):
    # turn df into 3d tensor and make training labels
    # Features list (assuming the DataFrame contains columns like 'close', 'high', 'low', 'atr', etc.)
    exclusion_list = ['datetime', 'time', 'target', 'year_week', ###############
                      'open', 'close', 'high', 'low', 'vol', 'ema_10', 'ema_50', 'ema_200',
                      'bband_up', 'bband_down', 'pivot', 'compression_threshold_up',
                      'compression_threshold_down', 'expansion_threshold_up', 'expansion_threshold_down',
                      'parabolic_sar',
                      'open_diff', 'high_diff', 'low_diff',
                      'bband_up_diff', 'bband_low_diff', 'compression_threshold_up_diff', 'compression_threshold_down_diff',
                      'pivot_diff', 'expansion_threshold_up_diff', 'expansion_threshold_down_diff']
    features = [col for col in df.columns if col not in exclusion_list]

    # Feature Scaling
    scaler = StandardScaler()
    # scaler = MinMaxScaler()
    scaled_features = scaler.fit_transform(df[features])

    # Prepare 3D tensor for LSTM input (X) and labels (y)
    X = [] # initial shape [num samples, num features]
    y = []
    unscaled_flat_X = df.iloc[timesteps : len(df) - look_ahead_period]

    # Loop over the dataset and create sequences
    for i in range(timesteps, len(df) - look_ahead_period):
        current_close = df.iloc[i]['close']
        future_close = df.iloc[i + look_ahead_period]['close']
        current_daily_atr = df.iloc[i]['daily_atr']

        # Extract the features for the previous timesteps timesteps
        # current_window = df.iloc[i - timesteps:i][features]

        # Normalize the features in the current window (relative to the prior timesteps)
        # Create a StandardScaler instance and fit it to the data within the sliding window
        # scaler = StandardScaler()
        # scaled_window = scaler.fit_transform(current_window)

        scaled_window = scaled_features[i - timesteps:i]

        # Append the scaled window to X and the corresponding label to y
        X.append(scaled_window)  # Shape: [timesteps, num_features]
        y.append((future_close - current_close) / current_daily_atr)  # Normalized label

    # Convert to NumPy arrays
    X = np.array(X) # resulting shape is [num samples, timesteps, num features]
    y = np.array(y)

    return X, y, unscaled_flat_X

def create_sparse_data_and_labels_lstm(df, timesteps, look_ahead_period):
    # turn df into 3d tensor and make training labels
    # Features list (assuming the DataFrame contains columns like 'close', 'high', 'low', 'atr', etc.)
    usage_list = ['close_diff', 'vol_diff',
                  'daily_atr', 'distance_high_to_close', 'distance_low_to_close',
                  'distance_open_to_close']
    features = [col for col in df.columns if col in usage_list]

    # Feature Scaling
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(df[features])

    # Prepare 3D tensor for LSTM input (X) and labels (y)
    X = [] # initial shape [num samples, num features]
    y = []
    unscaled_flat_X = df.iloc[timesteps : len(df) - look_ahead_period]

    # Loop over the dataset and create sequences
    for i in range(timesteps, len(df) - look_ahead_period):
        current_close = df.iloc[i]['close']
        future_close = df.iloc[i + look_ahead_period]['close']
        current_daily_atr = df.iloc[i]['daily_atr']

        scaled_window = scaled_features[i - timesteps:i]

        # Append the scaled window to X and the corresponding label to y
        X.append(scaled_window)  # Shape: [timesteps, num_features]
        y.append((future_close - current_close) / current_daily_atr)  # Normalized label

    # Convert to NumPy arrays
    X = np.array(X) # resulting shape is [num samples, timesteps, num features]
    y = np.array(y)

    return X, y, unscaled_flat_X

def create_data_and_extrema_labels(df, timesteps, look_ahead_period, order=6):
    # turn df into 3d tensor and make training labels
    # Features list (assuming the DataFrame contains columns like 'close', 'high', 'low', 'atr', etc.)
    exclusion_list = ['datetime', 'time', 'target', 'year_week', ###############
                      'open', 'close', 'high', 'low', 'vol', 'ema_10', 'ema_50', 'ema_200',
                      'bband_up', 'bband_down', 'pivot', 'compression_threshold_up',
                      'compression_threshold_down', 'expansion_threshold_up', 'expansion_threshold_down',
                      'parabolic_sar',
                      'open_diff', 'high_diff', 'low_diff',
                      'bband_up_diff', 'bband_low_diff', 'compression_threshold_up_diff', 'compression_threshold_down_diff',
                      'pivot_diff', 'expansion_threshold_up_diff', 'expansion_threshold_down_diff']
    features = [col for col in df.columns if col not in exclusion_list]

    # Feature Scaling
    scaler = StandardScaler()
    # scaler = MinMaxScaler()
    scaled_features = scaler.fit_transform(df[features])

    # Prepare 3D tensor for LSTM input (X) and labels (y)
    X = [] # initial shape [num samples, num features]
    y = []
    y2 = []
    z = []
    unscaled_flat_X = df.iloc[timesteps : len(df) - look_ahead_period]
    close_prices = df['close'].values
    local_minima = argrelextrema(close_prices, np.less, order=order)[0]  # Local minima
    local_maxima = argrelextrema(close_prices, np.greater, order=order)[0]  # Local maxima

    # Loop over the dataset and create sequences
    for i in range(timesteps, len(df) - look_ahead_period):
        current_close = df.iloc[i]['close']
        # future_close = df.iloc[i + look_ahead_period]['close']
        current_daily_atr = df.iloc[i]['daily_atr']

        scaled_window = scaled_features[i - timesteps:i]

        # Append the scaled window to X and the corresponding label to y
        X.append(scaled_window)  # Shape: [timesteps, num_features]
        # y.append((future_close - current_close) / current_daily_atr)  # Normalized label

        # Use the close price to determine local minima, maxima, and neutral points
        # close_prices = df['close'].iloc[i - order * 2:i + look_ahead_period * 2]  # look at the window of interest
        # Use scipy's argrelextrema to find local minima and maxima
        # local_minima = argrelextrema(close_prices.values, np.less, order=order)[0]  # Local minima
        # local_maxima = argrelextrema(close_prices.values, np.greater, order=order)[0]  # Local maxima

        # Label the current point based on its position relative to local minima and maxima
        if i in local_minima:
            y2.append(0)  # Local minima
        elif i in local_maxima:
            y2.append(2)  # Local maxima
        else:
            y2.append(1)  # Neutral

         # Combine both local minima and local maxima
        all_extrema = np.concatenate((local_minima, local_maxima))
        all_extrema_sorted = all_extrema[all_extrema > i]  # Filter extrema that occur after current index i

        if len(all_extrema_sorted) > 0:
            next_extrema = all_extrema_sorted[0]  # The closest extrema (minima or maxima)
            next_extrema_price = df['close'].iloc[next_extrema]

            # Calculate the distance to the next extrema (minima or maxima), normalized by ATR
            distance_to_extrema = (next_extrema_price - current_close) / current_daily_atr
            z.append(distance_to_extrema)
        else:
            # If no extrema is found, set distance to 0 (or other default value)
            z.append(0)

    # Convert to NumPy arrays
    X = np.array(X) # resulting shape is [num samples, timesteps, num features]
    # y = np.array(y)
    y2 = np.array(y2)
    z = np.array(z)
    y2_onehot = tf.keras.utils.to_categorical(y2, num_classes=3)


    return X, y2_onehot, z, unscaled_flat_X  #, y

def create_data_and_extrema_labels2(df, timesteps, look_ahead_period, order=6):
    # Exclusion list (columns not to be used for features)
    exclusion_list = ['datetime', 'time', 'target', 'year_week', 'open', 'close', 'high', 'low', 'vol',
                      'ema_10', 'ema_50', 'ema_200', 'bband_up', 'bband_down', 'pivot', 'compression_threshold_up',
                      'compression_threshold_down', 'expansion_threshold_up', 'expansion_threshold_down', 'parabolic_sar',
                      'open_diff', 'high_diff', 'low_diff',
                      'bband_up_diff', 'bband_low_diff', 'compression_threshold_up_diff', 'compression_threshold_down_diff',
                      'pivot_diff', 'expansion_threshold_up_diff', 'expansion_threshold_down_diff']
    features = [col for col in df.columns if col not in exclusion_list]

    # Feature Scaling (StandardScaler)
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(df[features])

    # Prepare 3D tensor for LSTM input (X) and labels (z1, z2)
    X = []  # Shape [num samples, timesteps, num features]
    z1 = []  # Label for next minima
    z2 = []  # Label for next maxima
    unscaled_flat_X = df.iloc[timesteps : len(df) - look_ahead_period]
    close_prices = df['close'].values

    # Find local minima and maxima
    local_minima = argrelextrema(close_prices, np.less, order=order)[0]  # Local minima indices
    local_maxima = argrelextrema(close_prices, np.greater, order=order)[0]  # Local maxima indices

    # Loop over the dataset and create sequences
    for i in range(timesteps, len(df) - look_ahead_period):
        current_close = df.iloc[i]['close']
        current_daily_atr = df.iloc[i]['daily_atr']

        scaled_window = scaled_features[i - timesteps:i]

        # Append the scaled window to X
        X.append(scaled_window)  # Shape: [timesteps, num_features]

        # Find the closest minima after current index
        next_minima = local_minima[local_minima > i] if len(local_minima[local_minima > i]) > 0 else []
        if next_minima.size > 0:
            next_minima_idx = next_minima[0]  # The first minima after current index
            next_minima_price = df['close'].iloc[next_minima_idx]
            z1_value = (next_minima_price - current_close) / current_daily_atr  # Distance normalized by ATR
        else:
            z1_value = 0  # No minima found after current index

        # Find the closest maxima after current index
        next_maxima = local_maxima[local_maxima > i] if len(local_maxima[local_maxima > i]) > 0 else []
        if next_maxima.size > 0:
            next_maxima_idx = next_maxima[0]  # The first maxima after current index
            next_maxima_price = df['close'].iloc[next_maxima_idx]
            z2_value = (next_maxima_price - current_close) / current_daily_atr  # Distance normalized by ATR
        else:
            z2_value = 0  # No maxima found after current index

        # Append both z1 (next minima) and z2 (next maxima) to their respective lists
        z1.append(z1_value)
        z2.append(z2_value)

    # Convert to NumPy arrays
    X = np.array(X)  # Shape: [num samples, timesteps, num features]
    z1 = np.array(z1)
    z2 = np.array(z2)

    return X, z1, z2, unscaled_flat_X

# hyperparameters
interval = '5min'
timesteps = 72
look_ahead_period = 12

epochs = 100
batch_size = 6
learning_rate = 0.0001

use_2024 = True
shuf = False

data_label_version = 4
# 1: simple close full feature set
# 2: extrema classification + distext
# 3: distext*2
# 4: simple close minimal features (OHLCV)

# load data
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# create df w features
# NOTE: models trained before 12/3/24 have -3 features [open_diff, high_diff, and low_diff]
# df = build_features_for_df_w_patterns(df, 'dff_w_patterns')

if use_2024 is not True:
    df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns.csv', parse_dates=['datetime'])
    df.sort_values('datetime', inplace=True)
else:
    df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
    df.sort_values('datetime', inplace=True)

if data_label_version == 1:
    # make 3d tensor and labels
    X, y, unscaled_flat_X = create_data_and_labels_lstm(df, timesteps, look_ahead_period)

    # Train-test split (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

elif data_label_version == 2:
    # make 3d tensor and labels
    X, y, z, unscaled_flat_X = create_data_and_extrema_labels(df, timesteps, look_ahead_period)

    X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, z, test_size=0.2, shuffle=True)

elif data_label_version == 3:
    # make 3d tensor and labels
    X, z1, z2, unscaled_flat_X = create_data_and_extrema_labels2(df, timesteps, look_ahead_period)

    X_train, X_test, z1_train, z1_test, z2_train, z2_test = train_test_split(X, z1, z2, test_size=0.2, shuffle=True)

elif data_label_version == 4:
    # make 3d tensor and labels
    X, y, unscaled_flat_X = create_sparse_data_and_labels_lstm(df, timesteps, look_ahead_period)

    # Train-test split (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=shuf)


print(f"X_train shape: {X_train.shape}")
# print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
# print(f"y_test shape: {y_test.shape}")

"""# Side-by-Side of LSTM, CNN, and Transformer Performance"""

df2 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
df2.sort_values('datetime', inplace=True)
for col in df.columns:
    if col not in df2.columns:
        print(f'Column {col} not in df2')

"""Make sure the fully featured data set is prepared before running the next cell."""

model_names = ['simpleLSTM_close_v5',
               'simpleCNN_close_v3',
               'transformer_close_v14'
               ]
for model_name in model_names:
    # Load the trained model
    model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras', safe_mode=False)
    # Evaluate the model
    train_loss = model.evaluate(X_train, y_train)
    print(f"Train Loss for {model_name}: {train_loss}")
    test_loss = model.evaluate(X_test, y_test)
    print(f"Test Loss for {model_name}: {test_loss}")

"""Make sure the sparsely featured data set is prepared before running the next cell."""

model_names = ['simpleLSTM_sparse_close_v1',
               'simpleCNN_sparse_close_v1',
               'transformer_sparse_close_v1'
               ]
for model_name in model_names:
    # Load the trained model
    model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras', safe_mode=False)
    # Evaluate the model
    train_loss = model.evaluate(X_train, y_train)
    print(f"Train Loss for {model_name}: {train_loss}")
    test_loss = model.evaluate(X_test, y_test)
    print(f"Test Loss for {model_name}: {test_loss}")

mean_daily_atr = unscaled_flat_X['daily_atr'].mean()
print(f"Mean value of 'daily_atr': {mean_daily_atr}")

print(f'The LSTM model trained on the fully featured data set has an MSE loss of: 0.0024')
print(f'The square root of our MSE multiplied by the daily_atr, gives us the mean error in terms of stock price.')
avg_err = (0.0024 ** 0.5) * mean_daily_atr
print(f'(0.0024 ** 0.5) * 4.0024 == {avg_err}')
print(f'This tell us the model predictions were off by an average of ${round(avg_err, 2)} on the training data.')

print(f'The CNN model trained on the fully featured data set has an MSE loss of: 0.0028')
print(f'The square root of our MSE multiplied by the daily_atr, gives us the mean error in terms of stock price.')
avg_err = (0.0028 ** 0.5) * mean_daily_atr
print(f'(0.0028 ** 0.5) * 4.0024 == {avg_err}')
print(f'This tell us the model predictions were off by an average of ${round(avg_err, 2)} on the training data.')

print(f'The Transformer (TF) model trained on the fully featured data set has an MSE loss of: 0.021')
print(f'The square root of our MSE multiplied by the daily_atr, gives us the mean error in terms of stock price.')
avg_err = (0.021 ** 0.5) * mean_daily_atr
print(f'(0.021 ** 0.5) * 4.0024 == {avg_err}')
print(f'This tell us the model predictions were off by an average of ${round(avg_err, 2)} on the training data.')

trade_model_name = 'simpleLSTM_extrema_v2'
history1 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
trade_model_name = 'simpleCNN_extrema_v1'
history2 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
transformer_name = 'transformer_extrema_v2'
history3 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv')
print(transformer_name)
plot_training_history(history1, history2, history3)

"""# Simple Linear Regression - Close"""

from sklearn.linear_model import LinearRegression

# Function to reshape 3D data into 2D for linear regression
def reshape_data_for_linear_regression(X):
    # Reshape (num_samples, timesteps, num_features) -> (num_samples, timesteps * num_features)
    num_samples, timesteps, num_features = X.shape
    return X.reshape(num_samples, timesteps * num_features)

# Create the Linear Regression model
def train_linear_regression(X_train, y_train):
    # Reshape X_train
    X_train_flat = reshape_data_for_linear_regression(X_train)

    # Initialize the model
    model = LinearRegression()

    # Train the model
    model.fit(X_train_flat, y_train)

    # Make predictions on the training data
    y_pred_train = model.predict(X_train_flat)

    # Evaluate the model
    mse = mean_squared_error(y_train, y_pred_train)
    r2 = r2_score(y_train, y_pred_train)

    print("Training Mean Squared Error:", mse)
    print("Training R^2 Score:", r2)

    return model


# Train the model
model = train_linear_regression(X_train, y_train)

"""# Simple LSTM - Close"""

def build_simple_lstm_model(input_shape, num_classes):
    # Build the LSTM model
    model = tf.keras.models.Sequential()
    # Input Layer
    model.add(tf.keras.layers.Input(shape=input_shape))
    # LSTM Layer
    model.add(tf.keras.layers.LSTM(units=320, return_sequences=False))
    # Dropout for regularization (optional, to avoid overfitting)
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Dense(units=128, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Dense(units=num_classes, activation='linear'))  # Predicting a single value

    return model

"""Compile and fit model..."""

trade_model_name = 'simpleLSTM_close_v5'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=trade_model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    input_shape = (X_train.shape[1], X_train.shape[2])
    # build model
    model = build_simple_lstm_model(input_shape=input_shape, num_classes=1)

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, y_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, y_test),
                            callbacks=[save_callback, early_stopping, lr_plateau]
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv', index=False)

        # # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')

"""Visualize the training history..."""

trade_model_name = 'simpleLSTM_close_v5'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
plot_training_history(history)

model = tf.keras.models.load_model('/content/drive/MyDrive/NeuralNetworkTradeBot/models/simpleLSTM_close_v5.keras')
# Use 1000 samples as background data
background_data = shap.sample(X_train, 10)
explainer = shap.KernelExplainer(model.predict, background_data)
# X_test_reshaped = X_test.reshape(X_test.shape[0], -1)
shap_values = explainer.shap_values(background_data)
shap.summary_plot(shap_values[0], X_test_reshaped)

"""Visualizing close price predictions on OHLC candlestick chart, as well as evaluating a basic trading strategy using the model's predictions."""

# load model
model = tf.keras.models.load_model('/content/drive/MyDrive/NeuralNetworkTradeBot/models/simpleLSTM_close_v5.keras')
timesteps = 72
look_ahead_period = 12

# load data (2024)
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered_2024.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# create df w features
# df = build_features_for_df_w_patterns(df, 'dff_w_patterns_2024')
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# make 3d tensor and labels
X, y, unscaled_flat_X = create_data_and_labels_lstm(df, timesteps, look_ahead_period)

print(f'X shape: {X.shape}')
print(f'y shape: {y.shape}')

# get model predictions
y_pred = model.predict(X)

print(f'y_pred shape: {y_pred.shape}')
y_pred_flat = y_pred.flatten()
print(f'y_pred_flat shape: {y_pred_flat.shape}')

# Run the evaluation
total_profit, successful_trades, _ = evaluate_simple_close_trading_strategy(X, y_pred, unscaled_flat_X)

# Create a scatter plot comparing the true values to the predicted ones
plot_prediction_scatter(y, y_pred_flat, "Sample Model Predictions")

# Split into chunks for plotting purposes
chunks = split_into_chunks(X, y_pred, unscaled_flat_X, chunk_size=400)

j = 0
# Plot each chunk using a for loop
for X_chunk, y_pred_chunk, df_chunk in chunks:
    print(f'X chunk shape: ', X_chunk.shape)
    print(f'y_pred_chunk shape: ', y_pred_chunk.shape)
    print(f'df_chunk shape: ', df_chunk.shape)
    print(f'df_chunk[close].values shape: ', np.array(df_chunk['close'].values.shape))
    plot_ohlc_w_simple_close_pred(X_chunk, y_pred_chunk, df_chunk)
    j += 1
    if j > 10:
        break

"""Now that we have 'simpleLSTM_close_v1' which predicts the closing price for 12 timesteps ahead (1hr), we can categorize each prediction into [Strong Sell, Sell, Hold, Buy, Strong Buy].

Stacking a simple linear NN on top of our existing model allows us to predict which category we are currently in, and receive a confidence level for each category.

First we calculate the percentiles of the predictions in order to guide the creation of our class labels...
"""

"""
Cell Requirements:
y_pred -or- model and X_test
"""
if y_pred is not None:
    y_pred = model.predict(X_test)

# Assuming y_pred is a numpy array or pandas Series
mean_y_pred = np.mean(y_pred)
std_y_pred = np.std(y_pred)

print(f"Mean of y_pred: {mean_y_pred}")
print(f"Standard Deviation of y_pred: {std_y_pred}")

# Calculate percentiles (e.g., 25th, 50th, 75th)
percentiles = np.percentile(y_pred, [12, 25, 50, 75, 88])
print(f"12th percentile: {percentiles[0]}")
print(f"25th percentile: {percentiles[1]}")
print(f"50th percentile (Median): {percentiles[2]}")
print(f"75th percentile: {percentiles[3]}")
print(f"88th percentile: {percentiles[4]}")

# Plot histogram and overlay normal distribution
plt.figure(figsize=(8, 6))
sns.histplot(y_pred, kde=True, stat='density', color='blue', bins=30)

# Overlay the normal distribution based on the mean and std dev of y_pred
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x, mean_y_pred, std_y_pred)
plt.plot(x, p, 'k', linewidth=2)

plt.title('Distribution of y_pred with Normal Distribution Overlay')
plt.xlabel('y_pred values')
plt.ylabel('Density')
plt.show()

"""Preparing stacked classifier model..."""

# hyperparameters
model = tf.keras.models.load_model('/content/drive/MyDrive/NeuralNetworkTradeBot/models/simpleLSTM_close_v5.keras',
                                   safe_mode=False)
interval = '5min'
timesteps = 72
st_timesteps = 36
look_ahead_period = 12
st_model_name = 'simpleLSTM_stacked_classifier_v5'
retrain = True

# load data
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# create df w features
# df = build_features_for_df_w_patterns(df, 'dff_w_patterns')
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns.csv', parse_dates=['datetime'])
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# make 3d tensor and labels
X, y, unscaled_flat_X = create_data_and_labels_lstm(df, timesteps, look_ahead_period)

y_supervised = []
up_threshold = 0.1
down_threshold = 0.1
for val in y.flatten():
    if val > 2 * up_threshold:
        y_supervised.append(4)
    elif val > up_threshold:
        y_supervised.append(3)
    elif val < -2 * down_threshold:
        y_supervised.append(0)
    elif val < -down_threshold:
        y_supervised.append(1)
    else:
        y_supervised.append(2)

y_supervised = np.array(y_supervised)
y_supervised_onehot = tf.keras.utils.to_categorical(y_supervised, num_classes=5)

y_pred = model.predict(X)
print(f'y_pred shape: ', y_pred.shape)

"""Compile and fit stacked model..."""

# create X-combined (X + y_pred)
retrain = False
X_combined = np.concatenate([X[:,-1,:], y_pred], axis=1)
print(f'X_combined shape: ', X_combined.shape)
print(f'y_supervised_onehot shape: ', y_supervised_onehot.shape)
print(f'y_pred shape: ', y_pred.shape)
print(f'X shape: ', X.shape)

X_combined_lstm = []
y_supervised_onehot_lstm = []
y_supervised_lstm = []

for i in range(st_timesteps, len(X_combined) - look_ahead_period):
    X_combined_lstm.append(X_combined[i-st_timesteps:i])
    y_supervised_onehot_lstm.append(y_supervised_onehot[i])
    y_supervised_lstm.append(y_supervised[i])

X_combined_lstm = np.array(X_combined_lstm)
y_supervised_onehot_lstm = np.array(y_supervised_onehot_lstm)
y_supervised_lstm = np.array(y_supervised_lstm)
print(f'X_combined_lstm shape: ', X_combined_lstm.shape)
print(f'y_supervised_lstm shape: ', y_supervised_lstm.shape)

if retrain:

    # Train-test split (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X_combined_lstm, y_supervised_onehot_lstm, test_size=0.2, shuffle=True)

    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=st_model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )


    input_shape = (X_train.shape[1], X_train.shape[2])

    # Define a new LSTM stacked on top of the inital LSTM
    st_model = tf.keras.models.Sequential()
    st_model.add(tf.keras.layers.LSTM(320, return_sequences=False,
                                      input_shape=input_shape, activation='relu'))  # Add some fully connected layers
    st_model.add(tf.keras.layers.Dropout(0.1))
    st_model.add(tf.keras.layers.Dense(128, activation='relu'))
    st_model.add(tf.keras.layers.Dense(5, activation='softmax'))  # Assuming 5 classes for classification

    # Compile the model - use categorical_crossentropy if y is onehot encoded
    st_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', f1_score])

    # Train the model on the labeled data
    with tf.device('/device:GPU:0'):
        history = st_model.fit(X_train, y_train,
                                epochs=100, batch_size=6,
                                validation_data=(X_test, y_test),
                                callbacks=[save_callback, early_stopping, lr_plateau]
                                )
    st_model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{st_model_name}.keras')

    # Convert history dictionary to a pandas DataFrame
    history_df = pd.DataFrame(history.history)

    # # Save history to a CSV file
    history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{st_model_name}_history.csv', index=False)

"""Visualizing the performance of the stacked model..."""

st_model_name = 'simpleLSTM_stacked_classifier_v5'
st_model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{st_model_name}.keras',
                                   custom_objects={'f1_score': f1_score}, safe_mode=False)
# load 2024 data

# get predictions

# combine data
visualize_model_performance(st_model, X_combined_lstm, y_supervised_lstm)

"""# Simple LSTM - Close (6 features vs 100+)"""

trade_model_name = 'simpleLSTM_sparse_close_v1'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=trade_model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    input_shape = (X_train.shape[1], X_train.shape[2])
    # build model
    model = build_simple_lstm_model(input_shape=input_shape, num_classes=1)

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, y_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, y_test),
                            callbacks=[save_callback, early_stopping, lr_plateau]
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv', index=False)

        # # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')

trade_model_name = 'simpleLSTM_sparse_close_v1'
history1 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
trade_model_name = 'simpleLSTM_close_v5'
history2 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
plot_training_history(history1, history2)

trade_model_name = 'simpleLSTM_sparse_close_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')
y_pred = model.predict(X_train)
y_pred_flat = y_pred.flatten()
plot_prediction_scatter(y_train, y_pred_flat, "Training Data Model Performance")

trade_model_name = 'simpleLSTM_sparse_close_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')
y_pred = model.predict(X_train)
y_pred_flat = y_pred.flatten()
plot_prediction_scatter(y_train, y_pred_flat, "Testing (Unseen) Data Model Performance")

# load model
model = tf.keras.models.load_model('/content/drive/MyDrive/NeuralNetworkTradeBot/models/simpleLSTM_sparse_close_v1.keras')
timesteps = 72
look_ahead_period = 12

y_pred = model.predict(X)

# Split into chunks for plotting purposes
chunks = split_into_chunks(X, y_pred, unscaled_flat_X, chunk_size=400)

j = 0
# Plot each chunk using a for loop
for X_chunk, y_pred_chunk, df_chunk in chunks:
    print(f'X chunk shape: ', X_chunk.shape)
    print(f'y_pred_chunk shape: ', y_pred_chunk.shape)
    print(f'df_chunk shape: ', df_chunk.shape)
    print(f'df_chunk[close].values shape: ', np.array(df_chunk['close'].values.shape))
    plot_ohlc_w_simple_close_pred(X_chunk, y_pred_chunk, df_chunk)
    j += 1
    if j > 10:
        break

# load model
model = tf.keras.models.load_model('/content/drive/MyDrive/NeuralNetworkTradeBot/models/simpleLSTM_sparse_close_v1.keras')
timesteps = 72
look_ahead_period = 12

y_pred = model.predict(X)

# Split into chunks for plotting purposes
chunks = split_into_chunks(X, y_pred, unscaled_flat_X, chunk_size=400)

j = 0
# Plot each chunk using a for loop
for X_chunk, y_pred_chunk, df_chunk in chunks:
    print(f'X chunk shape: ', X_chunk.shape)
    print(f'y_pred_chunk shape: ', y_pred_chunk.shape)
    print(f'df_chunk shape: ', df_chunk.shape)
    print(f'df_chunk[close].values shape: ', np.array(df_chunk['close'].values.shape))
    plot_ohlc_w_simple_close_pred(X_chunk, y_pred_chunk, df_chunk)
    j += 1
    if j > 10:
        break

"""# Simple LSTM - Extrema"""

model_name = 'simpleLSTM_extrema_v2'
# Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=model_name)
optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',   # Typically use val_loss or another metric to monitor
    factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
    patience=5,           # Wait 5 epochs before reducing LR
    min_lr=1e-6,          # Minimum LR to avoid too small values
    verbose=1             # Print learning rate updates
)

y_train_class_indices = np.argmax(y_train, axis=-1)  # Convert from one-hot to integer labels

# Calculate class weights using sklearn's compute_class_weight
class_weights = compute_class_weight(
    class_weight='balanced',  # 'balanced' computes weights inversely proportional to class frequencies
    classes=np.unique(y_train_class_indices),  # All unique class labels
    y=y_train_class_indices  # The true class labels
)

# Map each class index to its computed weight
class_weights_dict = dict(zip(np.unique(y_train_class_indices), class_weights))

input_shape = (X_train.shape[1], X_train.shape[2])

# Define a new LSTM stacked on top of the inital LSTM
st_model = tf.keras.models.Sequential()
st_model.add(tf.keras.layers.LSTM(320, return_sequences=False,
                                  input_shape=input_shape, activation='relu'))  # Add some fully connected layers
st_model.add(tf.keras.layers.Dropout(0.1))
st_model.add(tf.keras.layers.Dense(128, activation='relu'))
st_model.add(tf.keras.layers.Dense(3, activation='softmax'))

# Per-class precision
precision_class_0 = tf.keras.metrics.Precision(name='precision_0', class_id=0)
precision_class_1 = tf.keras.metrics.Precision(name='precision_1', class_id=1)
precision_class_2 = tf.keras.metrics.Precision(name='precision_2', class_id=2)

# Compile the model - use categorical_crossentropy if y is onehot encoded
st_model.compile(optimizer=optimizer, loss='categorical_crossentropy',
                 metrics=['accuracy', f1_score_multiclass,
                          precision_class_0, precision_class_1, precision_class_2])

# Train the model on the labeled data
with tf.device('/device:GPU:0'):
    history = st_model.fit(X_train, y_train,
                           epochs=100, batch_size=6,
                           validation_data=(X_test, y_test),
                           callbacks=[save_callback, early_stopping, lr_plateau],
                           class_weight=class_weights_dict
                           )
st_model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

# Convert history dictionary to a pandas DataFrame
history_df = pd.DataFrame(history.history)

# # Save history to a CSV file
history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{model_name}_history.csv', index=False)

# Count the number of positives (1s) in each class column
# print first 100 values of y_train
# print(y_train[:100])
# print(close_prices)
# print(y_train)
positive_counts = np.sum(y_train, axis=0)
# Print the number of positives for each class
print(f"Number of positives for each class: {positive_counts}")

trade_model_name = 'simpleLSTM_extrema_v2'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
plot_training_history(history)

"""# Simple LSTM - DistExt"""

trade_model_name = 'simpleLSTM_distext_v1'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=trade_model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    input_shape = (X_train.shape[1], X_train.shape[2])
    # build model
    model = build_simple_lstm_model(input_shape=input_shape, num_classes=1)

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, z_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, z_test),
                            callbacks=[save_callback, early_stopping, lr_plateau]
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv', index=False)

        # # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')

# load model
model_name = 'simpleLSTM_distext_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')
timesteps = 72
look_ahead_period = 12

# load data (2024)
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered_2024.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# create df w features
# df = build_features_for_df_w_patterns(df, 'dff_w_patterns_2024')
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# make 3d tensor and labels
X, y, z, unscaled_flat_X = create_data_and_extrema_labels(df, timesteps, look_ahead_period)

# get model predictions
y_pred = model.predict(X)

# Run the evaluation
# total_profit, successful_trades, _ = evaluate_simple_close_trading_strategy(X, y_pred, unscaled_flat_X)

# Split into chunks for plotting purposes
chunks = split_into_chunks(X, y_pred, unscaled_flat_X, chunk_size=400)

j = 0
# Plot each chunk using a for loop
for X_chunk, y_pred_chunk, df_chunk in chunks:
    print(f'X chunk shape: ', X_chunk.shape)
    print(f'y_pred_chunk shape: ', y_pred_chunk.shape)
    print(f'df_chunk shape: ', df_chunk.shape)
    print(f'df_chunk[close].values shape: ', np.array(df_chunk['close'].values.shape))
    plot_ohlc_w_simple_close_pred(X_chunk, y_pred_chunk, df_chunk)
    j += 1
    if j > 10:
        break

"""# Simple LSTM - DistExt (2 outputs)"""

def build_lstm_distext_model(input_shape):
    # Build the LSTM model with an explicit Input layer
    inputs = tf.keras.Input(shape=input_shape)

    # LSTM Layer
    x = tf.keras.layers.LSTM(units=320, return_sequences=False)(inputs)

    # Dropout for regularization
    x = tf.keras.layers.Dropout(0.1)(x)

    # Dense layer
    x = tf.keras.layers.Dense(units=128, activation='relu')(x)

    # Dropout again for regularization
    x = tf.keras.layers.Dropout(0.1)(x)

    # Output layers for z1 (minima) and z2 (maxima)
    z1_output = tf.keras.layers.Dense(1, name='z1_output')(x)  # Output for z1 (next minima)
    z2_output = tf.keras.layers.Dense(1, name='z2_output')(x)  # Output for z2 (next maxima)

    # Define the model with input and outputs
    model = tf.keras.Model(inputs=inputs, outputs=[z1_output, z2_output])

    return model

trade_model_name = 'simpleLSTM_distext2_v1'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=trade_model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    input_shape = (X_train.shape[1], X_train.shape[2])
    # build model
    model = build_lstm_distext_model(input_shape=input_shape)

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics={
                      'z1_output': 'mae',  # MAE for z1_output (next minima)
                      'z2_output': 'mae'   # MAE for z2_output (next maxima)
                  })

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train,
                            {'z1_output': z1_train, 'z2_output': z2_train},
                            epochs=100, batch_size=6,
                            validation_data=(X_test, {'z1_output': z1_test, 'z2_output': z2_test}),
                            callbacks=[save_callback, early_stopping, lr_plateau]
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv', index=False)

        # # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')

# load model
model_name = 'simpleLSTM_distext2_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')
timesteps = 72
look_ahead_period = 12

# load data (2024)
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered_2024.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# create df w features
# df = build_features_for_df_w_patterns(df, 'dff_w_patterns_2024')
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# make 3d tensor and labels
X, y, z, unscaled_flat_X = create_data_and_extrema_labels2(df, timesteps, look_ahead_period)

# get model predictions
y_pred = model.predict(X)

y_pred_min = np.array(y_pred)[0]
y_pred_max = np.array(y_pred)[1]

# Run the evaluation
# total_profit, successful_trades, _ = evaluate_simple_close_trading_strategy(X, y_pred, unscaled_flat_X)

evaluate_distext_strategy1(X, y_pred_min, y_pred_max, unscaled_flat_X)

# Split into chunks for plotting purposes
chunks = split_into_chunks(X, y_pred_min, unscaled_flat_X, y_pred2=y_pred_max, chunk_size=400)

j = 0
# Plot each chunk using a for loop
for X_chunk, y_pred_min_chunk, y_pred_max_chunk, df_chunk in chunks:
    print(f'X chunk shape: ', X_chunk.shape)
    print(f'y_pred_min_chunk shape: ', y_pred_min_chunk.shape)
    print(f'y_pred_max_chunk shape: ', y_pred_max_chunk.shape)
    print(f'df_chunk shape: ', df_chunk.shape)
    print(f'df_chunk[close].values shape: ', np.array(df_chunk['close'].values.shape))
    # plot_ohlc_w_simple_close_pred(X_chunk, y_pred_chunk, df_chunk)
    plot_ohlc_distext_pred(X_chunk, y_pred_min_chunk, y_pred_max_chunk, df_chunk)
    j += 1
    if j > 10:
        break

"""# DQN - Reinforcement Learning"""

class TradingEnv:
    def __init__(self, df, scaled_df, modelA, atr_column='daily_atr', max_trade_length=12):
        self.df = df
        self.scaled_df = scaled_df
        self.modelA = modelA
        self.atr_column = atr_column
        self.max_trade_length = max_trade_length  # Maximum trade length in timesteps
        self.reset()

    def preprocess_state(self, state):
        # Ensure each component is a 1D array or consistent shape
        processed_state = []
        for element in state:
            if isinstance(element, np.ndarray):
                # Flatten arrays if they have more than 1 dimension
                processed_state.append(element.flatten())
            else:
                # Convert scalar values to 1D arrays
                processed_state.append(np.array([element]))
        return np.concatenate(processed_state)  # Combine all components into a single 1D array

    def reset(self):
        self.account_value = 100000  # starting account value
        self.position_size = 0  # No position initially
        self.entry_price = 0
        self.in_active_long = 0
        self.in_active_short = 0
        self.cash = self.account_value
        self.current_step = 0
        self.done = False
        self.trade_duration = 0  # Initialize trade duration
        self.state_history = []  # Add state history to store the last 24 states
        self.previous_state = self._get_state()
        self._add_to_state_history(self.previous_state)  # Add the first state to the history
        return self.previous_state

    def _get_state(self):
        # Get the current state as before
        price = self.df.iloc[self.current_step]['close']
        # print(f'scaled_df shape: {self.scaled_df.shape}  --  _get_state()')
        cstep = self.scaled_df[self.current_step:self.current_step+1]
        # print(f'cstep shape: {cstep.shape}  --  _get_state()')
        # predicted_min = self.modelA.predict(cstep)[0]  # Assuming modelA outputs predicted minima
        # predicted_max = self.modelA.predict(cstep)[1]  # Assuming modelA outputs predicted maxima
        cstep_data = self.scaled_df[self.current_step, -1, :]
        # print(f'cstep_data shape: {cstep_data.shape}  --  _get_state()')
        # print(f'current step: {self.current_step}, price: {price}, predicted_min: {predicted_min}, predicted_max: {predicted_max}')
        state  = [self.account_value, self.in_active_long, self.in_active_short,
                  self.cash, self.position_size, self.trade_duration,]
                  # predicted_min, predicted_max, ]
        for item in cstep_data.flatten():
            state.append(item)
        state = self.preprocess_state(state)
        return state

    def _add_to_state_history(self, state):
        # Add the state to the history, maintaining only the last 24 states
        self.state_history.append(state)
        if len(self.state_history) > 24:
            self.state_history.pop(0)  # Keep only the last 24 states

    def step(self, action):
        # Perform the action as before
        price = self.df.iloc[self.current_step]['close']
        reward = 0
        self.done = False

        if self.in_active_long:
            current_position = 'Long'
        elif self.in_active_short:
            current_position = 'Short'
        else:
            current_position = 'None'

        print(f'ACTION CHOSEN: {action}, CURRENT_POS: {current_position}, ACCT_VAL: {self.account_value}, PRICE: {price}')

        if action == 0:  # Hold
            if not self.in_active_long and not self.in_active_short:
                self.cash -= self.cash * 0.000128 / 12
            pass
        elif action == 1:  # Enter long
            if self.cash >= price:
                self.position_size = (self.cash / 2) // price
                self.cash -= self.position_size * price
                self.in_active_long = 1
                self.trade_duration = 0  # Start counting trade duration
                self.entry_price = price
        elif action == 2:  # Enter short
            if self.cash >= price:
                self.position_size = (self.cash / 2) // price
                self.cash -= self.position_size * price
                self.in_active_short = 1
                self.trade_duration = 0  # Start counting trade duration
                self.entry_price = price
        elif action == 3:  # Exit long
            if self.in_active_long:
                self.cash += self.position_size * price
                self.position_size = 0
                self.in_active_long = 0
        elif action == 4:  # Exit short
            if self.in_active_short:
                self.cash += self.position_size * (self.entry_price * 2 - price)
                self.position_size = 0
                self.in_active_short = 0

        short_price = self.entry_price * 2 - price
        # Update account value
        if self.in_active_long:
            self.account_value = self.cash + (self.position_size * price)  # Value of long position
        elif self.in_active_short:
            self.account_value = self.cash + (self.position_size * short_price)  # Value of short position (it'll be negative)
        else:
            self.account_value = self.cash  # If no position, just cash

        # Reward based on profit/loss
        reward = self.account_value - self.previous_state[0]  # Change in account value

        # Set done if account value goes below zero
        if self.account_value <= 75000:
            self.done = True

        # Set done once end of data is reached
        if self.current_step == len(self.df) - 100:
            self.done = True

        # If in a trade, update the trade duration
        if self.in_active_long or self.in_active_short:
            self.trade_duration += 1

        # If trade duration exceeds max trade length, force exit
        if self.trade_duration >= self.max_trade_length:
            if self.in_active_long:
                self.cash += self.position_size * price
                self.position_size = 0
                self.in_active_long = 0
            elif self.in_active_short:
                self.cash += self.position_size * (self.entry_price * 2 - price)
                self.position_size = 0
                self.in_active_short = 0

        # Add the new state to the state history
        self.previous_state = self._get_state()
        self._add_to_state_history(self.previous_state)

        self.current_step += 1

        return self.state_history, reward, self.done

from collections import deque

class ExperienceReplayBuffer:
    def __init__(self, max_size=10000):
        self.buffer = deque(maxlen=max_size)

    def add(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def size(self):
        return len(self.buffer)

class DQNAgent:
    def __init__(self, state_size, action_size, batch_size=12, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update_freq=100):
        # Parameters
        self.state_size = state_size  # In your case, state_size would be 9 (features per state)
        self.action_size = action_size  # Number of possible actions
        self.batch_size = batch_size
        self.gamma = gamma  # Discount factor for future rewards
        self.epsilon = epsilon_start  # Exploration rate
        self.epsilon_min = epsilon_end  # Minimum exploration rate
        self.epsilon_decay = epsilon_decay  # Decay factor for epsilon
        self.target_update_freq = target_update_freq  # Frequency of target model updates

        # Experience replay buffer
        self.replay_buffer = ExperienceReplayBuffer()

        # Optimizer
        self.optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001)

        # Neural network models
        self.model = self.build_model((24, state_size), action_size)  # Input shape should be (24, state_size)
        self.target_model = self.build_model((24, state_size), action_size)
        self.target_model.set_weights(self.model.get_weights())  # Initialize target model weights with model weights

        # Step counter for target model updates
        self.steps = 0

    def build_model(self, input_shape, action_size):
        # Example model, can use LSTM or Dense layers, here we use a simple Dense model
        model = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=input_shape),
            tf.keras.layers.LSTM(150, activation='tanh', return_sequences=False),  # Example LSTM layer
            tf.keras.layers.Dropout(0.1),
            tf.keras.layers.Dense(150, activation='relu'),
            tf.keras.layers.Dense(action_size, activation='linear')  # Output layer with action_size outputs (Q-values)
        ])
        model.compile(optimizer=self.optimizer, loss='mse')
        return model

    def choose_action(self, state_hist):
        # Predict Q-values for each action based on the state sequence
        print(f'state_hist shape: {state_hist.shape}')
        q_values = self.model.predict(state_hist, verbose=0)  # Predict Q-values

        state = state_hist[-1][-1]
        print(f'state shape: {state.shape}')
        # state = np.expand_dims(state, axis=0)  # Reshape to (1, state_size)

        # Ensure valid actions based on the current state (no active position for entering, and position exists for exiting)
        valid_actions = [0]  # Always allow "Hold" action

        # If no active position, allow entering long or short
        if not state[1] and not state[2]:
            valid_actions.extend([1, 2])  # Allow long and short positions (1 = long, 2 = short)

        # If there is an active long position, allow exiting long
        if state[1]:
            valid_actions.append(3)  # Allow exiting long position (3)

        # If there is an active short position, allow exiting short
        if state[2]:
            valid_actions.append(4)  # Allow exiting short position (4)

        # If exploration is chosen, pick a random valid action
        if np.random.rand() <= self.epsilon:
            return np.random.choice(valid_actions)  # Random action from valid actions

        # If exploitation is chosen, select the best valid action
        # Set all Q-values for invalid actions to a very low value so they won't be chosen
        for i in range(self.action_size):
            if i not in valid_actions:
                q_values[0][i] = -np.inf  # Invalidate this action by assigning a very low value

        # Return the index of the action with the highest Q-value from the valid actions
        return np.argmax(q_values[0])

    def replay(self):
        # Train the model using a batch of experiences from the replay buffer
        if len(self.replay_buffer.buffer) < self.batch_size:
            return  # If not enough experiences to sample, return early

        # Sample a batch of experiences
        mini_batch = self.replay_buffer.sample(self.batch_size)

        # Prepare the data
        states, actions, rewards, next_states, dones = zip(*mini_batch)
        states = np.array(states)  # Shape (batch_size, 24, state_size)
        next_states = np.array(next_states)  # Shape (batch_size, 24, state_size)

        # If the extra dimension is present, remove it (assuming the 1 is the second dimension)
        if states.shape[1] == 1:
            states = np.squeeze(states, axis=1)  # Shape (batch_size, 24, state_size)

        if next_states.shape[1] == 1:
            next_states = np.squeeze(next_states, axis=1)  # Shape (batch_size, 24, state_size)


        print(f'states shape: {states.shape}  -- inside replay()')
        print(f'next_states shape: {next_states.shape}  -- inside replay()')

        # Predict the Q-values for the current states and next states
        q_values = self.model.predict(states)
        next_q_values = self.target_model.predict(next_states)

        print('predicting completed.')

        # Update the Q-values for the sampled batch
        for i in range(self.batch_size):
            if dones[i]:  # If episode is done, no future reward (TD target = reward)
                q_values[i][actions[i]] = rewards[i]
            else:  # If not done, calculate TD target with future discounted Q-values
                q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])

        # Train the model on the batch
        self.model.fit(states, q_values, epochs=1, verbose=0)

        print('single epoch trained.')

    def train(self):
        # Call replay and update the target model
        self.replay()

        # Update epsilon for exploration-exploitation balance
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

        # Update target model weights every `target_update_freq` steps
        if self.steps % self.target_update_freq == 0:
            self.update_target_model()

        self.steps += 1

    def update_target_model(self):
        # Copy weights from the main model to the target model
        self.target_model.set_weights(self.model.get_weights())

model_name = 'simpleLSTM_distext2_v1'
modelA = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras', safe_mode=False)

tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow logs

# state size = 9 or 143
# Initialize environment and DQN agent
with tf.device('/device:GPU:0'):
    env = TradingEnv(unscaled_flat_X, X, modelA)  # df is your market data, modelA provides predictions
    agent = DQNAgent(state_size=143, action_size=5, target_update_freq=100)  # 8 states, 5 actions

# Assuming the Agent and Environment classes are already defined

def train_dqn(agent, env, num_episodes=1000, batch_size=32, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):
    # Initialize epsilon for exploration (though agent will handle this now)
    rewards = []  # To store rewards during training

    # Train over multiple episodes
    for episode in range(num_episodes):
        state = env.reset()  # Reset the environment at the beginning of each episode
        # state = np.expand_dims(state, axis=0)  # Add a batch dimension
        total_reward = 0
        done = False
        step = 0
        print(f'Current episode: {episode}')

        # Run the agent in the environment
        while not done:
            while step < 24:
                action = 0
                next_state, reward, done = env.step(action)
                # next_state = np.array(next_state)  # Convert to a NumPy array
                next_state = np.expand_dims(next_state, axis=0)  # Add batch dimension
                # agent.replay_buffer.add((state, action, reward, next_state, done))
                state = next_state
                step += 1

            print(f'Current step: {step}')
            step += 1
            # Select action using epsilon-greedy strategy
            action = agent.choose_action(state)

            # Step in the environment
            next_state, reward, done = env.step(action)
            next_state = np.expand_dims(next_state, axis=0)  # Add batch dimension

            # Add experience to the replay buffer
            agent.replay_buffer.add((state, action, reward, next_state, done))

            # Train the agent (perform replay and update target model)
            agent.train()

            # Update the current state and accumulate reward for this episode
            state = next_state
            total_reward += reward

        # Print the reward every 100 episodes for monitoring
        if episode % 1 == 0:
            agent.model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/trained_dqn_model_episode_{episode}.keras')
            print(f"Episode {episode}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.4f}")

        rewards.append(total_reward)  # Record reward for this episode

    # Save the trained agent's model after training
    agent.model.save('/content/drive/MyDrive/NeuralNetworkTradeBot/models/trained_dqn_model.keras')
    print("Training finished. Model saved.")

    return rewards  # Return list of rewards for later analysis


# Example of how to initialize the environment and agent:
# env = YourEnvironmentClass()  # Replace with your actual environment
# agent = DQNAgent(state_size=env.state_space, action_size=env.action_space)

# Call the training function with the environment and agent:
with tf.device('/device:GPU:0'):
    rewards = train_dqn(agent, env, num_episodes=1000)

# clear keras backend
tf.keras.backend.clear_session()
# tf.keras.python.keras.backend.clear_session()

"""# Simple CNN (supervised) - Close"""

# Example 2nd CNN layer including L2 Regularization as well as Layer Normalization
# model.add(Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(l2_rate)))
# model.add(LayerNormalization())
# model.add(MaxPooling1D(pool_size=2))

def get_cnn_model(encoder_name, retrain,
                                 X_train, X_test, y_train, y_test,
                                 patience, batch_size, n_dense_units=128):
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=encoder_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    if retrain:
        # Define the CNN Encoder model
        input_layer = tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))  # window_size * # of inputs
        print(f'input_layer = {input_layer}')

        # Encoder (convolutional layers for feature extraction)
        x = tf.keras.layers.Conv1D(filters=320, kernel_size=3, activation='relu', padding='same')(input_layer)
        x = tf.keras.layers.AveragePooling1D(pool_size=2, padding='same')(x)

        x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.AveragePooling1D(pool_size=2, padding='same')(x)

        x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)
        encoded = tf.keras.layers.AveragePooling1D(pool_size=2, padding='same', name='encoded_layer')(x)

        # Flatten the output of the encoder to pass into dense layers
        encoded_flattened = tf.keras.layers.Flatten()(encoded)

        # Dense layers for supervised training (classification or regression)
        dense = tf.keras.layers.Dense(256, activation='relu')(encoded_flattened)
        dense = tf.keras.layers.Dropout(0.1)(dense)

        dense = tf.keras.layers.Dense(512, activation='relu')(dense)
        dense = tf.keras.layers.Dropout(0.1)(dense)

        dense = tf.keras.layers.Dense(140, activation='relu')(dense)
        output_layer = tf.keras.layers.Dense(1, activation='linear')(dense)  # Change activation as needed (softmax for multi-class classification)

        # Compile the model
        model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)
        model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

        model.summary()

        # Train the model
        with tf.device('/device:GPU:0'):
            history = model.fit(X_train, y_train,
                      epochs=100, batch_size=batch_size,
                      validation_data=(X_test, y_test),
                      callbacks=[early_stopping, save_callback, lr_plateau])

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{encoder_name}_history.csv', index=False)

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{encoder_name}.keras')

    else:
        # Load the pre-trained model
        model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{encoder_name}.keras')

    print('Model summary:')
    model.summary()

    # Create the encoder model (extracts the features before the dense layers)
    encoder = tf.keras.models.Model(model.input, model.get_layer('encoded_layer').output)

    return encoder, model

"""Build, compile, and fit model..."""

cnn_model_name = 'simpleCNN_close_v3'
retrain = True

encoder, model = get_cnn_model(encoder_name=cnn_model_name, retrain=retrain, X_train=X_train, X_test=X_test,
                             y_train=y_train, y_test=y_test, patience=10, batch_size=6)

cnn_model_name = 'simpleCNN_close_v3'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{cnn_model_name}_history.csv')
print(cnn_model_name)
plot_training_history(history)

# visualize training history

# visualize close price preds, and evaluate simple close trading strat

# create stacked lstm model (prep, compile, fit)

"""# Simple CNN - Close (Sparse)"""

cnn_model_name = 'simpleCNN_sparse_close_v1'
retrain = True

encoder, model = get_cnn_model(encoder_name=cnn_model_name, retrain=retrain, X_train=X_train, X_test=X_test,
                             y_train=y_train, y_test=y_test, patience=10, batch_size=6)

cnn_model_name = 'simpleCNN_sparse_close_v1'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{cnn_model_name}_history.csv')
print(cnn_model_name)
plot_training_history(history)

trade_model_name = 'simpleCNN_sparse_close_v1'
history1 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
trade_model_name = 'simpleCNN_close_v3'
history2 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
plot_training_history(history1, history2)

trade_model_name = 'simpleCNN_sparse_close_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')
y_pred = model.predict(X_train)
y_pred_flat = y_pred.flatten()
plot_prediction_scatter(y_train, y_pred_flat, "CNN - Training Data Performance")

trade_model_name = 'simpleCNN_sparse_close_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')
y_pred = model.predict(X_train)
y_pred_flat = y_pred.flatten()
plot_prediction_scatter(y_train, y_pred_flat, "CNN - Testing (Unseen) Data Performance")

"""# Simple CNN - Extrema"""

# Example 2nd CNN layer including L2 Regularization as well as Layer Normalization
# model.add(Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=l2(l2_rate)))
# model.add(LayerNormalization())
# model.add(MaxPooling1D(pool_size=2))

def get_cnn_extrema_model(encoder_name, retrain,
                                 X_train, X_test, y_train, y_test,
                                 patience, batch_size, n_dense_units=128):
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=encoder_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    if retrain:
        # Define the CNN Encoder model
        input_layer = tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))  # window_size * # of inputs
        print(f'input_layer = {input_layer}')

        # Encoder (convolutional layers for feature extraction)
        x = tf.keras.layers.Conv1D(filters=320, kernel_size=3, activation='relu', padding='same')(input_layer)
        x = tf.keras.layers.AveragePooling1D(pool_size=2, padding='same')(x)

        x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.AveragePooling1D(pool_size=2, padding='same')(x)

        x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)
        encoded = tf.keras.layers.AveragePooling1D(pool_size=2, padding='same', name='encoded_layer')(x)

        # Flatten the output of the encoder to pass into dense layers
        encoded_flattened = tf.keras.layers.Flatten()(encoded)

        # Dense layers for supervised training (classification or regression)
        dense = tf.keras.layers.Dense(256, activation='relu')(encoded_flattened)
        dense = tf.keras.layers.Dropout(0.1)(dense)

        dense = tf.keras.layers.Dense(512, activation='relu')(dense)
        dense = tf.keras.layers.Dropout(0.1)(dense)

        dense = tf.keras.layers.Dense(140, activation='relu')(dense)
        output_layer = tf.keras.layers.Dense(3, activation='softmax')(dense)  # Change activation as needed (softmax for multi-class classification)

        y_train_class_indices = np.argmax(y_train, axis=-1)  # Convert from one-hot to integer labels

        # Calculate class weights using sklearn's compute_class_weight
        class_weights = compute_class_weight(
            class_weight='balanced',  # 'balanced' computes weights inversely proportional to class frequencies
            classes=np.unique(y_train_class_indices),  # All unique class labels
            y=y_train_class_indices  # The true class labels
        )

        # Map each class index to its computed weight
        class_weights_dict = dict(zip(np.unique(y_train_class_indices), class_weights))

        # Per-class precision
        precision_class_0 = tf.keras.metrics.Precision(name='precision_0', class_id=0)
        precision_class_1 = tf.keras.metrics.Precision(name='precision_1', class_id=1)
        precision_class_2 = tf.keras.metrics.Precision(name='precision_2', class_id=2)

        # Compile the model
        model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)
        model.compile(optimizer=optimizer, loss='categorical_crossentropy',
                      metrics=['accuracy', f1_score_multiclass,
                      precision_class_0, precision_class_1, precision_class_2])

        model.summary()

        # Train the model
        with tf.device('/device:GPU:0'):
            history = model.fit(X_train, y_train,
                                epochs=100, batch_size=batch_size,
                                validation_data=(X_test, y_test),
                                callbacks=[early_stopping, save_callback, lr_plateau],
                                class_weight=class_weights_dict
                                )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{encoder_name}_history.csv', index=False)

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{encoder_name}.keras')

    else:
        # Load the pre-trained model
        model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{encoder_name}.keras')

    print('Model summary:')
    model.summary()

    # Create the encoder model (extracts the features before the dense layers)
    encoder = tf.keras.models.Model(model.input, model.get_layer('encoded_layer').output)

    return encoder, model

cnn_model_name = 'simpleCNN_extrema_v1'
retrain = True

encoder, model = get_cnn_extrema_model(encoder_name=cnn_model_name, retrain=retrain, X_train=X_train, X_test=X_test,
                                       y_train=y_train, y_test=y_test, patience=10, batch_size=1)

trade_model_name = 'simpleCNN_extrema_v1'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
plot_training_history(history)

"""# CNN Auto-Encoder (Unsupervised Pre-Training)

TODO: retrain autoencoder using **masked** data. This should prompt it to learn stronger patterns.
"""

def get_encoder(encoder_name, retrain, X_train, X_test, patience, batch_size):
    # TODO: create tuner and test dif hyperparameters (num_filters, num_filters2, thirdLayer, num_filters3, etc.)
    # encoder_name = 'encoderD_3layers'
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=encoder_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    if retrain:
        # Define the CNN Autoencoder model
        input_layer = tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))  # window_size * # of inputs
        print(f'input_layer = {input_layer}')

        # Encoder
        x = tf.keras.layers.Conv1D(filters=320, kernel_size=3, activation='relu', padding='same')(input_layer)
        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)
        x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)
        x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)
        encoded = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same', name='encoded_layer')(x)

        # Decoder
        x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(encoded)
        x = tf.keras.layers.UpSampling1D(size=2)(x)
        x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling1D(size=2)(x)
        x = tf.keras.layers.Conv1D(filters=320, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.UpSampling1D(size=2)(x)
        decoded = tf.keras.layers.Conv1D(filters=X_train.shape[2], kernel_size=3, activation='sigmoid', padding='same')(x)

        optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-5)

        # Compile the model
        autoencoder = tf.keras.models.Model(input_layer, decoded)
        autoencoder.compile(optimizer=optimizer, loss='mse')

        autoencoder.summary()

        # Train the autoencoder
        with tf.device('/device:GPU:0'):
            history = autoencoder.fit(X_train, X_train,
                            epochs=100, batch_size=batch_size,
                            validation_data=(X_test, X_test),
                            callbacks=[early_stopping, save_callback, lr_plateau])
            autoencoder.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{encoder_name}.keras')
        ae_model = autoencoder

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{encoder_name}_history.csv', index=False)


    else:
        # # Create/Load the encoder model
        ae_model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{encoder_name}.keras')

    print('ae model summary: ')
    ae_model.summary()
    encoder = tf.keras.models.Model(ae_model.input, ae_model.get_layer('encoded_layer').output)

    # encoder = Model(input_layer, encoded)

    return encoder

encoder_name = 'cnn_autoencoder_v1'
get_encoder(encoder_name=encoder_name, retrain=True,
            X_train=X_train, X_test=X_test, patience=10, batch_size=1)

"""Now that we have a CNN that has been partially trained in an unsupervised fashion, we can supervise the last bit of it's training by combining the encoder's output back into the input, then passing that through another linear layer that is trained to predict the upcoming price change."""

def get_supervised_encoder_model(model_name, retrain, encoder,
                                 X_train, X_test, y_train, y_test,
                                 patience, batch_size, n_dense_units=128):

    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )
    if retrain:
        # Define input layer with input shape based on X_train
        input_layer = tf.keras.layers.Input(shape=X_train.shape[1:])  # Automatically infer the shape from X_train

        # Pass the input through the encoder (which should output a fixed size)
        encoder_output = encoder(input_layer)

        # Combine the encoded output back into the initial input
        input_and_encoded = tf.keras.layers.Add()([encoder_output, input_layer])

        # TODO: compare feeding encoder_output, vs. input_and_encoded

        # Now create the supervised layers on top of the encoder
        x = tf.keras.layers.Dense(256, activation='relu')(encoder_output)
        x = tf.keras.layers.Dropout(0.1)(x)
        x = tf.keras.layers.Dense(512, activation='relu')(x)
        x = tf.keras.layers.Dropout(0.1)(x)
        x = tf.keras.layers.Dense(140, activation='relu')(x)
        x = tf.keras.layers.Dense(1, activation='linear')(x)

        # Define the complete model
        supervised_model = tf.keras.models.Model(inputs=input_layer, outputs=x)

        # Compile the model
        supervised_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])
        supervised_model.summary()

        # Train the model
        with tf.device('/device:GPU:0'):
            history = supervised_model.fit(X_train, y_train,
                                          epochs=100, batch_size=batch_size,
                                          validation_data=(X_test, y_test),
                                          callbacks=[early_stopping, save_callback, lr_plateau])

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{model_name}_history.csv', index=False)


        # Save the trained model
        supervised_model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

    else:
        # Load the pre-trained model
        supervised_model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

    print('Model summary:')
    supervised_model.summary()

    return supervised_model

# get encoder
encoder_name = 'cnn_autoencoder_v1_epoch_30'
encoder = get_encoder(encoder_name=encoder_name, retrain=False,
                      X_train=X_train, X_test=X_test, patience=10, batch_size=1)

# Make the encoder layers trainable (if you want to fine-tune)
for layer in encoder.layers:
    layer.trainable = True

supervised_model_name = 'cnn_autoencoder_close_v1'
supervised_model = get_supervised_encoder_model(model_name=supervised_model_name,
                                                retrain=True, encoder=encoder,
                                                X_train=X_train, X_test=X_test,
                                                y_train=y_train, y_test=y_test,
                                                patience=10, batch_size=1)

"""# CNN Auto-Encoder (UPT) + LSTM

Here is one of the things I am most excited to test. I hypothezize that since a convolutional neural network (CNN) is capable of identifying features/patterns in images containing 3 dimensions (x, y, color), it should be able to identify features/patterns in 'images' of stock data containing 6+ dimensions (open, high, low, close, volume, relative time, etc.).
**I am going to test stacking an LSTM on top of my CNN, which should be able to show us whether a CNN is any more capable of identifying notable patterns compared to an LSTM model by itself. **
"""

def get_encoder_lstm_model(model_name, retrain, encoder,
                           X_train, X_test, y_train, y_test,
                           patience, batch_size, n_dense_units=128, n_lstm_units=256):

    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    if retrain:
        # Define input layer with input shape based on X_train
        input_layer = tf.keras.layers.Input(shape=X_train.shape[1:])  # Automatically infer the shape from X_train

        # Pass the input through the encoder (which should output a fixed size)
        encoder_output = encoder(input_layer)

        # Combine the encoded output back into the initial input
        # input_and_encoded = tf.keras.layers.Add()([encoder_output, input_layer])

        # Add LSTM layer
        lstm_output = tf.keras.layers.LSTM(n_lstm_units)(encoder_output)
        x = tf.keras.layers.Dropout(0.1)(lstm_output)

        x = tf.keras.layers.Dense(128, activation='relu')(x)
        x = tf.keras.layers.Dense(1, activation='linear')(x)

        # Define the complete model
        model = tf.keras.models.Model(inputs=input_layer, outputs=x)

        # Compile the model
        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])
        model.summary()

        # Train the model
        with tf.device('/device:GPU:0'):
            history = model.fit(X_train, y_train,
                                epochs=100, batch_size=batch_size,
                                validation_data=(X_test, y_test),
                                callbacks=[early_stopping, save_callback, lr_plateau])

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{model_name}_history.csv', index=False)

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

# get encoder
encoder_name = 'cnn_autoencoder_v1_epoch_30'
encoder = get_encoder(encoder_name=encoder_name, retrain=False,
                      X_train=X_train, X_test=X_test, patience=10, batch_size=1)

# Make the encoder layers trainable (if you want to fine-tune)
for layer in encoder.layers:
    layer.trainable = True

model_name = 'cnn_lstm_close_v1'
model = get_encoder_lstm_model(model_name=model_name,
                               retrain=True, encoder=encoder,
                               X_train=X_train, X_test=X_test,
                               y_train=y_train, y_test=y_test,
                               patience=10, batch_size=1)

"""# LSTM + Simple CNN"""

def get_lstm_cnn_model(model_name, retrain,
                           X_train, X_test, y_train, y_test,
                           patience, batch_size, n_dense_units=128, n_lstm_units=160):

    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    if retrain:
        # Define input layer with input shape based on X_train
        input_layer = tf.keras.layers.Input(shape=X_train.shape[1:])  # Automatically infer the shape from X_train

        # Combine the encoded output back into the initial input
        # input_and_encoded = tf.keras.layers.Add()([encoder_output, input_layer])

        # Add LSTM layer
        lstm_output = tf.keras.layers.LSTM(n_lstm_units, return_sequences=True)(input_layer)
        x = tf.keras.layers.Dropout(0.1)(lstm_output)

        # Add CNN layers
        x = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)
        x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)
        x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(x)
        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)
        x = tf.keras.layers.Flatten()(x)

        x = tf.keras.layers.Dense(128, activation='relu')(x)
        x = tf.keras.layers.Dense(1, activation='linear')(x)

        # Define the complete model
        model = tf.keras.models.Model(inputs=input_layer, outputs=x)

        # Compile the model
        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])
        model.summary()

        # Train the model
        with tf.device('/device:GPU:0'):
            history = model.fit(X_train, y_train,
                                epochs=100, batch_size=batch_size,
                                validation_data=(X_test, y_test),
                                callbacks=[early_stopping, save_callback, lr_plateau])

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{model_name}_history.csv', index=False)


        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

    else:
        # Load the pre-trained model
        model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

    print('Model summary:')
    model.summary()

    return model

model_name = 'lstm_cnn_close_v1'
model = get_lstm_cnn_model(model_name=model_name,
                               retrain=True,
                               X_train=X_train, X_test=X_test,
                               y_train=y_train, y_test=y_test,
                               patience=10, batch_size=1)

"""# ConvLSTM"""

def get_convlstm_model(model_name, retrain,
                           X_train, X_test, y_train, y_test,
                           patience, batch_size):
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=model_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    if retrain:
        # Define input layer with input shape based on X_train
        input_layer = tf.keras.layers.Input(shape=X_train.shape[1:])  # Automatically infer the shape from X_train
        x = tf.keras.layers.ConvLSTM1D(filters=156, kernel_size=3, activation='relu', padding='same', return_sequences=True)(input_layer)
        x = tf.keras.layers.ConvLSTM1D(filters=156, kernel_size=3, activation='relu', padding='same', return_sequences=False)(x)
        x = tf.keras.layers.Dense(78, activation='relu')(x)
        x = tf.keras.layers.Dense(1, activation='linear')(x)

        # Define the complete model
        model = tf.keras.models.Model(inputs=input_layer, outputs=x)

        # Compile the model
        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae', 'mse'])
        model.summary()

        # Train the model
        with tf.device('/device:GPU:0'):
            history = model.fit(X_train, y_train,
                                epochs=100, batch_size=6,
                                validation_data=(X_test, y_test),
                                callbacks=[early_stopping, save_callback, lr_plateau])

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{model_name}_history.csv', index=False)


        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

    else:
        model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')

    model.summary()

    return model

model_name = 'convlstm_close_v1'
X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))  # Add channel dimension
X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))
X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))
model = get_convlstm_model(model_name=model_name,
                               retrain=True,
                               X_train=X_train_reshaped, X_test=X_test_reshaped,
                               y_train=y_train, y_test=y_test,
                               patience=10, batch_size=6)

"""# Transformer - Close"""

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = tf.keras.layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = tf.keras.layers.Dropout(dropout)(x)
    res = tf.keras.layers.Add()([x, inputs])

    # Feed Forward Part
    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)
    x = tf.keras.layers.Dense(ff_dim, activation="relu")(x)
    x = tf.keras.layers.Dropout(dropout)(x)
    x = tf.keras.layers.Dense(inputs.shape[-1], activation="relu")(x)
    return tf.keras.layers.Add()([x, res])

def build_transformer_model(input_shape, head_size, num_heads,
                            ff_dim, num_transformer_blocks, mlp_units,
                            dropout=0, mlp_dropout=0,
                            num_classes=1, output_type="linear"):
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = tf.keras.layers.GlobalAveragePooling1D()(x)  # data_format="channels_first" (changes from timesteps avging to feature avging)
    # x = tf.keras.layers.AveragePooling1D(pool_size=4, padding='same')(x)
    # x = tf.keras.layers.Flatten()(x)

    for dim in mlp_units:
        x = tf.keras.layers.Dense(dim, activation="relu")(x)
        x = tf.keras.layers.Dropout(mlp_dropout)(x)
    outputs = tf.keras.layers.Dense(num_classes, activation=output_type)(x)
    return tf.keras.Model(inputs, outputs)


# def get_transformer_model(model_name, retrain,
#                            X_train, X_test, y_train, y_test,
#                            patience, batch_size

transformer_name = 'transformer_close_v14'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=transformer_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0001)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    # define parameters needed to build the transformer
    input_shape = (X_train.shape[1], X_train.shape[2])
    head_size = 108  #96  #64
    num_heads = 4
    ff_dim = 256
    num_blocks = 2
    mlp_units = [128]

    # build model
    model = build_transformer_model(input_shape, head_size, num_heads,
                                    ff_dim, num_blocks, mlp_units,
                                    dropout=0.1, mlp_dropout=0.1)

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, y_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, y_test),
                            callbacks=[save_callback, early_stopping, lr_plateau]
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv', index=False)

        # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{transformer_name}.keras')

transformer_name = 'transformer_close_v14'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv')
print(transformer_name)
plot_training_history(history)

trade_model_name = 'transformer_close_v14'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')
y_pred = model.predict(X_train)
y_pred_flat = y_pred.flatten()
plot_prediction_scatter(y_train, y_pred_flat, "Fully Featured TF - Testing (Unseen) Data Performance")

"""Sparse featured version below...

# Transformer - Close (Sparse)
"""

transformer_name = 'transformer_sparse_close_v1'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=transformer_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0001)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.5,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    # define parameters needed to build the transformer
    input_shape = (X_train.shape[1], X_train.shape[2])
    head_size = 108  #96  #64
    num_heads = 4
    ff_dim = 256
    num_blocks = 2
    mlp_units = [128]

    # build model
    model = build_transformer_model(input_shape, head_size, num_heads,
                                    ff_dim, num_blocks, mlp_units,
                                    dropout=0.1, mlp_dropout=0.1)

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, y_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, y_test),
                            callbacks=[save_callback, early_stopping, lr_plateau]
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv', index=False)

        # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{transformer_name}.keras')

transformer_name = 'transformer_sparse_close_v1'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv')
print(transformer_name)
plot_training_history(history)

trade_model_name = 'transformer_sparse_close_v1'
history1 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
trade_model_name = 'transformer_close_v14'
history2 = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{trade_model_name}_history.csv')
print(trade_model_name)
plot_training_history(history1, history2)

trade_model_name = 'transformer_sparse_close_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')
y_pred = model.predict(X_train)
y_pred_flat = y_pred.flatten()
plot_prediction_scatter(y_train, y_pred_flat, "TF - Testing (Unseen) Data Performance")

"""# Transformer - Extrema"""

transformer_name = 'transformer_extrema_v2'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=transformer_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0001)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.62,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=10,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    # define parameters needed to build the transformer
    input_shape = (X_train.shape[1], X_train.shape[2])
    head_size = 96  #64
    num_heads = 4
    ff_dim = 384
    num_blocks = 2
    mlp_units = [128]

    # build model
    model = build_transformer_model(input_shape, head_size, num_heads,
                                    ff_dim, num_blocks, mlp_units,
                                    dropout=0.1, mlp_dropout=0.1,
                                    num_classes=3, output_type='softmax')

    # Per-class precision
    precision_class_0 = tf.keras.metrics.Precision(name='precision_0', class_id=0)
    precision_class_1 = tf.keras.metrics.Precision(name='precision_1', class_id=1)
    precision_class_2 = tf.keras.metrics.Precision(name='precision_2', class_id=2)

    y_train_class_indices = np.argmax(y_train, axis=-1)  # Convert from one-hot to integer labels

    # Calculate class weights using sklearn's compute_class_weight
    class_weights = compute_class_weight(
        class_weight='balanced',  # 'balanced' computes weights inversely proportional to class frequencies
        classes=np.unique(y_train_class_indices),  # All unique class labels
        y=y_train_class_indices  # The true class labels
    )

    # Map each class index to its computed weight
    class_weights_dict = dict(zip(np.unique(y_train_class_indices), class_weights))

    # Compile the model
    # loss='categorical_crossentropy', metrics=[f1_score, 'accuracy']
    model.compile(optimizer=optimizer, loss='categorical_crossentropy',
                      metrics=['accuracy', f1_score_multiclass,
                      precision_class_0, precision_class_1, precision_class_2])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, y_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, y_test),
                            callbacks=[save_callback, early_stopping, lr_plateau],
                            class_weight=class_weights_dict
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv', index=False)

        # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{transformer_name}.keras')

transformer_name = 'transformer_extrema_v2'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv')
print(transformer_name)
plot_training_history(history)

"""# Transformer - Distance to Extrema ('distext')"""

transformer_name = 'transformer_distext_v1'
retrain = True

if retrain:
    # Early stopping, save callback, AdamW optimizer, and Learning Rate on Plateau callback for AdamW
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
    save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models', model_name=transformer_name)
    optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=0.0001)
    lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',   # Typically use val_loss or another metric to monitor
        factor=0.62,           # Reduce LR by a factor of 0.2 when plateau is detected
        patience=5,           # Wait 5 epochs before reducing LR
        min_lr=1e-6,          # Minimum LR to avoid too small values
        verbose=1             # Print learning rate updates
    )

    # define parameters needed to build the transformer
    input_shape = (X_train.shape[1], X_train.shape[2])
    head_size = 96  #64
    num_heads = 4
    ff_dim = 384
    num_blocks = 2
    mlp_units = [128]

    # build model
    model = build_transformer_model(input_shape, head_size, num_heads,
                                    ff_dim, num_blocks, mlp_units,
                                    dropout=0.1, mlp_dropout=0.1,
                                    num_classes=1, output_type='linear')

    # Compile the model
    model.compile(optimizer=optimizer,
                  loss='mean_squared_error',
                  metrics=['mae', 'mse'])

    # Summary of the model
    model.summary()

    with tf.device('/device:GPU:0'):
        # Fit the model to the training data
        history = model.fit(X_train, z_train,
                            epochs=100, batch_size=6,
                            validation_data=(X_test, z_test),
                            callbacks=[save_callback, early_stopping, lr_plateau],
                            )

        # Convert history dictionary to a pandas DataFrame
        history_df = pd.DataFrame(history.history)

        # # Save history to a CSV file
        history_df.to_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv', index=False)

        # Evaluate the model
        # test_loss = model.evaluate(X_test, y_test)
        # print(f"Test Loss: {test_loss}")

        # Save the trained model
        model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{transformer_name}.keras')

transformer_name = 'transformer_distext_v1'
history = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/training_history/{transformer_name}_history.csv')
print(transformer_name)
plot_training_history(history)

# load model
model_name = 'transformer_distext_v1'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{model_name}.keras')
timesteps = 72
look_ahead_period = 12

# load data (2024)
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered_2024.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# create df w features
# df = build_features_for_df_w_patterns(df, 'dff_w_patterns_2024')
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns_2024.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# make 3d tensor and labels
X, y, z, unscaled_flat_X = create_data_and_extrema_labels(df, timesteps, look_ahead_period)

# get model predictions
y_pred = model.predict(X)

# Run the evaluation
# total_profit, successful_trades, _ = evaluate_simple_close_trading_strategy(X, y_pred, unscaled_flat_X)

# Split into chunks for plotting purposes
chunks = split_into_chunks(X, y_pred, unscaled_flat_X, chunk_size=400)

j = 0
# Plot each chunk using a for loop
for X_chunk, y_pred_chunk, df_chunk in chunks:
    print(f'X chunk shape: ', X_chunk.shape)
    print(f'y_pred_chunk shape: ', y_pred_chunk.shape)
    print(f'df_chunk shape: ', df_chunk.shape)
    print(f'df_chunk[close].values shape: ', np.array(df_chunk['close'].values.shape))
    plot_ohlc_w_simple_close_pred(X_chunk, y_pred_chunk, df_chunk)
    j += 1
    if j > 10:
        break

"""# Quantile Transformer"""

def build_quantile_transformer_model(input_shape, head_size, num_heads, ff_dim,
                                     num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0,
                                     quantiles=[0.15, 0.5, 0.85], add_close_pred=False):
    inputs = Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = GlobalAveragePooling1D()(x)  # data_format="channels_first" (changes from timesteps avging to feature avging)
    for dim in mlp_units:
        x = Dense(dim, activation="relu")(x)
        x = Dropout(mlp_dropout)(x)
        z = Dense(dim, activation="relu")(x)
        z = Dropout(mlp_dropout)(z)

    # Output for max upward movement with multiple quantiles
    max_up_outputs = [Dense(1, activation="linear", name=f"max_up_output_{q}")(x) for q in quantiles]

    # Output for max downward movement with multiple quantiles
    max_down_outputs = [Dense(1, activation="linear", name=f"max_down_output_{q}")(x) for q in quantiles]

    if add_close_pred:
        close_quarter_outputs = [Dense(1, activation="linear", name=f"close_quarter_output_{q}")(z) for q in quantiles]
        close_half_outputs = [Dense(1, activation="linear", name=f"close_half_output_{q}")(z) for q in quantiles]
        close_full_outputs = [Dense(1, activation="linear", name=f"close_full_output_{q}")(z) for q in quantiles]

        model = Model(inputs, max_up_outputs + max_down_outputs + close_quarter_outputs + close_half_outputs + close_full_outputs)  # Combine both output lists

    else:
        model = Model(inputs, max_up_outputs + max_down_outputs)  # Combine both output lists

    return model

"""# P3T6 Quantile Model"""

# from posixpath import defpath
def build_p3t6_transformer_model(input_shape, head_size, num_heads, ff_dim,
                                  num_transformer_blocks, mlp_units, look_ahead_periods,
                                  dropout=0, mlp_dropout=0,
                                  quantiles=[0.15, 0.5, 0.85]):
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = inputs

    # Apply transformer blocks
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    # Global pooling layer - # data_format="channels_first" (changes from timesteps avging to feature avging)
    x = tf.keras.layers.GlobalAveragePooling1D()(x)  # GlobalAveragePooling1D()(x)

    # Multi-layer perceptron
    for dim in mlp_units:
        x = tf.keras.layers.Dense(dim, activation="relu")(x)
        x = tf.keras.layers.Dropout(mlp_dropout)(x)

    # Create outputs for each lookahead period and quantile
    outputs = []
    for lookahead in look_ahead_periods:

        lookahead_output = tf.keras.layers.Dense(128, activation="relu")(x)
        lookahead_output = tf.keras.layers.Dropout(mlp_dropout)(lookahead_output)

        # Separate pathways for each metric (close, max up, max down) per look-ahead period
        # Close price pathway
        for q in quantiles:
            # close_output = tf.keras.layers.Dense(128, activation="relu")(x)
            # close_output = tf.keras.layers.Dropout(mlp_dropout)(close_output)
            # close_output = tf.keras.layers.Dense(1, activation="linear", name=f"close_{lookahead}_output_{q}")(close_output)
            close_output = tf.keras.layers.Dense(1, activation="linear", name=f"close_{lookahead}_output_{q}")(lookahead_output)
            outputs.append(close_output)

        # Max up pathway
        for q in quantiles:
            # up_output = tf.keras.layers.Dense(128, activation="relu")(x)
            # up_output = tf.keras.layers.Dropout(mlp_dropout)(up_output)
            # up_output = tf.keras.layers.Dense(1, activation="linear", name=f"mu_{lookahead}_output_{q}")(up_output)
            up_output = tf.keras.layers.Dense(1, activation="linear", name=f"mu_{lookahead}_output_{q}")(lookahead_output)
            outputs.append(up_output)

        # Max down pathway
        for q in quantiles:
            # down_output = tf.keras.layers.Dense(128, activation="relu")(x)
            # down_output = tf.keras.layers.Dropout(mlp_dropout)(down_output)
            # down_output = tf.keras.layers.Dense(1, activation="linear", name=f"md_{lookahead}_output_{q}")(down_output)
            down_output = tf.keras.layers.Dense(1, activation="linear", name=f"md_{lookahead}_output_{q}")(lookahead_output)
            outputs.append(down_output)

        # sig_output = tf.keras.layers.Dense(5, activation="softmax", name=f"sig_{lookahead}_output")(lookahead_output)
        # outputs.append(sig_output)

    # Create the model with the complete list of outputs
    model = tf.keras.Model(inputs, outputs)

    return model

"""# Masked AE P3T6 Model"""

# Example: Assume input_data is your time series data with shape (batch_size, sequence_length, feature_dim)
input_data = np.random.rand(32, 100, 1)  # Example shape (batch_size=32, sequence_length=100, feature_dim=1)

# Create mask
masked_data, mask = create_mask(input_data, mask_percentage=0.2)

def build_mae_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, dropout=0):
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = inputs

    # Apply transformer blocks
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    # Masked reconstruction (output should have same shape as input)
    outputs = tf.keras.layers.Dense(input_shape[-1], activation='linear')(x)

    model = tf.keras.Model(inputs, outputs)
    return model

def masked_mae_loss(y_true, y_pred, mask_value=0.0):
    """
    Custom loss function for Masked Autoencoder. Only computes loss where true value is not masked.
    :param y_true: Actual values (original input data).
    :param y_pred: Predicted values (reconstructed values).
    :param mask_value: Value used for masking (e.g., 0 or NaN).
    :return: Masked mean absolute error loss.
    """
    mask = tf.cast(tf.not_equal(y_true, mask_value), tf.float32)
    loss = tf.abs(y_true - y_pred)  # Mean Absolute Error
    masked_loss = loss * mask  # Apply mask to loss
    return tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)  # Normalize by number of unmasked values

# Mask the training data (e.g., 15% of data will be masked)
X_train_masked, mask = mask_data(X_train, mask_fraction=0.15)

# Build the MAE model (unsupervised)
mae_model = build_mae_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, lookahead_periods, dropout)

# Compile the model with the custom MAE loss function
mae_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss=lambda y_true, y_pred: mae_loss(y_true, y_pred, mask))

# Pretrain the model on the masked data (input) and original data (target)
mae_model.fit(X_train_masked, X_train, epochs=10, batch_size=32)

# Save the pretraining weights
mae_model.save_weights("pretrained_mae_weights.h5")

# Fine-tuning model for supervised task (e.g., price prediction)
supervised_model = build_p3t6_transformer_model(input_shape, head_size=64, num_heads=8, ff_dim=128, num_transformer_blocks=4)

# Load the pretrained weights into the new model (for fine-tuning)
supervised_model.load_weights("pretrained_mae_weights.h5", by_name=True)

# Compile the model for the supervised task (e.g., MSE for regression)
supervised_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')

# Train the model on supervised data
supervised_model.fit(X_train, y_train, epochs=10, batch_size=32)

"""# Training Labels for P3T6 Model"""

def create_quantile_data_and_labels_p3t6(df, timesteps, look_ahead_periods, split=False,
                                         use_daily_atr=True, add_close_pred=False, create_labels=True):
    """
    Creates input sequences and output labels for a model with predictions over multiple lookahead periods.

    Args:
    - df: DataFrame containing the input data.
    - timesteps: Number of timesteps for input sequences.
    - look_ahead_periods: List of lookahead periods (in timesteps) for which predictions are made.
    - split: Boolean flag to indicate whether to split into train and test sets.

    Returns:
    - X: Array of input sequences.
    - Various y arrays: Arrays for each target variable and lookahead period.
    """
    # Exclusion list for columns that should not be used as features
    exclusion_list = ['datetime', 'time', 'target', 'year_week']
    features = [col for col in df.columns if col not in exclusion_list]

    # Scale features
    scaler = StandardScaler()
    sdf = scaler.fit_transform(df[features])

    # Initialize lists for storing input sequences and output labels
    X = []
    y_mu = {period: [] for period in look_ahead_periods}  # Max up (mu) for each period
    y_md = {period: [] for period in look_ahead_periods}  # Max down (md) for each period
    y_close = {period: [] for period in look_ahead_periods}  # Close for each period
    y_sig = {period: [] for period in look_ahead_periods}  # Signal for each period

    # Loop through the dataset and create input sequences and output labels
    for i in range(timesteps, len(df) - max(look_ahead_periods)):
        # Create the input sequence (lookback window)
        X.append(sdf[i - timesteps:i])  # Input sequence with lookback period

        # Current close price at the end of the lookback window
        current_close = df.iloc[i]['close']
        current_daily_atr = df.iloc[i]['daily_atr'] if use_daily_atr else df.iloc[i]['atr']

        # For each lookahead period, calculate the required predictions
        for period in look_ahead_periods:
            future_prices = df.iloc[i+1:i+1+period]['close'].values
            future_highs = df.iloc[i+1:i+1+period]['high'].values
            future_lows = df.iloc[i+1:i+1+period]['low'].values

            # Calculate max upward movement and max downward movement
            max_up = max(future_highs) - current_close
            max_down = current_close - min(future_lows)

            # Calculate close price delta for the lookahead period
            close_delta = (future_prices[-1] - current_close) / current_daily_atr

            # Normalize values by current daily ATR
            y_mu[period].append(max_up / current_daily_atr)
            y_md[period].append(max_down / current_daily_atr)
            y_close[period].append(close_delta)

    # Convert to NumPy arrays for model training
    X = np.array(X)
    y_mu = {period: np.array(y_mu[period]) for period in look_ahead_periods}
    y_md = {period: np.array(y_md[period]) for period in look_ahead_periods}
    y_close = {period: np.array(y_close[period]) for period in look_ahead_periods}

    # Print shapes for debugging
    print(f"X shape: {X.shape}")
    for period in look_ahead_periods:
        print(f"y_mu_{period} shape: {y_mu[period].shape}")
        print(f"y_md_{period} shape: {y_md[period].shape}")
        print(f"y_close_{period} shape: {y_close[period].shape}")

    # Perform synchronized train-test split on the entire dataset if split is True
    if split:
        # Prepare labels as a single dictionary of arrays
        y_all = np.concatenate([y_mu[period][:, np.newaxis] for period in look_ahead_periods] +
                               [y_md[period][:, np.newaxis] for period in look_ahead_periods] +
                               [y_close[period][:, np.newaxis] for period in look_ahead_periods], axis=1)

        # Synchronized split for X and all y labels
        X_train, X_test, y_all_train, y_all_test = train_test_split(X, y_all, test_size=0.2, shuffle=True)

        # Separate y_all back into individual dictionaries for train/test
        y_train, y_test = {}, {}
        split_idx = 0
        for metric, y_dict in zip(['mu', 'md', 'close'], [y_mu, y_md, y_close]):
            for period in look_ahead_periods:
                y_train[f"{metric}_{period}"] = y_all_train[:, split_idx]
                y_test[f"{metric}_{period}"] = y_all_test[:, split_idx]
                split_idx += 1

        return X_train, X_test, y_train, y_test

    else:
        return X, y_mu, y_md, y_close

"""# P3T6 - Compile and Fit"""

# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# with tf.device('/GPU:0'):
#     df = build_features_for_df_w_patterns(df, 'dff_w_patterns')
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# Paths to save/load data
train_data_path = '/content/drive/MyDrive/NeuralNetworkTradeBot/p3t6_train_data2.npz'
test_data_path = '/content/drive/MyDrive/NeuralNetworkTradeBot/p3t6_test_data2.npz'

# Lookahead periods as per the recent changes
look_ahead_periods = [1, 3, 6, 12, 24, 48]
quantiles = [0.15, 0.5, 0.85]

# Load or generate data
if os.path.exists(train_data_path) and os.path.exists(test_data_path):
    with np.load(train_data_path) as train_data, np.load(test_data_path) as test_data:
        X_train = train_data['X_train']
        X_test = test_data['X_test']

        # Load all y_train and y_test labels for each period
        y_train = {f"{metric}_{period}": train_data[f"{metric}_{period}"]
                   for metric in ['mu', 'md', 'close'] for period in look_ahead_periods}
        y_test = {f"{metric}_{period}": test_data[f"{metric}_{period}"]
                  for metric in ['mu', 'md', 'close'] for period in look_ahead_periods}
else:
    # Generate data if no saved files
    (X_train, X_test, y_train, y_test) = create_quantile_data_and_labels_p3t6(
        df, timesteps=96, look_ahead_periods=look_ahead_periods, split=True,
        use_daily_atr=True, add_close_pred=True
    )

    # Save the generated data
    np.savez_compressed(train_data_path, X_train=X_train, **y_train)
    np.savez_compressed(test_data_path, X_test=X_test, **y_test)

trade_model_name = f'{interval}tradeModel_quantile_p3t6_2'

input_shape = (X_train.shape[1], X_train.shape[2])  # Example shape, adjust as needed

# Initialize AdamW optimizer
optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)

# Early stopping and save callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
save_callback = SaveEvery5thEpoch(save_path='/content/drive/MyDrive/NeuralNetworkTradeBot/models')

# Build the model
model = build_p3t6_transformer_model(
    input_shape=input_shape, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim,
    num_transformer_blocks=num_blocks, mlp_units=[mlp_units],
    dropout=0.1, mlp_dropout=0.1, quantiles=quantiles, look_ahead_periods=look_ahead_periods
)

# Define a dictionary for each quantile output loss
loss_dict = {}
for metric in ['mu', 'md', 'close']:
    for period in look_ahead_periods:
        for q in quantiles:
            output_name = f"{metric}_{period}_output_{q}"
            loss_dict[output_name] = quantile_loss_wrapper(q)

# Adding unique MAE for each output
metrics_dict = {}
for metric in ['mu', 'md', 'close']:
    for period in look_ahead_periods:
        for q in quantiles:
            output_name = f"{metric}_{period}_output_{q}"
            metrics_dict[output_name] = tf.keras.metrics.MeanAbsoluteError(name=f"mae")

# Compile the model with each quantile output mapped to its quantile loss function
model.compile(optimizer=optimizer, loss=loss_dict, metrics=metrics_dict)

# Prepare training and validation targets for each quantile output layer
# y_train_targets = [y_train[f"y_train_{metric}_{period}"] for metric in ['mu', 'md', 'close']
#                    for period in look_ahead_periods for _ in quantiles]
# y_test_targets = [y_test[f"y_test_{metric}_{period}"] for metric in ['mu', 'md', 'close']
#                   for period in look_ahead_periods for _ in quantiles]

# Prepare training and validation targets for each quantile output layer
# y_train_targets = [y_train[f"{metric}_{period}"] for metric in ['mu', 'md', 'close']
#                    for period in look_ahead_periods for _ in quantiles]

# y_test_targets = [y_test[f"{metric}_{period}"] for metric in ['mu', 'md', 'close']
#                   for period in look_ahead_periods for _ in quantiles]

# Prepare training and validation targets for each quantile output layer

# Assuming you have a list of quantiles defined (e.g., [0.1, 0.25, 0.5, 0.75, 0.9])

y_train_targets = (
    [y_train['mu_1']] * len(quantiles) +
    [y_train['mu_3']] * len(quantiles) +
    [y_train['mu_6']] * len(quantiles) +
    [y_train['mu_12']] * len(quantiles) +
    [y_train['mu_24']] * len(quantiles) +
    [y_train['mu_48']] * len(quantiles) +

    [y_train['md_1']] * len(quantiles) +
    [y_train['md_3']] * len(quantiles) +
    [y_train['md_6']] * len(quantiles) +
    [y_train['md_12']] * len(quantiles) +
    [y_train['md_24']] * len(quantiles) +
    [y_train['md_48']] * len(quantiles) +

    [y_train['close_1']] * len(quantiles) +
    [y_train['close_3']] * len(quantiles) +
    [y_train['close_6']] * len(quantiles) +
    [y_train['close_12']] * len(quantiles) +
    [y_train['close_24']] * len(quantiles) +
    [y_train['close_48']] * len(quantiles)
)

y_test_targets = (
    [y_test['mu_1']] * len(quantiles) +
    [y_test['mu_3']] * len(quantiles) +
    [y_test['mu_6']] * len(quantiles) +
    [y_test['mu_12']] * len(quantiles) +
    [y_test['mu_24']] * len(quantiles) +
    [y_test['mu_48']] * len(quantiles) +

    [y_test['md_1']] * len(quantiles) +
    [y_test['md_3']] * len(quantiles) +
    [y_test['md_6']] * len(quantiles) +
    [y_test['md_12']] * len(quantiles) +
    [y_test['md_24']] * len(quantiles) +
    [y_test['md_48']] * len(quantiles) +

    [y_test['close_1']] * len(quantiles) +
    [y_test['close_3']] * len(quantiles) +
    [y_test['close_6']] * len(quantiles) +
    [y_test['close_12']] * len(quantiles) +
    [y_test['close_24']] * len(quantiles) +
    [y_test['close_48']] * len(quantiles)
)

# Learning Rate on Plateau callback for AdamW
lr_plateau = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',   # Typically use val_loss or another metric to monitor
    factor=0.2,           # Reduce LR by a factor of 0.2 when plateau is detected
    patience=5,           # Wait 5 epochs before reducing LR
    min_lr=1e-6,          # Minimum LR to avoid too small values
    verbose=1             # Print learning rate updates
)

model.summary()

# Train the model
with tf.device('/GPU:0'):
    model.fit(
        X_train,
        y_train_targets,
        epochs=250,
        batch_size=6,
        validation_data=(X_test, y_test_targets),
        callbacks=[save_callback, early_stopping, lr_plateau]
    )

# Save the trained model
model.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras')

"""# Stacked LSTM on P3T6"""

# get data
# df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered.csv', parse_dates=['datetime'])
# df.sort_values('datetime', inplace=True)

# with tf.device('/GPU:0'):
#     df = build_features_for_df_w_patterns(df, 'dff_w_patterns')
df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_w_patterns.csv', parse_dates=['datetime'])
df.sort_values('datetime', inplace=True)

# Paths to save/load data
train_data_path = '/content/drive/MyDrive/NeuralNetworkTradeBot/p3t6_train_data2.npz'
test_data_path = '/content/drive/MyDrive/NeuralNetworkTradeBot/p3t6_test_data2.npz'

# Lookahead periods as per the recent changes
look_ahead_periods = [1, 3, 6, 12, 24, 48]
quantiles = [0.15, 0.5, 0.85]

# Load or generate data
if os.path.exists(train_data_path) and os.path.exists(test_data_path):
    with np.load(train_data_path) as train_data, np.load(test_data_path) as test_data:
        X_train = train_data['X_train']
        X_test = test_data['X_test']

        # Load all y_train and y_test labels for each period
        y_train = {f"{metric}_{period}": train_data[f"{metric}_{period}"]
                   for metric in ['mu', 'md', 'close'] for period in look_ahead_periods}
        y_test = {f"{metric}_{period}": test_data[f"{metric}_{period}"]
                  for metric in ['mu', 'md', 'close'] for period in look_ahead_periods}
else:
    print('--Failed to load data--')
#     # Generate data if no saved files
#     (X_train, X_test, y_train, y_test) = create_quantile_data_and_labels_p3t6(
#         df, timesteps=96, look_ahead_periods=look_ahead_periods, split=True,
#         use_daily_atr=True, add_close_pred=True
#     )

#     # Save the generated data
#     np.savez_compressed(train_data_path, X_train=X_train, **y_train)
#     np.savez_compressed(test_data_path, X_test=X_test, **y_test)

y_train_targets = (
    [y_train['mu_1']] * len(quantiles) +
    [y_train['mu_3']] * len(quantiles) +
    [y_train['mu_6']] * len(quantiles) +
    [y_train['mu_12']] * len(quantiles) +
    [y_train['mu_24']] * len(quantiles) +
    [y_train['mu_48']] * len(quantiles) +

    [y_train['md_1']] * len(quantiles) +
    [y_train['md_3']] * len(quantiles) +
    [y_train['md_6']] * len(quantiles) +
    [y_train['md_12']] * len(quantiles) +
    [y_train['md_24']] * len(quantiles) +
    [y_train['md_48']] * len(quantiles) +

    [y_train['close_1']] * len(quantiles) +
    [y_train['close_3']] * len(quantiles) +
    [y_train['close_6']] * len(quantiles) +
    [y_train['close_12']] * len(quantiles) +
    [y_train['close_24']] * len(quantiles) +
    [y_train['close_48']] * len(quantiles)
)

y_test_targets = (
    [y_test['mu_1']] * len(quantiles) +
    [y_test['mu_3']] * len(quantiles) +
    [y_test['mu_6']] * len(quantiles) +
    [y_test['mu_12']] * len(quantiles) +
    [y_test['mu_24']] * len(quantiles) +
    [y_test['mu_48']] * len(quantiles) +

    [y_test['md_1']] * len(quantiles) +
    [y_test['md_3']] * len(quantiles) +
    [y_test['md_6']] * len(quantiles) +
    [y_test['md_12']] * len(quantiles) +
    [y_test['md_24']] * len(quantiles) +
    [y_test['md_48']] * len(quantiles) +

    [y_test['close_1']] * len(quantiles) +
    [y_test['close_3']] * len(quantiles) +
    [y_test['close_6']] * len(quantiles) +
    [y_test['close_12']] * len(quantiles) +
    [y_test['close_24']] * len(quantiles) +
    [y_test['close_48']] * len(quantiles)
)

trade_model_name = f'{interval}tradeModel_quantile_p3t6_2_epoch_15'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras', custom_objects={'quantile_loss': quantile_loss}, safe_mode=False)
# Define a dictionary for each quantile output loss
loss_dict = {}
for metric in ['mu', 'md', 'close']:
    for period in look_ahead_periods:
        for q in quantiles:
            output_name = f"{metric}_{period}_output_{q}"
            loss_dict[output_name] = quantile_loss_wrapper(q)

# Adding unique MAE for each output
metrics_dict = {}
for metric in ['mu', 'md', 'close']:
    for period in look_ahead_periods:
        for q in quantiles:
            output_name = f"{metric}_{period}_output_{q}"
            metrics_dict[output_name] = tf.keras.metrics.MeanAbsoluteError(name=f"mae")

# Initialize AdamW optimizer
optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)
# Compile the model with each quantile output mapped to its quantile loss function
model.compile(optimizer=optimizer, loss=loss_dict, metrics=metrics_dict)

# get p3t6 model predictions
with tf.device('/GPU:0'):
    print('Train data evaluation: ')
    model.evaluate(X_train, y_train_targets)
    print('Test data evaluation: ')
    model.evaluate(X_test, y_test_targets)
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

print(np.array(y_train_pred).shape)
print(np.array(y_test_pred).shape)
print('---')
print(X_train.shape)
print(X_test.shape)
print('---')
print(df.shape)  # +timesteps from start, -max lookahead from end
# print(df.columns)

print(np.squeeze(np.array(y_train_pred)).T[:10, 46])

y_train_pred = np.array(y_train_pred)
y_test_pred = np.array(y_test_pred)

# squeeze and transpose preds
# flatten X timesteps dimension
# combine X and preds
# create 3d tensor of train data and labels

y_train_pred = np.squeeze(y_train_pred).T
y_test_pred = np.squeeze(y_test_pred).T

print(y_train_pred.shape)
print(y_test_pred.shape)

X_train_combined = np.hstack((y_train_pred, X_train[:, -1, :]))
X_test_combined = np.hstack((y_test_pred, X_test[:, -1, :]))

print(X_train_combined.shape)
print(X_test_combined.shape)

# Initialize lists for labels
y_train_classes = []
y_test_classes = []

X_train_combined_lstm = []
for i in range(timesteps, len(X_train_combined)):
    X_train_combined_lstm.append(X_train_combined[i-X_train.shape[1]:i])
    close_delta_for_window = y_train_pred[i][46]
    y_train_classes.append(classify_movement(close_delta_for_window))
print(f'X_train_combined_lstm shape: {np.array(X_train_combined_lstm).shape}')
X_train_combined_lstm = np.array(X_train_combined_lstm)

X_test_combined_lstm = []
for i in range(timesteps, len(X_test_combined)):
    X_test_combined_lstm.append(X_test_combined[i-X_train.shape[1]:i])
print(f'X_test_combined_lstm shape: {np.array(X_test_combined_lstm).shape}')
X_test_combined_lstm = np.array(X_test_combined_lstm)




# make labels based on desired max length of trade, and threshold
#  -(strong sell, sell, hold, buy, strong buy)
# or make labels based on max length
#  -(predict optimal entry price, and exit price)

# WIP___Define thresholds for classification
strong_sell_threshold = -0.3  # -3% of daily atr change
sell_threshold = -0.15         # -1% of daily atr change
hold_threshold = 0.15          # +1% of daily atr change
buy_threshold = 0.3           # +30% of daily atr change



# WIP____Define classification function based on thresholds
def classify_movement(pred_close_delta):
    if pred_close_delta <= strong_sell_threshold:
        return 0  # Strong Sell
    elif pred_close_delta <= sell_threshold:
        return 1  # Sell
    elif pred_close_delta <= hold_threshold:
        return 2  # Hold
    elif pred_close_delta <= buy_threshold:
        return 3  # Buy
    else:
        return 4  # Strong Buy

# Assuming y_train_pred and y_test_pred are initially of shape [num_features1, num_samples, 1]
# y_train_pred = np.squeeze(y_train_pred)  # Shape: [num_samples, num_features1]
# y_test_pred = np.squeeze(y_test_pred)    # Shape: [num_samples, num_features1]

# Verify shapes
print("y_train_pred shape after reshape:", y_train_pred.shape)
print("y_test_pred shape after reshape:", y_test_pred.shape)

close_price_delta_index = 46  # should be close_12_0.5 pred

# WIP___Apply classification based on close price delta
# Assuming y_train_pred and y_test_pred are arrays of close price deltas for each period
for pred in y_train_pred:
    y_train_classes.append(classify_movement(pred[close_price_delta_index]))  # Adjust indexing if necessary for specific period

for pred in y_test_pred:
    y_test_classes.append(classify_movement(pred[close_price_delta_index]))  # Adjust indexing if necessary for specific period

# Convert to arrays
y_train_classes = np.array(y_train_classes)
y_test_classes = np.array(y_test_classes)

# Initialize AdamW optimizer
optimizer = tf.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-4)

# create a model that determines whether a trade is available, and whether it is
# a strong sell, sell, buy, or strong buy, and tells the optimal entry price
def build_cnn_classifier(input_shape, num_classes=5):
    model = tf.keras.models.Sequential([
        tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu'),
        tf.keras.layers.MaxPooling1D(pool_size=2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.1),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',
                  metrics=['accuracy', f1_score])
    return model

with tf.device('/GPU:0'):
    # Initialize CNN model with input shape of combined data
    input_shape = (X_train_combined.shape[1], X_train_combined.shape[2])  # Adjust shape as needed
    cnn_classifier = build_cnn_classifier(input_shape)

    # Train the CNN classifier
    cnn_classifier.fit(X_train_combined, y_train_classes, validation_data=(X_test_combined, y_test_classes),
                      epochs=20, batch_size=6)

    # Save the trained model
    cnn_classifier.save(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}_classifier.keras')

"""# ---Backtesting and Evaluating Models---

# Backtesting OG Transformer
"""

def visualize_model_performance(model, X_test, y_test_onehot, df, test_start_index):
    """
    Visualizes the performance of a trained model on the test data.

    Parameters:
    - model: Trained LSTM model.
    - X_test: Test data (features).
    - y_test: True labels for the test data.
    - class_labels: List of class labels (default is [-2, -1, 0, 1, 2]).

    Outputs:
    - Bar chart showing precision for each class.
    - Confusion matrix displaying model performance.
    """
    # Convert one-hot encoded labels to class labels
    y_test = np.argmax(y_test_onehot, axis=1)

    # Predict the labels for the test data
    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)  # Adjust to match class labels

    # Generate a classification report
    report = classification_report(y_test, y_pred_classes, target_names=[str(label) for label in class_labels], output_dict=True)

    # Extract precision for each class
    precision_values = [report[str(label)]['precision'] for label in class_labels]

    # Plot precision for each class
    plt.figure(figsize=(10, 6))
    sns.barplot(x=class_labels, y=precision_values, palette="viridis")
    plt.title('Precision for Each Class')
    plt.xlabel('Class Labels')
    plt.ylabel('Precision')
    plt.show()

    # Generate and display the confusion matrix
    cm = confusion_matrix(y_test, y_pred_classes, labels=class_labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()

    # df, y_test (true labels)
    # create df_test
    # buy_indices = np.where(df_test['datetime'])

    # Align predictions with the correct indices in the full dataset
    test_indices = range(test_start_index, test_start_index + len(y_pred_classes))

    # Slice the DataFrame to start at the test_start_index
    df_test = df.iloc[test_start_index:test_start_index + len(y_pred_classes)].copy()

    # Step 1: Make sure df_test and y_pred_classes have the same length
    assert len(df_test) == len(y_pred_classes), "Length of df_test and predictions must match."

    # Step 2: Add a new column in df_test to store predicted labels
    df_test['predicted_labels'] = y_pred_classes

    # Ensure datetime is in datetime format and group by date
    df_test['date'] = pd.to_datetime(df_test['datetime']).dt.date
    grouped = df_test.groupby('date')

    # Plot each day's performance separately
    for date, group in grouped:
        # Prepare data for candlestick chart
        ohlc = group[['relative_time_of_day','datetime', 'open', 'high', 'low', 'close', 'target', 'predicted_labels',
                      'ema_10', 'ema_50', 'ema_200', 'daily_atr']].copy()

        # Debugging: print out the first few datetime values for comparison
        # print("OHLC datetime values:", ohlc['datetime'].head())
        # print("Group datetime values:", group['datetime'].head())

        ohlc.set_index('datetime', inplace=True)

        # Create a figure for each day
        fig, ax = plt.subplots(figsize=(14, 8))

        # custom fn to add (true) markers to plot
        true_buy_signals = np.where(ohlc['target'] == buy_target, 1, np.nan) * ohlc['low'] - 0.05
        true_sell_signals = np.where(ohlc['target'] == sell_target, 1, np.nan) * ohlc['high'] + 0.05
        pred_buy_signals = np.where(ohlc['predicted_labels'] == buy_target, 1, np.nan) * ohlc['low'] - 0.15
        pred_sell_signals = np.where(ohlc['predicted_labels'] == sell_target, 1, np.nan) * ohlc['high'] + 0.15
        apd = [
            mpf.make_addplot(ohlc['ema_10'], color='orange', ax=ax, label='10 EMA'),
            mpf.make_addplot(ohlc['ema_50'], color='purple', ax=ax, label='50 EMA'),
            mpf.make_addplot(ohlc['ema_200'], color='green', ax=ax, label='200 EMA'),
            mpf.make_addplot(true_buy_signals, scatter=True, markersize=100, marker=r'$\Uparrow$', color='green', ax=ax),
            mpf.make_addplot(true_sell_signals, scatter=True, markersize=100, marker=r'$\Downarrow$', color='red', ax=ax),
            mpf.make_addplot(pred_buy_signals, scatter=True, markersize=100, marker=r'$\Uparrow$', color='blue', ax=ax),
            mpf.make_addplot(pred_sell_signals, scatter=True, markersize=100, marker=r'$\Downarrow$', color='magenta', ax=ax),
        ]
        # Plot the candlestick chart using mplfinance
        mpf.plot(ohlc, type='candle', ax=ax, style='charles', show_nontrading=False, addplot=apd)

        latest_atr = ohlc['daily_atr'].iloc[-1]  # Get the latest ATR value
        ax.text(1.0, 1.02, f"Daily ATR: {latest_atr:.2f}", transform=ax.transAxes, fontsize=12, ha='right', va='top')

        ax.set_title(f'Stock Candlestick Chart with Model Classifications ({date})')
        ax.set_xlabel('Time')
        ax.set_ylabel('Price')
        ax.legend()

        # Save the plot
        filename = f'/content/drive/MyDrive/NeuralNetworkTradeBot/charts/{trade_model_name}_{date}.png'
        plt.savefig(filename)
        plt.close()  # Close the plot to free memory and avoid display

model = keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/{trade_model_name}.keras', custom_objects={'f1_score': f1_score})

X_train, X_test, y_train, y_test = train_test_split(X_combined_lstm, y_combined_lstm, test_size=0.2, shuffle=False)

test_start_index = len(df) - len(X_test)  # This assumes a simple time-based split

# Visualize model performance
visualize_model_performance(model, X_test, y_test, df, test_start_index)

"""# Evaluate p4t3 trades"""

import pandas as pd

def evaluate_p4t3_trades(data, y_pred, look_ahead_period, threshold, limit_one_active_trade=True,
                             trading_start_time="09:30", trading_end_time="15:30", instant_entry=True):
    """
    Evaluate trades based on quantile predictions across multiple lookahead periods.
    """

    # Define trading window
    trading_start_time = pd.to_datetime(trading_start_time).time()
    trading_end_time = pd.to_datetime(trading_end_time).time()

    # Extract predictions from y_pred in the expected order
    y_pred_dict = {
        'max_up': {
            'quarter': {0.15: y_pred[0, :], 0.5: y_pred[1, :], 0.85: y_pred[2, :]},
            'half': {0.15: y_pred[3, :], 0.5: y_pred[4, :], 0.85: y_pred[5, :]},
            'full': {0.15: y_pred[6, :], 0.5: y_pred[7, :], 0.85: y_pred[8, :]}
        },
        'max_down': {
            'quarter': {0.15: y_pred[9, :], 0.5: y_pred[10, :], 0.85: y_pred[11, :]},
            'half': {0.15: y_pred[12, :], 0.5: y_pred[13, :], 0.85: y_pred[14, :]},
            'full': {0.15: y_pred[15, :], 0.5: y_pred[16, :], 0.85: y_pred[17, :]}
        },
        'close': {
            'quarter': {0.15: y_pred[18, :], 0.5: y_pred[19, :], 0.85: y_pred[20, :]},
            'half': {0.15: y_pred[21, :], 0.5: y_pred[22, :], 0.85: y_pred[23, :]},
            'full': {0.15: y_pred[24, :], 0.5: y_pred[25, :], 0.85: y_pred[26, :]}
        }
    }

    # Initialize storage for results
    long_trades = []
    short_trades = []
    break_even_long_trades = []
    break_even_short_trades = []
    long_max_drawdown_and_profit = []
    short_max_drawdown_and_profit = []
    double_look_ahead_period = 2 * look_ahead_period

    # Loop through data to evaluate trades
    for i in range(len(data) - look_ahead_period):
        current_time = data.iloc[i]['datetime'].time()
        in_trading_window = trading_start_time <= current_time <= trading_end_time
        current_close = data.iloc[i]['close']
        current_daily_atr = data.iloc[i]['daily_atr']

        # Thresholds for different lookahead periods
        thresholds = {
            'quarter': 3 * current_daily_atr / 78,
            'half': 6 * current_daily_atr / 78,
            'full': 12 * current_daily_atr / 78
        }

        # Load predictions for this time step
        predictions = {
            period: {
                lookahead: {q: (y_pred_dict[period][lookahead][q][i] * current_daily_atr) for q in [0.15, 0.5, 0.85]}
                for lookahead in ['quarter', 'half', 'full']
            } for period in ['max_up', 'max_down', 'close']
        }

        # Define conditions for long and short trades based on thresholds and predictions
        long_condition = (
            predictions['close']['full'][0.15] > thresholds['half'] and predictions['close']['quarter'][0.5] > thresholds['quarter']
            and in_trading_window
        )
        short_condition = (
            predictions['close']['full'][0.85] < -thresholds['half'] and predictions['close']['quarter'][0.5] < -1 * thresholds['quarter']
            and in_trading_window
        )

        # Long trade condition
        if long_condition:
            entry_price, max_drawdown, max_profit, entry_time = current_close, 0, 0, None
            take_profit = current_close +  1 * predictions['max_up']['half'][0.5] #thresholds['full']  #
            stop_loss = current_close - 1.5 * predictions['max_up']['full'][0.5] # 2 * thresholds['full']#
            # entry_price = current_close - predictions['max_down']['quarter'][0.15]
            entered = instant_entry
            entry_time = 0
            max_high, min_low = entry_price, entry_price
            future_closes = data.iloc[i+1:i+1+double_look_ahead_period]['close'].values

            # Manage trade lifecycle
            for j, (future_high, future_low) in enumerate(zip(data.iloc[i+1:i+1+double_look_ahead_period]['high'],
                                                              data.iloc[i+1:i+1+double_look_ahead_period]['low'])):
                cq_0_5 = y_pred_dict['close']['quarter'][0.5][i+j+1]
                current_long_condition = cq_0_5 > -thresholds['quarter']
                if not entered and j < look_ahead_period:  # and predictions['close']['quarter'][0.5] > -thresholds['quarter']:
                    if future_low <= entry_price:
                        entered, entry_time = True, j
                if entered:
                    max_high = max(max_high, future_high)
                    min_low = min(min_low, future_low)
                    max_profit = max(max_profit, max_high - entry_price)
                    max_drawdown = min(max_drawdown, min_low - entry_price)

                    if j >= entry_time + look_ahead_period: # or not current_long_condition:
                        break_even_long_trades.append((False, future_closes[j], future_closes[j] - entry_price))
                        break
                    # elif future_high >= take_profit:
                    #     long_trades.append((True, take_profit, take_profit - entry_price))
                    #     break
                    # elif future_low <= stop_loss:
                    #     long_trades.append((False, stop_loss, stop_loss - entry_price))
                    #     break
            if limit_one_active_trade and entered:
                i = i + j
                long_max_drawdown_and_profit.append((max_drawdown, max_profit, current_daily_atr, predictions['max_down']['full'][0.5], predictions['max_up']['full'][0.5]))


        # Short trade condition
        elif short_condition:
            entry_price, max_drawdown, max_profit, entry_time = current_close, 0, 0, None
            take_profit = current_close - 1 * predictions['max_down']['half'][0.5] #thresholds['full']  #
            stop_loss = current_close + 1.5 * predictions['max_down']['full'][0.5]  #2 * thresholds['full']  #
            # entry_price = current_close + predictions['max_up']['quarter'][0.15]
            entered = instant_entry
            entry_time = 0
            max_high, min_low = entry_price, entry_price
            future_closes = data.iloc[i+1:i+1+double_look_ahead_period]['close'].values

            # Manage trade lifecycle
            for j, (future_high, future_low) in enumerate(zip(data.iloc[i+1:i+1+double_look_ahead_period]['high'],
                                                              data.iloc[i+1:i+1+double_look_ahead_period]['low'])):
                cq_0_5 = y_pred_dict['close']['quarter'][0.5][i+j+1]
                current_short_condition = cq_0_5 < thresholds['quarter']
                if not entered and j < look_ahead_period:  # and predictions['close']['quarter'][0.5] < thresholds['quarter']:
                    if future_high >= entry_price:
                        entered, entry_time = True, j
                if entered:
                    max_high = max(max_high, future_high)
                    min_low = min(min_low, future_low)
                    max_profit = max(max_profit, entry_price - min_low)
                    max_drawdown = min(max_drawdown, entry_price - max_high)

                    if j >= entry_time + look_ahead_period: # or not current_short_condition:
                        break_even_short_trades.append((False, future_closes[j], entry_price - future_closes[j]))
                        break
                    # elif future_low <= take_profit:
                    #     short_trades.append((True, take_profit, entry_price - take_profit))
                    #     break
                    # elif future_high >= stop_loss:
                    #     short_trades.append((False, stop_loss, entry_price - stop_loss))
                    #     break
            if limit_one_active_trade and entered:
                i = i + j
                short_max_drawdown_and_profit.append((max_drawdown, max_profit, current_daily_atr, predictions['max_down']['full'][0.5], predictions['max_up']['full'][0.5]))


    # Summarize results for evaluation
    results_string = summarize_trade_results(long_trades, short_trades, break_even_long_trades, break_even_short_trades,
                                             long_max_drawdown_and_profit, short_max_drawdown_and_profit)
    print(results_string)
    return results_string

def summarize_trade_results(long_trades, short_trades, break_even_long_trades, break_even_short_trades,
                            long_max_drawdown_and_profit, short_max_drawdown_and_profit):
    """
    Summarize long and short trades for final evaluation.
    """
    # Summarize results
    total_long_trades = len(long_trades) + len(break_even_long_trades)
    total_short_trades = len(short_trades) + len(break_even_short_trades)

    num_winning_long_trades = sum(1 for trade in long_trades if trade[0])
    num_losing_long_trades = sum(1 for trade in long_trades if not trade[0])
    num_winning_short_trades = sum(1 for trade in short_trades if trade[0])
    num_losing_short_trades = sum(1 for trade in short_trades if not trade[0])

    total_long_profit = sum(trade[2] for trade in long_trades if trade[0])
    total_short_profit = sum(trade[2] for trade in short_trades if trade[0])

    total_break_even_long_profit = sum(trade[2] for trade in break_even_long_trades)
    total_break_even_short_profit = sum(trade[2] for trade in break_even_short_trades)

    total_long_loss = sum(trade[2] for trade in long_trades if not trade[0])
    total_short_loss = sum(trade[2] for trade in short_trades if not trade[0])

    average_long_profit = total_long_profit / num_winning_long_trades if num_winning_long_trades > 0 else 0
    average_short_profit = total_short_profit / num_winning_short_trades if num_winning_short_trades > 0 else 0

    net_long_profit = total_long_profit + total_long_loss + total_break_even_long_profit
    net_short_profit = total_short_profit + total_short_loss + total_break_even_short_profit

    average_long_loss = total_long_loss / num_losing_long_trades if num_losing_long_trades > 0 else 0
    average_short_loss = total_short_loss / num_losing_short_trades if num_losing_short_trades > 0 else 0

    average_long_breakeven = total_break_even_long_profit / len(break_even_long_trades) if len(break_even_long_trades) > 0 else 0
    avearage_short_breakeven = total_break_even_short_profit / len(break_even_short_trades) if len(break_even_short_trades) > 0 else 0

    average_max_drawdown_long = sum(max_drawdown for max_drawdown, _, _, _, _ in long_max_drawdown_and_profit) / len(long_max_drawdown_and_profit)
    average_max_profit_long = sum(max_profit for _, max_profit, _, _, _ in long_max_drawdown_and_profit) / len(long_max_drawdown_and_profit)
    average_max_up05_long = sum(max_up_0_5 for _, _, _, _, max_up_0_5 in long_max_drawdown_and_profit) / len(long_max_drawdown_and_profit)
    average_max_down05_long = sum(max_down_0_5 for _, _, _, max_down_0_5, _ in long_max_drawdown_and_profit) / len(long_max_drawdown_and_profit)
    average_max_drawdown_short = sum(max_drawdown for max_drawdown, _, _, _, _ in short_max_drawdown_and_profit) / len(short_max_drawdown_and_profit)
    average_max_profit_short = sum(max_profit for _, max_profit, _, _, _ in short_max_drawdown_and_profit) / len(short_max_drawdown_and_profit)
    average_max_up05_short = sum(max_up_0_5 for _, _, _, _, max_up_0_5 in short_max_drawdown_and_profit) / len(short_max_drawdown_and_profit)
    average_max_down05_short = sum(max_down_0_5 for _, _, _, max_down_0_5, _ in short_max_drawdown_and_profit) / len(short_max_drawdown_and_profit)


    results_string = f"""
        Trade Evaluation Results:
        ------------------------------------------
        Total Long Trades: {total_long_trades}
        Winning Long Trades: {num_winning_long_trades} ({num_winning_long_trades / total_long_trades if total_long_trades > 0 else 0})
        Losing Long Trades: {num_losing_long_trades} ({num_losing_long_trades / total_long_trades if total_long_trades > 0 else 0})
        Break-even Long Trades: {len(break_even_long_trades)}

        Total Long Profit (from winning trades): {total_long_profit}
        Total Long Loss (from losing trades): {total_long_loss}
        Total Break-even Long P/L: {total_break_even_long_profit}

        Average Long Profit (per winning trade): {average_long_profit}
        Average Long Loss (per losing trade): {average_long_loss}
        Average Long Break-even P/L: {average_long_breakeven}

        Average Max Profit Long: {average_max_profit_long}
        Average Max Drawdown Long: {average_max_drawdown_long}
        Average Max Up 0.5 Long: {average_max_up05_long}
        Average Max Down 0.5 Long: {average_max_down05_long}



        Total Short Trades: {total_short_trades}
        Winning Short Trades: {num_winning_short_trades} ({num_winning_short_trades / total_short_trades if total_short_trades > 0 else 0})
        Losing Short Trades: {num_losing_short_trades} ({num_losing_short_trades / total_short_trades if total_short_trades > 0 else 0})
        Break-even Short Trades: {len(break_even_short_trades)}

        Total Short Profit (from winning trades): {total_short_profit}
        Total Short Loss (from losing trades): {total_short_loss}
        Total Break-even Short P/L: {total_break_even_short_profit}

        Average Short Profit (per winning trade): {average_short_profit}
        Average Short Loss (per losing trade): {average_short_loss}
        Average Short Break-even P/L: {avearage_short_breakeven}

        Average Max Profit Short: {average_max_profit_short}
        Average Max Drawdown Short: {average_max_drawdown_short}
        Average Max Down 0.5 Short: {average_max_down05_short}
        Average Max Up 0.5 Short: {average_max_up05_short}
        ------------------------------------------
        Net Long Profit: {net_long_profit}, Net Short Profit: {net_short_profit}
        Total Net Profit: {(net_long_profit) + (net_short_profit)}
        Winrate (total): {(num_winning_long_trades + num_winning_short_trades) / (total_long_trades + total_short_trades) if (total_long_trades + total_short_trades) > 0 else 0}
        ------------------------------------------
        note: $1 on SPY == $500 on 1 /ES contract
    """
    return results_string

"""# Quantile Backtesting Fn 2"""

def visualize_quantile_trades(data, y_pred_max_up_0_15, y_pred_max_up_0_5, y_pred_max_up_0_85,
                             y_pred_max_down_0_15, y_pred_max_down_0_5, y_pred_max_down_0_85,
                             look_ahead_period, threshold, limit_one_active_trade,
                             trading_start_time="09:30", trading_end_time="15:30", live=False):
    # plot lines every 30 min that indicate the predicted values (9:30, 10:00, 10:30, 11:00, ..., 2:30, 3:00, 3:30)
    # plot generated signals (indicate whether they succeeded or failed w/ color)
    """
    Visualizes the predictions and generated signals from the quantile transformer model.

    Parameters:
    - data: DataFrame containing the OHLC and technical indicators.
    - y_pred_max_up_0_5: Predicted max upward movement (0.5 quantile).
    - y_pred_max_up_0_85: Predicted max upward movement (0.85 quantile).
    - y_pred_max_down_0_5: Predicted max downward movement (0.5 quantile).
    - y_pred_max_down_0_85: Predicted max downward movement (0.85 quantile).
    - look_ahead_period: Time steps to look ahead for trade evaluations.
    - threshold: Minimum ratio between upward and downward moves to trigger a trade.
    - limit_one_active_trade: Whether to limit to one trade per day.
    - trading_start_time: Start time for filtering trades (default: 09:30).
    - trading_end_time: End time for filtering trades (default: 15:30).
    - trade_model_name: Name to use when saving charts.

    Outputs:
    - Saves a candlestick chart for each day with trade predictions and performance.
    """
    #  # Align predictions with the correct indices in the full dataset
    # test_indices = range(test_start_index, test_start_index + len(y_pred_classes))

    # # Slice the DataFrame to start at the test_start_index
    # df_test = df.iloc[test_start_index:test_start_index + len(y_pred_classes)].copy()

    # # Step 1: Make sure df_test and y_pred_classes have the same length
    # assert len(df_test) == len(y_pred_classes), "Length of df_test and predictions must match."


    # Ensure the input data has the required columns
    required_columns = ['datetime', 'open', 'high', 'low', 'close', 'ema_10', 'ema_50', 'ema_200', 'daily_atr']
    for col in required_columns:
        if col not in data.columns:
            raise ValueError(f"Data is missing required column: {col}")

    # Add predicted values to the DataFrame for plotting purposes
    data['max_up_0_15'] = y_pred_max_up_0_15 * data['daily_atr']
    data['max_up_0_5'] = y_pred_max_up_0_5 * data['daily_atr']
    data['max_up_0_85'] = y_pred_max_up_0_85 * data['daily_atr']
    data['max_down_0_15'] = y_pred_max_down_0_15 * data['daily_atr']
    data['max_down_0_5'] = y_pred_max_down_0_5 * data['daily_atr']
    data['max_down_0_85'] = y_pred_max_down_0_85 * data['daily_atr']

    # Get the current date
    current_date = pd.Timestamp.now().date()

    # Group data by day for plotting individual charts
    data['date'] = pd.to_datetime(data['datetime']).dt.date
    grouped = data.groupby('date')
    group = grouped.get_group(current_date)


    # Iterate over each day's group for plotting
    for date, group in grouped:
        ohlc = group[['datetime', 'open', 'high', 'low', 'close', 'ema_10', 'ema_50', 'ema_200', 'daily_atr']].copy()

        # Set datetime as index for OHLC plotting
        ohlc.set_index('datetime', inplace=True)

        # Create the figure for each day's chart
        fig, ax = plt.subplots(figsize=(14, 8))

        # Filter active trading hours (if needed)
        group['time'] = group['datetime'].dt.time
        print(group['datetime'])
        print(group['time'])
        print(trading_start_time)
        print(trading_end_time)

        filtered_group = group[(group['time'] >= pd.to_datetime(trading_start_time).time()) & (group['time'] <= pd.to_datetime(trading_end_time).time())]

        # Ensure filtered_group is not empty
        if filtered_group.empty:
            continue

        # Add predicted signals as markers
        true_buy_signals = np.where((filtered_group['max_up_0_5'] >= threshold * filtered_group['max_down_0_5']), filtered_group['low'] - 0.05, np.nan)
        true_sell_signals = np.where((filtered_group['max_down_0_5'] >= threshold * filtered_group['max_up_0_5']), filtered_group['high'] + 0.05, np.nan)

        print('True Buy/Sell signals:')
        print(true_buy_signals)
        print(true_sell_signals)

        # Create the prediction bands that are displayed across 6 time steps (30 minutes)
        plot_len = len(filtered_group)  # Number of rows in filtered group
        intervals = range(0, plot_len, look_ahead_period)

        apd = []

        up_15_lines = []
        up_50_lines = []
        up_85_lines = []
        down_15_lines = []
        down_50_lines = []
        down_85_lines = []

        for i in range(len(filtered_group)):
            j = i - (i % look_ahead_period)  # Find the closest multiple of look_ahead_period
            up_15_lines.append(filtered_group['max_up_0_15'].iloc[j] + filtered_group['close'].iloc[j])
            up_50_lines.append(filtered_group['max_up_0_5'].iloc[j] + filtered_group['close'].iloc[j])
            up_85_lines.append(filtered_group['max_up_0_85'].iloc[j] + filtered_group['close'].iloc[j])
            down_15_lines.append(filtered_group['close'].iloc[j] - filtered_group['max_down_0_15'].iloc[j])
            down_50_lines.append(filtered_group['close'].iloc[j] - filtered_group['max_down_0_5'].iloc[j])
            down_85_lines.append(filtered_group['close'].iloc[j] - filtered_group['max_down_0_85'].iloc[j])

        print('Up/Down Lines:')
        up_15_lines = np.array(up_15_lines)
        up_50_lines = np.array(up_50_lines)
        up_85_lines = np.array(up_85_lines)
        down_15_lines = np.array(down_15_lines)
        down_50_lines = np.array(down_50_lines)
        down_85_lines = np.array(down_85_lines)
        print(up_50_lines)
        print(down_50_lines)

        # # Adding stop loss and take profit levels based on predictions
        # take_profit_long = filtered_group['close'] + filtered_group['max_up_0_5']
        # stop_loss_long = filtered_group['close'] - filtered_group['max_down_0_85']
        # take_profit_short = filtered_group['close'] - filtered_group['max_down_0_5']
        # stop_loss_short = filtered_group['close'] + filtered_group['max_up_0_85']

        # Generate additional plots for predicted take-profit/stop-loss levels
        iapd = [
            mpf.make_addplot(filtered_group['ema_10'], color='orange', ax=ax, label='10 EMA'),
            mpf.make_addplot(filtered_group['ema_50'], color='purple', ax=ax, label='50 EMA'),
            mpf.make_addplot(filtered_group['ema_200'], color='green', ax=ax, label='200 EMA'),
            # mpf.make_addplot(take_profit_long, color='blue', linestyle='--', ax=ax, label='TP Long (max_up_0.5)'),
            # mpf.make_addplot(stop_loss_long, color='red', linestyle='--', ax=ax, label='SL Long (max_down_0.85)'),
            # mpf.make_addplot(take_profit_short, color='blue', linestyle=':', ax=ax, label='TP Short (max_down_0.5)'),
            # mpf.make_addplot(stop_loss_short, color='red', linestyle=':', ax=ax, label='SL Short (max_up_0.85)'),
            mpf.make_addplot(true_buy_signals, scatter=True, markersize=100, marker=r'$\Uparrow$', color='green', ax=ax, label='Buy Signal'),
            mpf.make_addplot(true_sell_signals, scatter=True, markersize=100, marker=r'$\Downarrow$', color='red', ax=ax, label='Sell Signal'),
            mpf.make_addplot(up_15_lines, scatter=True, markersize=200, marker='_', color='blue', ax=ax, label='Up Band'),
            mpf.make_addplot(up_50_lines, scatter=True, markersize=200, marker='_', color='blue', ax=ax, label='Up Band'),
            mpf.make_addplot(up_85_lines, scatter=True, markersize=200, marker='_', color='blue', ax=ax, label='Up Band'),
            mpf.make_addplot(down_15_lines, scatter=True, markersize=200, marker='_', color='m', ax=ax, label='Down Band'),
            mpf.make_addplot(down_50_lines, scatter=True, markersize=200, marker='_', color='m', ax=ax, label='Down Band'),
            mpf.make_addplot(down_85_lines, scatter=True, markersize=200, marker='_', color='m', ax=ax, label='Down Band'),
        ]
        apd.extend(iapd)

        # Plot the candlestick chart with quantile-based signals
        mpf.plot(ohlc, type='candle', ax=ax, style='charles', show_nontrading=False, addplot=apd)

        # Add ATR to the chart
        latest_atr = filtered_group['daily_atr'].iloc[-1]
        ax.text(1.0, 1.02, f"Daily ATR: {latest_atr:.2f}", transform=ax.transAxes, fontsize=12, ha='right', va='top')

        ax.set_title(f'Stock Candlestick Chart with Quantile Predictions ({date})')
        ax.set_xlabel('Time')
        ax.set_ylabel('Price')

        # Adjust and show legend
        ax.legend(loc='upper left', fontsize=10)

        # Save the plot to a file
        filename = f'/content/drive/MyDrive/NeuralNetworkTradeBot/charts/{trade_model_name}_quantile_trades_{date}.png'
        plt.savefig(filename)
        # plt.close()  # Close the plot to free memory
        return plt

"""# The Backtest"""

interval = '5min'
timesteps = 48
look_ahead_period = 12
add_close_pred = True
data_test = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/df_with_features.csv', parse_dates=['datetime'])
data_test.sort_values('datetime', inplace=True)

#----for piecewise test-------
# # Select rows from index 150000 to 150160 using iloc
# data_test = data_test.iloc[150000:150161]  # .iloc is exclusive of the end index, so 150161 will include index 150160
#-----------

trade_model_name = f'{interval}tradeModel_quantile_B_v1_epoch_60'

exclusion_list = ['datetime', 'time', 'target', ]

features = [col for col in data_test.columns if col not in exclusion_list]

scaler = StandardScaler()
sdf = scaler.fit_transform(data_test[features])

# Initialize lists for storing input sequences and output labels
X = []

# Loop through the dataset and create input sequences
for i in range(timesteps, len(data_test) - look_ahead_period):
    # Create the input sequence (lookback window)
    X.append(sdf[i - timesteps:i])  # Input sequence with lookback period

# Convert to NumPy arrays for model training
X = np.array(X)

# Shape of X_train: (number of samples, lookback_period, number of features)
print(f"X_train shape: {X.shape}")

model = keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras', custom_objects={'quantile_loss': quantile_loss}, safe_mode=False)

# Make predictions for each quantile
y_pred = model.predict(X)
y_pred = np.array(y_pred)
print(y_pred.shape)

#-----for piecewise test-------
# y_pred_piecewise = []
# for i in range(len(X)):
#     pX = X[i:i+1, :, :]
#     pred = np.array(model.predict(pX))
#     y_pred_piecewise.append(pred[1, -1])  # appends last ypredmaxup05
# y_pred_piecewise = np.array(y_pred_piecewise)
# print(y_pred_piecewise.shape)
# print(y_pred[1, -50:])  # gets ypredmaxup05
# print(y_pred_piecewise[-50:])
#------end piecewise test--------

# Now you can slice it
y_pred_max_up_0_15 = y_pred[0, :]  # Extract the 15th percentile for upward move
y_pred_max_up_0_5 = y_pred[1, :]   # Extract the 50th percentile (median) for upward move
y_pred_max_up_0_85 = y_pred[2, :]
y_pred_max_down_0_15 = y_pred[3, :]  # Extract the 15th percentile for upward move
y_pred_max_down_0_5 = y_pred[4, :]   # Extract the 50th percentile (median) for upward move
y_pred_max_down_0_85 = y_pred[5, :]
if add_close_pred:
    y_pred_close_quarter_0_15 = y_pred[6, :]
    y_pred_close_quarter_0_5 = y_pred[7, :]
    y_pred_close_quarter_0_85 = y_pred[8, :]
    y_pred_close_half_0_15 = y_pred[9, :]
    y_pred_close_half_0_5 = y_pred[10, :]
    y_pred_close_half_0_85 = y_pred[11, :]
    y_pred_close_full_0_15 = y_pred[12, :]
    y_pred_close_full_0_5 = y_pred[13, :]
    y_pred_close_full_0_85 = y_pred[14, :]

threshold = 1.1
trading_start_time = "09:30"
trading_end_time = "15:30"
limit_one_active_trade = True

# slice data_test to align with y predictions
print(len(y_pred_max_up_0_15))
test_start_index = timesteps
data_test = data_test.iloc[test_start_index:].copy()
data_test = data_test[:-look_ahead_period]
print(len(data_test))

# Check alignment
assert len(data_test) == len(y_pred_max_up_0_15), "Predictions and data length must match!"

results = evaluate_quantile_trades(
    data=data_test,
    y_pred_max_up_0_15=y_pred_max_up_0_15,
    y_pred_max_up_0_5=y_pred_max_up_0_5,
    y_pred_max_up_0_85=y_pred_max_up_0_85,
    y_pred_max_down_0_15=y_pred_max_down_0_15,
    y_pred_max_down_0_5=y_pred_max_down_0_5,
    y_pred_max_down_0_85=y_pred_max_down_0_85,
    look_ahead_period=look_ahead_period,
    threshold=1, add_close_pred=add_close_pred,
    y_pred_close_quarter_0_15=y_pred_close_quarter_0_15,
    y_pred_close_quarter_0_5=y_pred_close_quarter_0_5,
    y_pred_close_quarter_0_85=y_pred_close_quarter_0_85,
    y_pred_close_half_0_15=y_pred_close_half_0_15,
    y_pred_close_half_0_5=y_pred_close_half_0_5,
    y_pred_close_half_0_85=y_pred_close_half_0_85,
    y_pred_close_full_0_15=y_pred_close_full_0_15,
    y_pred_close_full_0_5=y_pred_close_full_0_5,
    y_pred_close_full_0_85=y_pred_close_full_0_85
)

visualize_quantile_trades(
    data=data_test,
    y_pred_max_up_0_15=y_pred_max_up_0_15,
    y_pred_max_up_0_5=y_pred_max_up_0_5,
    y_pred_max_up_0_85=y_pred_max_up_0_85,
    y_pred_max_down_0_15=y_pred_max_down_0_15,
    y_pred_max_down_0_5=y_pred_max_down_0_5,
    y_pred_max_down_0_85=y_pred_max_down_0_85,
    look_ahead_period=look_ahead_period,
    threshold=threshold,
    limit_one_active_trade=limit_one_active_trade,
    trading_start_time=trading_start_time,
    trading_end_time="16:00"
)

"""# Testing on Unseen Data (2024)"""

# load df_raw 2024 data
interval = '5min'
timesteps = 96 #48
look_ahead_period = 48 #12
add_close_pred = True
df_raw = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/data/SPY{interval}_unadjusted_basic_filtered_2024.csv', parse_dates=['datetime'])

# create df_with_features from df_raw (na rows are dropped)
# df_with_features = build_features_for_df(df_raw, 'dff_2024')
df_with_features = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/dff_2024.csv', parse_dates=['datetime'])
df_with_features.sort_values('datetime', inplace=True)

# (X_lstm starts at df[timesteps] and ends at df[-look_ahead_period])
(X_lstm, y_mu_q, y_mu_h, y_mu_f,
  y_md_q, y_md_h, y_md_f,
  y_close_q, y_close_h, y_close_f) = create_quantile_data_and_labels2(df_with_features, timesteps, look_ahead_period)
dff = df_with_features.iloc[timesteps:-look_ahead_period]
print(f'X_lstm shape: {X_lstm.shape}')
print(f'y_mu_q shape: {y_mu_q.shape}')
print(f'dff shape: {dff.shape}')

# load and evaluate model
quantiles = [0.15, 0.5, 0.85]

trade_model_name = f'{interval}tradeModel_quantile_B_v3'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras', custom_objects={'quantile_loss': quantile_loss}, safe_mode=False)
# Build the loss function dictionary for quantiles
loss_dict = {}
for q in quantiles:
        loss_dict[f"mu_quarter_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"mu_half_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"mu_full_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"md_quarter_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"md_half_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"md_full_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"close_quarter_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"close_half_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)
        loss_dict[f"close_full_output_{q}"] = lambda y, y_pred, q=q: quantile_loss(q, y, y_pred)

# Initialize AdamW optimizer
optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001, weight_decay=1e-5)
# Compile the model with separate quantile losses for each output (upward and downward)
model.compile(optimizer=optimizer,
              loss=loss_dict,
              metrics=[tf.keras.metrics.MeanAbsoluteError()] * len(loss_dict))
              # metrics=[MeanAbsoluteError(), MeanSquaredError()])

with tf.device('/device:GPU:0'):
    y_pred = model.predict(X_lstm)

# Convert to NumPy array if it's a list
if isinstance(y_pred, list):
    y_pred = np.array(y_pred)

with tf.device('/device:GPU:0'):
    results = evaluate_p4t3_trades(
        data=dff,
        y_pred=y_pred,
        look_ahead_period=look_ahead_period,
        threshold=1.1
    )

# Step 1: Reshape y_pred
y_pred_squeezed = np.squeeze(y_pred)  # Shape: (15, 14592)
y_pred_transposed = y_pred_squeezed.T  # Shape: (14592, 15)

# Check that the number of samples match
assert X_lstm.shape[0] == y_pred_transposed.shape[0], "Sample size mismatch between X_lstm and y_pred"

# Step 2: Expand y_pred to match the time steps in X_lstm
# We will repeat the y_pred values for each timestep, so they become an additional feature at each timestep.
timesteps = X_lstm.shape[1]
y_pred_expanded = np.repeat(y_pred_transposed[:, np.newaxis, :], timesteps, axis=1)  # Shape: (14592, timesteps, 15)

# Step 3: Combine X_lstm and y_pred
# X_lstm shape: (14592, timesteps, num_features)
# y_pred_expanded shape: (14592, timesteps, 15)
X_combined = np.concatenate([X_lstm, y_pred_expanded], axis=2)  # Shape: (14592, timesteps, num_features + 15)

"""# Live Model Helper Functions"""

def message_post(token, channel_id, message):
    # url = f"https://discord.com/api/v9/channels/{channel_id}/messages"
    url = 'https://discord.com/api/webhooks/1253402890526134295/kR3NR9FPNrPPq3Ws62Y1DVouLomTU5XCCFw82vdkubm5DG3g87LXoVYuK-8C80B-hlZq'

    headers = {
        "Authorization": f"{token}",
    }

    data = {
        "content": message
    }

    response = requests.post(url, headers=headers, json=data)

    if response.status_code < 400:
        print("Message sent successfully.")
    else:
        print(f'{response.status_code}: Failed to send the message.')


channel_id = 1112148552039288892
token = 'MTExMjE0NzAxNjg2OTQ5NDg4Ng.Gh6UbG.54FbJ_JwPTjg7dfubsEF5GZ8xxwNN3TJe_z5Dc'

def plot_live_quantile_trades(data, y_pred,
                             look_ahead_period, threshold, limit_one_active_trade,
                             trading_start_time="09:30", trading_end_time="15:30", live=False):
    # plot lines every 30 min that indicate the predicted values (9:30, 10:00, 10:30, 11:00, ..., 2:30, 3:00, 3:30)
    # plot generated signals (indicate whether they succeeded or failed w/ color)
    """
    Visualizes the predictions and generated signals from the quantile transformer model.

    Parameters:
    - data: DataFrame containing the OHLC and technical indicators.
    - y_pred_max_up_0_5: Predicted max upward movement (0.5 quantile).
    - y_pred_max_up_0_85: Predicted max upward movement (0.85 quantile).
    - y_pred_max_down_0_5: Predicted max downward movement (0.5 quantile).
    - y_pred_max_down_0_85: Predicted max downward movement (0.85 quantile).
    - look_ahead_period: Time steps to look ahead for trade evaluations.
    - threshold: Minimum ratio between upward and downward moves to trigger a trade.
    - limit_one_active_trade: Whether to limit to one trade per day.
    - trading_start_time: Start time for filtering trades (default: 09:30).
    - trading_end_time: End time for filtering trades (default: 15:30).
    - trade_model_name: Name to use when saving charts.

    Outputs:
    - Saves a candlestick chart for each day with trade predictions and performance.
    """
    # Extract predictions from y_pred in the expected order
    y_pred_dict = {
        'max_up': {
            'quarter': {0.15: y_pred[0, :], 0.5: y_pred[1, :], 0.85: y_pred[2, :]},
            'half': {0.15: y_pred[3, :], 0.5: y_pred[4, :], 0.85: y_pred[5, :]},
            'full': {0.15: y_pred[6, :], 0.5: y_pred[7, :], 0.85: y_pred[8, :]}
        },
        'max_down': {
            'quarter': {0.15: y_pred[9, :], 0.5: y_pred[10, :], 0.85: y_pred[11, :]},
            'half': {0.15: y_pred[12, :], 0.5: y_pred[13, :], 0.85: y_pred[14, :]},
            'full': {0.15: y_pred[15, :], 0.5: y_pred[16, :], 0.85: y_pred[17, :]}
        },
        'close': {
            'quarter': {0.15: y_pred[18, :], 0.5: y_pred[19, :], 0.85: y_pred[20, :]},
            'half': {0.15: y_pred[21, :], 0.5: y_pred[22, :], 0.85: y_pred[23, :]},
            'full': {0.15: y_pred[24, :], 0.5: y_pred[25, :], 0.85: y_pred[26, :]}
        }
    }

    # Ensure the input data has the required columns
    required_columns = ['datetime', 'open', 'high', 'low', 'close', 'ema_10', 'ema_50', 'ema_200', 'daily_atr']
    for col in required_columns:
        if col not in data.columns:
            raise ValueError(f"Data is missing required column: {col}")

    # Add predicted values to the DataFrame for plotting purposes
    data['max_up_0_15'] = y_pred_dict['max_up']['quarter'][0.15]
    data['max_up_0_5'] = y_pred_dict['max_up']['quarter'][0.5]
    data['max_up_0_85'] = y_pred_dict['max_up']['quarter'][0.85]

    data['max_down_0_15'] = y_pred_dict['max_down']['quarter'][0.15]
    data['max_down_0_5'] = y_pred_dict['max_down']['quarter'][0.5]
    data['max_down_0_85'] = y_pred_dict['max_down']['quarter'][0.85]

    data['close_quarter_0_15'] = y_pred_dict['close']['quarter'][0.15]
    data['close_quarter_0_5'] = y_pred_dict['close']['quarter'][0.5]
    data['close_quarter_0_85'] = y_pred_dict['close']['quarter'][0.85]

    data['close_half_0_15'] = y_pred_dict['close']['half'][0.15]
    data['close_half_0_5'] = y_pred_dict['close']['half'][0.5]
    data['close_half_0_85'] = y_pred_dict['close']['half'][0.85]

    data['close_full_0_15'] = y_pred_dict['close']['full'][0.15]
    data['close_full_0_5'] = y_pred_dict['close']['full'][0.5]
    data['close_full_0_85'] = y_pred_dict['close']['full'][0.85]

    # Get the current date
    current_date = pd.Timestamp.now().date()

    # Group data by day for plotting individual charts
    data['date'] = pd.to_datetime(data['datetime']).dt.date
    grouped = data.groupby('date')
    group = grouped.get_group(current_date)

    ohlc = group[['datetime', 'open', 'high', 'low', 'close', 'ema_10', 'ema_50', 'ema_200', 'daily_atr']].copy()

    # Set datetime as index for OHLC plotting
    ohlc.set_index('datetime', inplace=True)

    # Create the figure for each day's chart
    fig, ax = plt.subplots(figsize=(14, 8))

    # Filter active trading hours (if needed)
    group['time'] = group['datetime'].dt.time
    # print(group['datetime'])
    # print(group['time'])
    # print(trading_start_time)
    # print(trading_end_time)

    filtered_group = group[(group['time'] >= pd.to_datetime(trading_start_time).time()) & (group['time'] <= pd.to_datetime(trading_end_time).time())]

    up_thresh = 8/78
    down_thresh = 8/78
    # Add predicted signals as markers
    true_buy_signals = np.where((filtered_group['close_quarter_0_15'] >= up_thresh), filtered_group['low'] - 0.05, np.nan)
    true_sell_signals = np.where((filtered_group['close_quarter_0_85'] <= -down_thresh), filtered_group['high'] + 0.05, np.nan)

    # print('True Buy/Sell signals:')
    # print(true_buy_signals)
    # print(true_sell_signals)

    # Create the prediction bands that are displayed across 6 time steps (30 minutes)
    plot_len = len(filtered_group)  # Number of rows in filtered group
    intervals = range(0, plot_len, look_ahead_period)

    apd = []

    up_15_lines = []
    up_50_lines = []
    up_85_lines = []
    down_15_lines = []
    down_50_lines = []
    down_85_lines = []
    cq_15_lines = []
    cq_50_lines = []
    cq_85_lines = []
    ch_15_lines = []
    ch_50_lines = []
    ch_85_lines = []
    cf_15_lines = []
    cf_50_lines = []
    cf_85_lines = []

    for i in range(len(filtered_group)):
        lap = int(look_ahead_period / 4)
        j = i - (i % lap)  # Find the closest multiple of look_ahead_period
        up_15_lines.append(filtered_group['max_up_0_15'].iloc[j] * filtered_group['daily_atr'].iloc[j] + filtered_group['close'].iloc[j])
        up_50_lines.append(filtered_group['max_up_0_5'].iloc[j] * filtered_group['daily_atr'].iloc[j] + filtered_group['close'].iloc[j])
        up_85_lines.append(filtered_group['max_up_0_85'].iloc[j] * filtered_group['daily_atr'].iloc[j] + filtered_group['close'].iloc[j])
        down_15_lines.append(filtered_group['close'].iloc[j] - filtered_group['max_down_0_15'].iloc[j] * filtered_group['daily_atr'].iloc[j])
        down_50_lines.append(filtered_group['close'].iloc[j] - filtered_group['max_down_0_5'].iloc[j] * filtered_group['daily_atr'].iloc[j])
        down_85_lines.append(filtered_group['close'].iloc[j] - filtered_group['max_down_0_85'].iloc[j] * filtered_group['daily_atr'].iloc[j])

        # y = i - (i % int(look_ahead_period / 2))
        ch_15_lines.append(filtered_group['close_half_0_15'].iloc[i] * filtered_group['daily_atr'].iloc[i] + filtered_group['close'].iloc[i])
        ch_50_lines.append(filtered_group['close_half_0_5'].iloc[i] * filtered_group['daily_atr'].iloc[i] + filtered_group['close'].iloc[i])
        ch_85_lines.append(filtered_group['close_half_0_85'].iloc[i] * filtered_group['daily_atr'].iloc[i] + filtered_group['close'].iloc[i])


    # print('Up/Down Lines:')
    # up_15_lines = np.array(up_15_lines)
    # up_50_lines = np.array(up_50_lines)
    # up_85_lines = np.array(up_85_lines)
    # down_15_lines = np.array(down_15_lines)
    # down_50_lines = np.array(down_50_lines)
    # down_85_lines = np.array(down_85_lines)
    # print(up_50_lines)
    # print(down_50_lines)

    # # Adding stop loss and take profit levels based on predictions
    # take_profit_long = filtered_group['close'] + filtered_group['max_up_0_5']
    # stop_loss_long = filtered_group['close'] - filtered_group['max_down_0_85']
    # take_profit_short = filtered_group['close'] - filtered_group['max_down_0_5']
    # stop_loss_short = filtered_group['close'] + filtered_group['max_up_0_85']

    # Generate additional plots for predicted take-profit/stop-loss levels
    iapd = [
        mpf.make_addplot(filtered_group['ema_10'], color='orange', ax=ax, label='10 EMA'),
        mpf.make_addplot(filtered_group['ema_50'], color='purple', ax=ax, label='50 EMA'),
        mpf.make_addplot(filtered_group['ema_200'], color='green', ax=ax, label='200 EMA'),
        # mpf.make_addplot(take_profit_long, color='blue', linestyle='--', ax=ax, label='TP Long (max_up_0.5)'),
        # mpf.make_addplot(stop_loss_long, color='red', linestyle='--', ax=ax, label='SL Long (max_down_0.85)'),
        # mpf.make_addplot(take_profit_short, color='blue', linestyle=':', ax=ax, label='TP Short (max_down_0.5)'),
        # mpf.make_addplot(stop_loss_short, color='red', linestyle=':', ax=ax, label='SL Short (max_up_0.85)'),
        mpf.make_addplot(true_buy_signals, scatter=True, markersize=100, marker=r'$\Uparrow$', color='green', ax=ax, label='Buy Signal'),
        mpf.make_addplot(true_sell_signals, scatter=True, markersize=100, marker=r'$\Downarrow$', color='red', ax=ax, label='Sell Signal'),
        mpf.make_addplot(up_15_lines, scatter=True, markersize=200, marker='_', color='blue', ax=ax, label='Up Band'),
        mpf.make_addplot(up_50_lines, scatter=True, markersize=200, marker='_', color='blue', ax=ax, label='Up Band'),
        mpf.make_addplot(up_85_lines, scatter=True, markersize=200, marker='_', color='blue', ax=ax, label='Up Band'),
        mpf.make_addplot(down_15_lines, scatter=True, markersize=200, marker='_', color='m', ax=ax, label='Down Band'),
        mpf.make_addplot(down_50_lines, scatter=True, markersize=200, marker='_', color='m', ax=ax, label='Down Band'),
        mpf.make_addplot(down_85_lines, scatter=True, markersize=200, marker='_', color='m', ax=ax, label='Down Band'),
        mpf.make_addplot(ch_15_lines, scatter=True, markersize=50, marker='o', color='red', ax=ax, label='Close Half Band'),
        mpf.make_addplot(ch_50_lines, scatter=True, markersize=50, marker='o', color='blue', ax=ax, label='Close Half Band'),
        mpf.make_addplot(ch_85_lines, scatter=True, markersize=50, marker='o', color='green', ax=ax, label='Close Half Band')
    ]
    apd.extend(iapd)

    # Plot the candlestick chart with quantile-based signals
    mpf.plot(ohlc, type='candle', ax=ax, style='charles', show_nontrading=False, addplot=apd)

    # Add ATR to the chart
    latest_atr = filtered_group['daily_atr'].iloc[-1]
    ax.text(1.0, 1.02, f"Daily ATR: {latest_atr:.2f}", transform=ax.transAxes, fontsize=12, ha='right', va='top')

    ax.set_title(f'Stock Candlestick Chart with Quantile Predictions ({current_date})')
    ax.set_xlabel('Time')
    ax.set_ylabel('Price')

    # Adjust and show legend
    ax.legend(loc='upper left', fontsize=10)

    # Save the plot to a file
    # plt.close()  # Close the plot to free memory
    return plt

def plot_live_signals(df, y_pred_classes, live_lookback=200):
    # Extract relevant columns for OHLC (Open, High, Low, Close) for plotting
    df_plot = df[['datetime', 'open', 'high', 'low', 'close', 'ema_10', 'ema_50', 'ema_200', 'daily_atr']].copy()[-(live_lookback-48):]
    df_plot.set_index('datetime', inplace=True)

    # Ensure that predictions match the filtered data length
    assert len(y_pred_classes[-len(df_plot):]) == len(df_plot), "Prediction length must match the data length."

    # Add the latest predictions to the filtered DataFrame for plotting
    df_plot['predicted_labels'] = y_pred_classes[-len(df_plot):]

    # Create markers for predicted Buy and Sell signals
    pred_buy_signals = np.where(df_plot['predicted_labels'] == 2, df_plot['low'] - 0.05, np.nan)
    pred_sell_signals = np.where(df_plot['predicted_labels'] == 0, df_plot['high'] + 0.05, np.nan)

    fig, ax = plt.subplots(figsize=(14, 8))

    # Step 1: Add EMAs and ATR as additional plots
    apd = [
        mpf.make_addplot(df_plot['ema_10'], color='orange', ax=ax, label='10 EMA'),
        mpf.make_addplot(df_plot['ema_50'], color='purple', ax=ax, label='50 EMA'),
        mpf.make_addplot(df_plot['ema_200'], color='green', ax=ax, label='200 EMA'),
        mpf.make_addplot(pred_buy_signals, scatter=True, markersize=100, marker=r'$\Uparrow$', color='blue', ax=ax),
        mpf.make_addplot(pred_sell_signals, scatter=True, markersize=100, marker=r'$\Downarrow$', color='red', ax=ax)
    ]

    # Plot the candlestick chart using mplfinance
    mpf.plot(df_plot, type='candle', ax=ax, style='charles', show_nontrading=False, addplot=apd)

    # Step 2: Add the daily ATR text on the top right corner
    latest_atr = df_plot['daily_atr'].iloc[-1]  # Get the latest ATR value
    ax.text(1.0, 1.02, f"Daily ATR: {latest_atr:.2f}", transform=ax.transAxes, fontsize=12, ha='right', va='top')

    # Customize the plot titles and labels
    ax.set_title(f'Stock Candlestick Chart with Model Classifications')
    ax.set_xlabel('Time')
    ax.set_ylabel('Price')

    # Step 3: Add a legend for the EMAs
    custom_lines = [
        plt.Line2D([0], [0], color='orange', lw=2, label='10 EMA'),
        plt.Line2D([0], [0], color='purple', lw=2, label='50 EMA'),
        plt.Line2D([0], [0], color='green', lw=2, label='200 EMA'),
        plt.Line2D([0], [0], color='blue', lw=2, label='Buy Signal'),
        plt.Line2D([0], [0], color='red', lw=2, label='Sell Signal')
    ]
    ax.legend(handles=custom_lines, loc='upper left')

    # Show the plot
    plt.show()

"""# Live Data Prep"""

def refresh_live_data(interval):
    combineDataToCSV_AV(symbol='SPY', interval=interval, time_period=14, month=None)
    combineDailyATRToCSV_AV(symbol='SPY', interval='daily', time_period=14)
    look_ahead_period = 9
    live_lookback = 200
    target_datr_percentage = (1/3)

    # Load the data
    df = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/live/current.csv', parse_dates=['datetime'])
    df.sort_values('datetime', inplace=True)

    # Calculate Technical Indicators
    df['obv'] = ta.OBV(df['close'], df['vol'])
    df['rsi'] = ta.RSI(df['close'], timeperiod=14)
    df['atr'] = ta.ATR(df['high'], df['low'], df['close'], timeperiod=14)
    df['macd'], df['macd_signal'], df['macd_hist'] = ta.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)
    df['ema_10'] = ta.EMA(df['close'], timeperiod=10)
    df['ema_200'] = ta.EMA(df['close'], timeperiod=200)
    df['ema_50'] = ta.EMA(df['close'], timeperiod=50)
    df['distance_to_ema_10'] = df['close'] - df['ema_10']
    df['distance_to_ema_200'] = df['close'] - df['ema_200']
    df['distance_to_ema_50'] = df['close'] - df['ema_50']
    df['parabolic_sar'] = ta.SAR(df['high'], df['low'], acceleration=0.02, maximum=0.2)
    df['cmf'] = ta.ADOSC(df['high'], df['low'], df['close'], df['vol'], fastperiod=3, slowperiod=10)

    # Candlestick Patterns
    df['doji'] = ta.CDLDOJI(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['hammer'] = ta.CDLHAMMER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['inverted_hammer'] = ta.CDLINVERTEDHAMMER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['shooting_star'] = ta.CDLSHOOTINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['bullish_engulfing'] = ta.CDLENGULFING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x == 100 else 0)
    df['bearish_engulfing'] = ta.CDLENGULFING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x == -100 else 0)
    df['morning_star'] = ta.CDLMORNINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['evening_star'] = ta.CDLEVENINGSTAR(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['harami'] = ta.CDLHARAMI(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['dark_cloud_cover'] = ta.CDLDARKCLOUDCOVER(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['piercing_pattern'] = ta.CDLPIERCING(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['three_white_soldiers'] = ta.CDL3WHITESOLDIERS(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['three_black_crows'] = ta.CDL3BLACKCROWS(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['marubozu'] = ta.CDLMARUBOZU(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)
    df['spinning_top'] = ta.CDLSPINNINGTOP(df['open'], df['high'], df['low'], df['close']).apply(lambda x: 1 if x != 0 else 0)

    # Saty Phase Oscillator Indicators
    df['pivot'] = ta.EMA(df['close'], timeperiod=21)
    df['above_pivot'] = (df['close'] >= df['pivot']).astype(int)
    df['bband_up'] = df['pivot'] + 2.0 * ta.STDDEV(df['close'], timeperiod=21)
    df['bband_down'] = df['pivot'] - 2.0 * ta.STDDEV(df['close'], timeperiod=21)
    df['compression_threshold_up'] = df['pivot'] + (2.0 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['compression_threshold_down'] = df['pivot'] - (2.0 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['expansion_threshold_up'] = df['pivot'] + (1.854 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['expansion_threshold_down'] = df['pivot'] - (1.854 * ta.ATR(df['high'], df['low'], df['close'], timeperiod=14))
    df['compression'] = np.where(df['above_pivot'], df['bband_up'] - df['compression_threshold_up'], df['compression_threshold_down'] - df['bband_down'])
    df['in_expansion_zone'] = np.where(df['above_pivot'], df['bband_up'] - df['expansion_threshold_up'], df['expansion_threshold_down'] - df['bband_down'])
    df['compression_tracker'] = np.where((df['compression'].shift(1) <= df['compression']) & (df['in_expansion_zone'] > 0), 0, np.where(df['compression'] <= 0, 1, 0))
    df['raw_signal'] = ((df['close'] - df['pivot']) / (3.0 * df['atr'])) * 100
    df['oscillator'] = ta.EMA(df['raw_signal'], timeperiod=3)

    # Differentials and Additional Features
    df['vol_diff'] = df['vol'].diff()
    df['obv_diff'] = df['obv'].diff()
    df['rsi_diff'] = df['rsi'].diff()
    df['macd_diff'] = df['macd'].diff()
    df['close_diff'] = df['close'].diff()
    df['atr_diff'] = df['atr'].diff()
    df['parabolic_sar_diff'] = df['parabolic_sar'].diff()
    df['cmf_diff'] = df['cmf'].diff()
    df['ema_10_diff'] = df['ema_10'].diff()
    df['ema_200_diff'] = df['ema_200'].diff()
    df['ema_50_diff'] = df['ema_50'].diff()
    df['distance_to_pivot'] = (df['close'] - df['pivot']) / df['atr']
    df['distance_to_bband_up'] = df['close'] - df['bband_up']
    df['distance_to_bband_down'] = df['close'] - df['bband_down']
    df['price_return'] = df['close'].pct_change(periods=12)
    df['vroc'] = df['vol'].pct_change(periods=12)
    df = addFVGandATRFeatures(df) # add fvg signals and distances to atr lines and daily atr val
    df = add_relative_time_of_day(df) # add relative time of day
    df['distance_open_to_close'] = df['open'] - df['close']  # Calculate the distance of open from close
    df['distance_high_to_close'] = df['high'] - df['close']  # Calculate the distance of high from close
    df['distance_low_to_close'] = df['low'] - df['close']  # Calculate the distance of low from close

    num_classes = 3

    # Extract the time from 'datetime'
    df['time'] = df['datetime'].dt.time
    df['date'] = df['datetime'].dt.date

    # Step 1: Calculate daily high and low (based on 'close' prices) for each date
    daily_high_low = df.groupby('date')['close'].agg(prev_day_high='max', prev_day_low='min')

    # Step 2: Shift the high and low values by 1 to get the previous day's high and low
    daily_high_low_shifted = daily_high_low.shift(1)

    # Step 3: Merge the shifted daily high and low back into the original DataFrame
    df = df.merge(daily_high_low_shifted, how='left', on='date')

    # Step 4: Calculate the distance from current close price to the previous day's high and low
    df['distance_from_prev_day_high'] = df['close'] - df['prev_day_high']
    df['distance_from_prev_day_low'] = df['close'] - df['prev_day_low']

    # Drop the 'date' column if it's no longer needed
    df.drop(columns=['date'], inplace=True)

    # Initialize the target column with the default value of 1 (Hold)
    df['target'] = 1
    # Loop through each row in the dataframe, except the last `look_ahead_period` rows
    for i in range(len(df) - look_ahead_period):
        current_close = df.iloc[i]['close']
        atr_value = df.iloc[i]['daily_atr'] * target_datr_percentage  # Current ATR value

        # Define the upward and downward thresholds
        upward_threshold = current_close + atr_value
        downward_threshold = current_close - atr_value

        # Iterate over the next `look_ahead_period` time steps to see which threshold is reached first
        future_prices = df.iloc[i+1:i+1+look_ahead_period]['close'].values
        future_highs = df.iloc[i+1:i+1+look_ahead_period]['high'].values
        future_lows = df.iloc[i+1:i+1+look_ahead_period]['low'].values

        target = 1  # Default to Hold

        # Check the future highs and lows to see which threshold is reached first
        for future_high, future_low in zip(future_highs, future_lows):
            if future_high >= upward_threshold:
                target = 2  # Upward move of 1 ATR first -> Buy
                break
            elif future_low <= downward_threshold:
                target = 0  # Downward move of 1 ATR first -> Sell
                break

        df.at[i, 'target'] = target

    df['past_target'] = df['target'].shift(look_ahead_period)

    # Drop rows with NaN values created by the shift operation
    df = df[-live_lookback:]

    print(df.columns)

    return df

"""# Live Pipeline"""

live = True
interval = '5min'
encoder_name = f'{interval}encoder_10t12'
trade_model_name = f'{interval}tradeModel_10t12'
exclusion_list = ['datetime', 'target', 'time']
df = refresh_live_data(interval)
features = [col for col in df.columns if col not in exclusion_list]

encoder = get_encoder(encoder_name, False, None, None, None, None, None, None)
model = keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/{trade_model_name}.keras', custom_objects={'f1_score': f1_score})

while live:
    current_minute = time.localtime().tm_min

    # time.delay(60)

    if current_minute % 1 == 0:
        df = refresh_live_data(interval)

        scaler = StandardScaler()
        X = scaler.fit_transform(df[features])

        # Reshape Data for LSTM
        timesteps = 24  # Example: 12 timesteps (1 hour)
        X_lstm = []

        for i in range(timesteps, len(X)):
            X_lstm.append(X[i-timesteps:i])

        X_lstm = np.array(X_lstm)

        print(f'X_lstm shape: {X_lstm.shape}')

        # encode features
        encoded_X = encoder.predict(X_lstm)
        print(f'encoded_X shape: {encoded_X.shape}')

        # Flatten encoded features
        encoded_flat = encoded_X.reshape(encoded_X.shape[0], -1)
        print(f'encoded_flat shape: {encoded_flat.shape}')

        encoded_flat_lstm = []

        # Combine encoded features with raw data
        X_combined = np.hstack((encoded_flat, X_lstm[:, -1, :]))
        print(f"X_combined shape: {X_combined.shape}")

        X_combined_lstm = []

        for i in range(timesteps, len(X_combined)):
            X_combined_lstm.append(X_combined[i-timesteps:i])
        print(f'X_combined_lstm shape: {np.array(X_combined_lstm).shape}')

        # Convert lists to NumPy arrays
        X_combined_lstm = np.array(X_combined_lstm)

        # Predict the labels for the test data
        y_pred = model.predict(X_combined_lstm)
        y_pred_classes = np.argmax(y_pred, axis=1)  # Adjust to match class labels
        print(f'y_pred shape: {y_pred.shape}')

        print(y_pred[-1])
        print(y_pred_classes)

        plot_live_signals(df, y_pred_classes)

    time.sleep(60)

"""# Live Quantile Model"""

live = True
interval = '5min'
threshold  = 1.2
limit_one_active_trade = True

# load model
trade_model_name = f'{interval}tradeModel_quantile_B_v3'  #f'{interval}tradeModel_quantile_v6_epoch_90'
model = tf.keras.models.load_model(f'/content/drive/MyDrive/NeuralNetworkTradeBot/models/{trade_model_name}.keras', custom_objects={'quantile_loss': quantile_loss}, safe_mode=False)

timesteps = 96
look_ahead_period = 48
todays_predictions = []
todays_trades = []
combineDailyATRToCSV_AV(symbol='SPY', interval='daily', time_period=14)
old_bias = None

# charting_data =

while live:

    current_minute = time.localtime().tm_min
    current_hour = time.localtime().tm_hour - 4

    if current_minute % 1 == 0:

        time.sleep(10)

        # refresh live data every 5min
        combineDataToCSV_AV(symbol='SPY', interval=interval, time_period=14, month=None)

        # Load the data
        df_raw = pd.read_csv(f'/content/drive/MyDrive/NeuralNetworkTradeBot/live/current.csv', parse_dates=['datetime'])
        df_raw.sort_values('datetime', inplace=True)

        # create df_with_features from df_raw (na rows are dropped)
        # use build_features_for_df() if dropna doesnt drop any recent rows
        # *make sure to not drop any of the latest rows*
        # (ex. if daily atr won't load current day then use the prior day)
        df = build_features_for_df(df_raw, 'dff_live')

        current_daily_atr = df.iloc[-1]['daily_atr']
        current_close = df.iloc[-1]['close']
        current_time = df.iloc[-1]['datetime'].time()

        print(f'current_daily_atr: {current_daily_atr}')
        print(f'current_close: {current_close}')
        print(f'current_time: {current_time}')

        # (X_lstm starts at df[timesteps] and ends at df[-look_ahead_period])

        with tf.device('/device:GPU:0'):
            # scale the data, then augment into 3d w/ look_back_period
            exclusion_list = ['datetime', 'time', 'target', ]
            features = [col for col in df.columns if col not in exclusion_list]
            scaler = StandardScaler()
            sdf = scaler.fit_transform(df[features])
            X_lstm = []
            # X_raw = []
            for i in range(timesteps, len(df)):
                X_lstm.append(sdf[i-timesteps:i])
                # X_raw.append(df.iloc[i])
            # X_raw = np.array(X_raw)
            X_lstm = np.array(X_lstm)

            y_pred = model.predict(X_lstm)

            # Convert to NumPy array if it's a list
            if isinstance(y_pred, list):
                y_pred = np.array(y_pred)

        # if y_pred[1, -1] > y_pred[4, -1] * threshold:
        #     bias = 'Up'
        # elif y_pred[4, -1] > y_pred[1, -1] * threshold:
        #     bias = 'Down'
        # else:
        #     bias = 'Neutral'

        # latest_preds = y_pred[:, -1]
        # todays_predictions.append(latest_preds)

        # # Convert to NumPy array if it's a list
        # if isinstance(y_pred, list):
        #     y_pred = np.array(y_pred)

        # # Now you can slice it
        # y_pred_max_up_0_15 = y_pred[0, :]
        # y_pred_max_up_0_5 = y_pred[1, :]
        # y_pred_max_up_0_85 = y_pred[2, :]
        # y_pred_max_down_0_15 = y_pred[3, :]
        # y_pred_max_down_0_5 = y_pred[4, :]
        # y_pred_max_down_0_85 = y_pred[5, :]

        # up15 = y_pred_max_up_0_15[-1][0] * current_daily_atr
        # up5 = y_pred_max_up_0_5[-1][0] * current_daily_atr
        # up85 = y_pred_max_up_0_85[-1][0] * current_daily_atr

        # down15 = y_pred_max_down_0_15[-1][0] * current_daily_atr
        # down5 = y_pred_max_down_0_5[-1][0] * current_daily_atr
        # down85 = y_pred_max_down_0_85[-1][0] * current_daily_atr

        #-----------
        y_pred_dict = {
            'max_up': {
                'quarter': {0.15: y_pred[0, :], 0.5: y_pred[1, :], 0.85: y_pred[2, :]},
                'half': {0.15: y_pred[3, :], 0.5: y_pred[4, :], 0.85: y_pred[5, :]},
                'full': {0.15: y_pred[6, :], 0.5: y_pred[7, :], 0.85: y_pred[8, :]}
            },
            'max_down': {
                'quarter': {0.15: y_pred[9, :], 0.5: y_pred[10, :], 0.85: y_pred[11, :]},
                'half': {0.15: y_pred[12, :], 0.5: y_pred[13, :], 0.85: y_pred[14, :]},
                'full': {0.15: y_pred[15, :], 0.5: y_pred[16, :], 0.85: y_pred[17, :]}
            },
            'close': {
                'quarter': {0.15: y_pred[18, :], 0.5: y_pred[19, :], 0.85: y_pred[20, :]},
                'half': {0.15: y_pred[21, :], 0.5: y_pred[22, :], 0.85: y_pred[23, :]},
                'full': {0.15: y_pred[24, :], 0.5: y_pred[25, :], 0.85: y_pred[26, :]}
            }
        }
        predictions = {
            period: {
                lookahead: {q: (y_pred_dict[period][lookahead][q]) for q in [0.15, 0.5, 0.85]}
                for lookahead in ['quarter', 'half', 'full']
            } for period in ['max_up', 'max_down', 'close']
        }
        # up15 = predictions['max_up']['quarter'][0.15]
        # up5 = predictions['max_up']['quarter'][0.5]
        # up85 = predictions['max_up']['quarter'][0.85]
        # bias = predictions['close']['quarter'][0.5]
        # down15 = predictions['max_down']['quarter'][0.15]
        # down5 = predictions['max_down']['quarter'][0.5]
        # down85 = predictions['max_down']['quarter'][0.85]
        #-----------

        # message = f'current_time: {current_time}\n' \
        #           f'15% chance: {up15[-1]* current_daily_atr + current_close}\n' \
        #           f'50% chance: {up5[-1]* current_daily_atr + current_close}\n' \
        #           f'85% chance: {up15[-1]* current_daily_atr + current_close}\n' \
        #           f'Current Close: {current_close}\n' \
        #           f'Current Bias: {bias[-1]* current_daily_atr + current_close}\n' \
        #           f'85% chance: {current_close - down85[-1]* current_daily_atr}\n' \
        #           f'50% chance: {current_close - down5[-1]* current_daily_atr}\n' \
        #           f'15% chance: {current_close - down15[-1]* current_daily_atr}\n' \
        #           f'----------------------------------- \n' \

        message = f'current_time: {current_time}\n' \
          f'--- Upward Predictions ---\n' \
          f'Max Up Quarter 15%: {predictions["max_up"]["quarter"][0.15][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Quarter 50%: {predictions["max_up"]["quarter"][0.5][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Quarter 85%: {predictions["max_up"]["quarter"][0.85][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Half 15%: {predictions["max_up"]["half"][0.15][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Half 50%: {predictions["max_up"]["half"][0.5][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Half 85%: {predictions["max_up"]["half"][0.85][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Full 15%: {predictions["max_up"]["full"][0.15][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Full 50%: {predictions["max_up"]["full"][0.5][-1] * current_daily_atr + current_close}\n' \
          f'Max Up Full 85%: {predictions["max_up"]["full"][0.85][-1] * current_daily_atr + current_close}\n\n' \
          f'--- Downward Predictions ---\n' \
          f'Max Down Quarter 15%: {current_close - predictions["max_down"]["quarter"][0.15][-1] * current_daily_atr}\n' \
          f'Max Down Quarter 50%: {current_close - predictions["max_down"]["quarter"][0.5][-1] * current_daily_atr}\n' \
          f'Max Down Quarter 85%: {current_close - predictions["max_down"]["quarter"][0.85][-1] * current_daily_atr}\n' \
          f'Max Down Half 15%: {current_close - predictions["max_down"]["half"][0.15][-1] * current_daily_atr}\n' \
          f'Max Down Half 50%: {current_close - predictions["max_down"]["half"][0.5][-1] * current_daily_atr}\n' \
          f'Max Down Half 85%: {current_close - predictions["max_down"]["half"][0.85][-1] * current_daily_atr}\n' \
          f'Max Down Full 15%: {current_close - predictions["max_down"]["full"][0.15][-1] * current_daily_atr}\n' \
          f'Max Down Full 50%: {current_close - predictions["max_down"]["full"][0.5][-1] * current_daily_atr}\n' \
          f'Max Down Full 85%: {current_close - predictions["max_down"]["full"][0.85][-1] * current_daily_atr}\n\n' \
          f'--- Close Predictions ---\n' \
          f'Close Quarter 15%: {predictions["close"]["quarter"][0.15][-1] * current_daily_atr + current_close}\n' \
          f'Close Quarter 50%: {predictions["close"]["quarter"][0.5][-1] * current_daily_atr + current_close}\n' \
          f'Close Quarter 85%: {predictions["close"]["quarter"][0.85][-1] * current_daily_atr + current_close}\n' \
          f'Close Half 15%: {predictions["close"]["half"][0.15][-1] * current_daily_atr + current_close}\n' \
          f'Close Half 50%: {predictions["close"]["half"][0.5][-1] * current_daily_atr + current_close}\n' \
          f'Close Half 85%: {predictions["close"]["half"][0.85][-1] * current_daily_atr + current_close}\n' \
          f'Close Full 15%: {predictions["close"]["full"][0.15][-1] * current_daily_atr + current_close}\n' \
          f'Close Full 50%: {predictions["close"]["full"][0.5][-1] * current_daily_atr + current_close}\n' \
          f'Close Full 85%: {predictions["close"]["full"][0.85][-1] * current_daily_atr + current_close}\n\n' \
          f'Current Close: {current_close}\n' \
          f'-----------------------------------\n'

        message_post(token, channel_id, message)

        if current_minute % 30 == 0:
            hours_left_in_market = 16 - current_hour
            min_left_in_market = 60 - current_minute if current_minute > 0 else 0
            message = f'*30min alert* @everyone \n'
            message_post(token, channel_id, message)

        # if old_bias is not None and old_bias != bias:
        #     message = f'*Bias has changed to {bias}* @everyone \n'
        #     message_post(token, channel_id, message)
        # old_bias = bias



        print(message)

        df_plot = df.iloc[timesteps:].copy()

        # y_pred_max_up_0_15 = y_pred_max_up_0_15.flatten()
        # y_pred_max_up_0_5 = y_pred_max_up_0_5.flatten()
        # y_pred_max_up_0_85 = y_pred_max_up_0_85.flatten()
        # y_pred_max_down_0_15 = y_pred_max_down_0_15.flatten()
        # y_pred_max_down_0_5 = y_pred_max_down_0_5.flatten()
        # y_pred_max_down_0_85 = y_pred_max_down_0_85.flatten()
        # y_pred_max_up_0_15 = up15
        # y_pred_max_up_0_5 = up5
        # y_pred_max_up_0_85 = up85
        # y_pred_max_down_0_15 = down15
        # y_pred_max_down_0_5 = down5
        # y_pred_max_down_0_85 = down85

        # print(df_plot.shape)
        # print(y_pred_max_up_0_15.shape)

        # results = evaluate_quantile_trades(df_plot, y_pred_max_up_0_15, y_pred_max_up_0_5,
        #                                 y_pred_max_up_0_85, y_pred_max_down_0_15, y_pred_max_down_0_5,
        #                                 y_pred_max_down_0_85, look_ahead_period, threshold=1.2,
        #                                 limit_one_active_trade=True)

        # plt.show()

        plot_live_quantile_trades(
            data=df_plot,
            y_pred=y_pred,
            look_ahead_period=look_ahead_period,
            threshold=threshold,
            limit_one_active_trade=limit_one_active_trade,
            trading_start_time="09:30",
            trading_end_time="16:00"
        )
        plt.show()
        time.sleep(50)

    else:
        time.sleep(10)